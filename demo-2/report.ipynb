{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation and Report Generation\n",
    "\n",
    "The final phase of SDMT involves aggregating evidence, validating the metrics reflected by the evidence we collected, and displaying this information in a report."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(store_path, exist_ok=True)   # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"ns\", \"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which reports are stored\n",
    "REPORTS_DIR = Path(os.getcwd()) / \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Values and get an updated `ValidatedSpec` with `Result`s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `Spec` ready and we have enough evidence, we create a `SpecValidator` with our spec, and add all the `Value`s we have. With that we can validate our spec and generate an output `ValidatedSpec`, with the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "from mlte.validation.spec_validator import SpecValidator\n",
    "\n",
    "from mlte.measurement.cpu import CPUStatistics\n",
    "from mlte.measurement.memory import MemoryStatistics\n",
    "from mlte.value.types.image import Image\n",
    "from mlte.value.types.integer import Integer\n",
    "\n",
    "from multiple_accuracy import MultipleAccuracy\n",
    "from multiple_ranksums import MultipleRanksums\n",
    "from ranksums import RankSums\n",
    "\n",
    "# Load the specification\n",
    "spec = Spec.load()\n",
    "\n",
    "# Add all values to the validator.\n",
    "spec_validator = SpecValidator(spec)\n",
    "spec_validator.add_value(MultipleAccuracy.load(\"accuracy across gardens.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur2x8.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur5x8.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur0x8.value\"))\n",
    "spec_validator.add_value(MultipleRanksums.load(\"multiple ranksums for clade2.value\"))\n",
    "spec_validator.add_value(MultipleRanksums.load(\"multiple ranksums between clade2 and 3.value\"))\n",
    "spec_validator.add_value(Integer.load(\"model size.value\"))\n",
    "spec_validator.add_value(CPUStatistics.load(\"predicting cpu.value\"))\n",
    "spec_validator.add_value(MemoryStatistics.load(\"predicting memory.value\"))\n",
    "spec_validator.add_value(Image.load(\"image attributions.value\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate requirements and get validated details.\n",
    "validated_spec = spec_validator.validate()\n",
    "validated_spec.save(force=True)\n",
    "\n",
    "# We want to see the validation results in the Notebook, regardles sof them being saved.\n",
    "validated_spec.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see some of the results of the validation.\n",
    "\n",
    "For example, there is a significant difference between original model with no blur and blur 0x8. So we see a drop in model accuracy with increasing blur. But aside from max blur (0x8), the model accuracy fall off isn't bad.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Report generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
