{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect Evidence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries for loading the package locally\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def package_root() -> str:\n",
    "    \"\"\"Resolve the path to the project root.\"\"\"\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src/\"))\n",
    "\n",
    "sys.path.append(package_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which datasets are stored\n",
    "DATASETS_DIR = Path(os.getcwd()) / \"data\"\n",
    "# The path at which models are stored\n",
    "MODELS_DIR = Path(os.getcwd()) / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from typing import Tuple\n",
    "\n",
    "def load_data() -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Load machine learning dataset.\n",
    "    :return (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    return train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training dataset for use by training procedure\n",
    "X_train, _, y_train, _ = load_data()\n",
    "X_train.to_csv(DATASETS_DIR / \"data.csv\")\n",
    "y_train.to_csv(DATASETS_DIR / \"target.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize MLTE Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlte\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "\n",
    "mlte.set_model(\"IrisClassifier\", \"0.0.1\")\n",
    "mlte.set_artifact_store_uri(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Cost Measurements\n",
    "\n",
    "This section demonstrates the simplest possible use-case. We import a MLTE-defined `Measurement`, which is then invoked to produce a `Result`. This result can then be inspected and automatically saved to the artifact store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.result import Integer\n",
    "\n",
    "# Create a measurement\n",
    "measurement = LocalObjectSize(\"model size\")\n",
    "# Execute the measurement\n",
    "size: Integer = measurement.evaluate(MODELS_DIR / \"model.pkl\")\n",
    "\n",
    "# Inspec results\n",
    "print(size)\n",
    "\n",
    "# Save to artifact store\n",
    "size.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Cost Measurements\n",
    "\n",
    "Evidence in this section is largely the same as that demonstrated in the previous section, except it requires some additional setup from the user's perspective. Again, we utilize MLTE-defined `Measurement`s to produce `Result`s that can then be saved to the artifact store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def interpreter_path() -> Path:\n",
    "    \"\"\"Get the path to the current interpreter.\"\"\"\n",
    "    return Path(sys.executable)\n",
    "\n",
    "def script_path() -> Path:\n",
    "    \"\"\"Get the path to the training script.\"\"\"\n",
    "    return (Path(os.getcwd()) / \"train.py\").absolute()\n",
    "\n",
    "def spawn_training_job() -> int:\n",
    "    \"\"\"Spawn the training job and return its process identifier.\"\"\"\n",
    "    python = interpreter_path()\n",
    "    command = [\n",
    "        str(python),\n",
    "        str(script_path()),\n",
    "        \"--dataset-dir\", str(DATASETS_DIR.absolute()),\n",
    "        \"--models-dir\", str(MODELS_DIR.absolute())\n",
    "    ]\n",
    "    p = subprocess.Popen(command)\n",
    "    thread = threading.Thread(target=lambda: p.wait())\n",
    "    thread.start()\n",
    "    return p.pid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first evidence we collect are CPU utilization statistics for a local training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 1.33%\n",
      "Minimum: 0.00%\n",
      "Maximum: 2.75%\n"
     ]
    }
   ],
   "source": [
    "from mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics\n",
    "\n",
    "# Create a measurement\n",
    "measurement = LocalProcessCPUUtilization(\"training cpu\")\n",
    "# Execute the measurement\n",
    "cpu_stats: CPUStatistics = measurement.evaluate(spawn_training_job())\n",
    "\n",
    "# Inspect results\n",
    "print(cpu_stats)\n",
    "\n",
    "# Save to artifact store\n",
    "cpu_stats.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform a similar procedure to measure the memory consumption of a local training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 528357\n",
      "Minimum: 15764\n",
      "Maximum: 751704\n"
     ]
    }
   ],
   "source": [
    "from mlte.measurement.memory import LocalProcessMemoryConsumption, MemoryStatistics\n",
    "\n",
    "# Create a measurement\n",
    "measurement = LocalProcessMemoryConsumption(\"training memory\")\n",
    "# Execute the measurement\n",
    "mem_stats: MemoryStatistics = measurement.evaluate(spawn_training_job())\n",
    "\n",
    "# Inspect results\n",
    "print(mem_stats)\n",
    "\n",
    "# Save to artifact store\n",
    "mem_stats.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Efficacy Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import tree\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Train a classifier and save.\"\"\"\n",
    "    X_train, _, y_train, _ = load_data()\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    with (MODELS_DIR / \"model.pkl\").open(\"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    path = MODELS_DIR / \"model.pkl\"\n",
    "    with path.open(\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from mlte.measurement.result import Real\n",
    "from mlte.measurement import MeasurementMetadata, Identifier\n",
    "\n",
    "# Load the test dataset\n",
    "_, X_test, _, y_test = load_data()\n",
    "\n",
    "# Load the model\n",
    "model = load_model()\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = Real(\n",
    "    MeasurementMetadata(\"accuracy_score\", Identifier(\"accuracy\")),\n",
    "    accuracy_score(y_test, y_pred)\n",
    ")\n",
    "\n",
    "# Inspect result\n",
    "print(accuracy)\n",
    "\n",
    "# Save to artifact store\n",
    "accuracy.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
