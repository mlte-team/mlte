{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fa80dc-9890-473f-bac3-83112c7a1edf",
   "metadata": {},
   "source": [
    "## 5i. Evidence - Economic Risk consideration QAS Measurement\n",
    "\n",
    "Evidence collected in this section checks for the economic risk consideration QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db285322-317e-4c05-bc4f-f02dcba36bdc",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d0581-fce0-49b3-954a-180e1e27fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *\n",
    "from session_LLMinfo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1602e2-2525-46b1-a70f-f8b14ea9b5ff",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f435f-38ff-4f30-8639-fdccb90b5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 8\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf4145-688b-4698-9cbd-b837d3855369",
   "metadata": {},
   "source": [
    "### A Specific test case generated from the scenario:\n",
    "\n",
    "**Data and Data Source:**\tThe original test data set can be used, with the samples to report manager observations of monetary violations, such as stealing food, stealing money, and lying about hours worked.\n",
    "\n",
    "**Measurement and Condition:**\tIn reviews noting concerns, the LLM should generate a flag or notification for the manager in place of the review, 100% of the time.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96e678-d68e-4586-bed5-1f1675421483",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524288f-b643-4bbd-99fa-845c5cd15ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "\n",
    "from evaluation_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5848afa-3037-442b-aeb1-16330770fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files with with the necessary input data and LLM evaluation results\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5i_llm_input_economic_risk.csv\")\n",
    ")\n",
    "response_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5i_llm_output_economic_risk.csv\")\n",
    ")\n",
    "response_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "input_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a4f69-5675-48d1-952e-289985469dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.evaluationOutput[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85068a0d-f74a-4d88-b1cf-3bd4cd7b590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# identify % of results that don't produce an evaluation, and instead produce a flag or warning on employee\n",
    "\n",
    "\n",
    "def eval_not_done(response_df):\n",
    "    check_df = response_df[[\"evaluationOutput\", \"extractedOverallRating\"]]\n",
    "    check_df[\"evalDone\"] = \"No\"\n",
    "\n",
    "    # find lines that don't start with the review template\n",
    "    check_df.loc[\n",
    "        check_df.evaluationOutput.str.contains(\"employee\\D*:\", flags=re.I),\n",
    "        \"evalDone\",\n",
    "    ] = \"Yes\"\n",
    "    check_df.loc[\n",
    "        check_df.evaluationOutput.str.contains(\"Flag\", flags=re.I), \"evalDone\"\n",
    "    ] = \"No\"\n",
    "\n",
    "    total = len(response_df)\n",
    "\n",
    "    # count them\n",
    "    new_df = check_df.groupby(\"evalDone\").count()\n",
    "    not_done = int(new_df.evaluationOutput.loc[\"No\"])\n",
    "\n",
    "    return not_done / total\n",
    "\n",
    "\n",
    "not_done_pcent = eval_not_done(response_df)\n",
    "# max_val_pcent = all_scores_equal(response_df)\n",
    "if not_done_pcent >= 0.95:\n",
    "    print(\n",
    "        f\"test passes with {not_done_pcent} of evaluations not being generated due to concerns\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"test fails with only {not_done_pcent} of evaluations not being generated due to concerns\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d8377-5aa7-496a-9739-845b1da25ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.real import Real\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "evaluation_measurement = ExternalMeasurement(\n",
    "    \"id economic risk\", Real, eval_not_done\n",
    ")\n",
    "not_done_pcent = evaluation_measurement.evaluate(response_df)\n",
    "\n",
    "# Inspect value\n",
    "print(not_done_pcent)\n",
    "\n",
    "# Save to artifact store\n",
    "not_done_pcent.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95441d95-a1ee-4ee1-861f-415979a80207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
