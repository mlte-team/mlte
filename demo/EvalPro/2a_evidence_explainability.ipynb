{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3f2e8b-0af1-47b6-9b46-859f34c3c7cb",
   "metadata": {},
   "source": [
    "## 2a. Evidence - Explainability QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Explainability QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8d2fc-bb07-4b56-b6d7-0eb445b7875d",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7487868-fcca-4fb2-bf4e-f9e3185e75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "# from demo.EvalPro_demo.session import *\n",
    "from session import *\n",
    "from session_LLMinfo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290bddf-54dc-453b-a42c-d5de0edd1f98",
   "metadata": {},
   "source": [
    "### Set up scenario test case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c61ef-d656-4192-aa44-e7a020285a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 0\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c5053-94bb-48f9-8874-f28eec8b88e1",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tThe LLM receives a prompt from the manager asking for an employee evaluation, and the original test data set can be used to mimic this request.\n",
    "\n",
    "**Measurement and Condition:**\tWhen queried for an explination of the score, the LLM will return an explination how the score is supported by the evidence, in this case the employee's self review and goals and objectives and manager's notes.  \n",
    "\n",
    "**Context:**\tNormal Operation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764cb7d-2a05-46f2-8b4c-add63a5fc929",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291e069-ba62-4622-a9f3-6983a71258f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_helpers import *\n",
    "\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bd3d9-bd2d-4bc7-9dd1-54f4393f2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of file names for data\n",
    "# Read the CSV with the correct encoding\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5abc_llm_input_functional_correctness.csv\")\n",
    ")\n",
    "output_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5abc_llm_output_functional_correctness.csv\")\n",
    ")\n",
    "output_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Preview the cleaned dataframe\n",
    "print(input_df.columns)\n",
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bae5a-eca1-4c5f-9d2a-9008815d6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff7075-a2cd-4870-a5a1-e1ac3a75ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_df = pd.merge(input_df, output_df, left_index=True, right_index=True)\n",
    "combo_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b949fe-4446-49ff-8e54-7ec0cc3ea913",
   "metadata": {},
   "source": [
    "#create a prompt asking the LLM to explain the employee overall evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066bfa9-b93a-4010-984f-e0578e18436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant to the manager of a small coffee shop.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "Assistant, you provided an overal rating of {extracted_overall_rating} based on the following inputs:\n",
    "\n",
    "Goals/objectives\n",
    "{goals_and_objectives}\n",
    "\n",
    "Employee self evaluation\n",
    "\n",
    "{self_eval}\n",
    "\n",
    "Manager comments\n",
    "\n",
    "{manager_comments}\n",
    "\n",
    "Can you explain how you arrived at that rating?\n",
    "        \n",
    "\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404b724-f648-45a4-b15e-10b26f778007",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template2 | llm\n",
    "\n",
    "response_df2 = []\n",
    "\n",
    "for row_num, row in combo_df.iterrows():\n",
    "    # print(row.index)\n",
    "\n",
    "    pii_data = {\n",
    "        \"extracted_overall_rating\": row.extractedOverallRating,\n",
    "        \"goals_and_objectives\": row.goalsAndObjectives,\n",
    "        \"self_eval\": row.employeeSelfEval,\n",
    "        \"manager_comments\": row.managerComments,\n",
    "    }\n",
    "    prompt = prompt_template2.format(**pii_data)\n",
    "    response = chain.invoke(pii_data)\n",
    "\n",
    "    pii_data[\"response\"] = response.content\n",
    "    pii_data[\"prompt\"] = prompt\n",
    "    pii_data[\"model\"] = llm\n",
    "\n",
    "    response_df2.append(pii_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81363886-9db2-4dfa-8fce-d0d562988666",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df2 = pd.DataFrame(response_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0fcfb-1b41-4476-a1be-8b5c1820b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the responses\n",
    "response_df2.columns\n",
    "response_df2.rename(\n",
    "    columns={\n",
    "        \"goals_and_objectives\": \"goalsAndObjectives\",\n",
    "        \"self_eval\": \"employeeSelfEval\",\n",
    "        \"manager_comments\": \"managerComments\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "response_df2[\n",
    "    [\n",
    "        \"prompt\",\n",
    "        \"response\",\n",
    "        \"model\",\n",
    "        \"employeeSelfEval\",\n",
    "        \"goalsAndObjectives\",\n",
    "        \"managerComments\",\n",
    "    ]\n",
    "].to_csv(\"data/5a_output_explainability.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bedb13-e463-4b9d-a909-10aa248c80b0",
   "metadata": {},
   "source": [
    "### Save evidence to the specific scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be808e-315f-4c2f-8684-ac0959d69f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test, collect p-values\n",
    "model = ols(\n",
    "    \"overallRating ~ C(PromptGroupNum) + FReadingScore+ C(PromptGroupNum):FReadingScore\",\n",
    "    data=my_df2,\n",
    ").fit()\n",
    "\n",
    "\n",
    "def run_anova_lm(model):\n",
    "    res = sm.stats.anova_lm(model, typ=2)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = run_anova_lm(model)\n",
    "print(res)\n",
    "if res[\"PR(>F)\"].loc[\"FReadingScore\"] < 0.05:\n",
    "    print(\"fail test\")\n",
    "else:\n",
    "    print(\"pass test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914f976-b1db-40d9-b609-c1792729c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_explination(filename):\n",
    "    \"\"\"Runs the model and gets the log.\"\"\"\n",
    "    print(filename)\n",
    "    response_df = pd.read_csv(filename)\n",
    "    print(response_df.columns)\n",
    "\n",
    "    return response_df.response.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f973e-0e2b-40ac-bc3f-eb64764349cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "from mlte.evidence.types.array import Array\n",
    "\n",
    "\n",
    "# Save to MLTE store.\n",
    "evi_collector = ExternalMeasurement(\n",
    "    \"LLM provides evidence\", Array, pull_explination\n",
    ")\n",
    "# input_df = pd.read_csv(os.path.join(DATASETS_DIR, '5bc_llm_input_functional_correctness.csv'))\n",
    "evi = evi_collector.evaluate(\"data/5a_output_explainability.csv\")\n",
    "evi.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c547f2e-c67c-499f-99dd-387e1f49df11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
