{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3807d9f-ce38-4c34-8364-bac40a3aaea7",
   "metadata": {},
   "source": [
    "## 2f. Evidence - Time Behaviour QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the time behaviour QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4dac5f-217e-4ed3-8dcb-8b12b34ec31b",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c5382-e1b9-46c0-b9a8-26339f1bae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *\n",
    "from session_LLMinfo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f49cc3-7816-4b20-bd6c-a5c88fb7c927",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd390bba-fab7-4b2e-8a82-65e50c49c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 5\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5aa4f1-41d2-4362-a97e-8e6a483742f0",
   "metadata": {},
   "source": [
    "### A Specific test case generated from the scenario:\n",
    "\n",
    "**Data and Data Source:**\tThe original test data set can be used.\n",
    "\n",
    "**Measurement and Condition:**\tThe longest time for a single prompt model response completes is no more than 10 seconds after submission\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bef743-f644-457a-b75b-871aad0e1555",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6135e81-dcc1-4bbe-bca9-4057e6bb8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from evaluation_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814685a8-f8bf-43d0-a030-b860acc35ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files with with the necessary input data and LLM evaluation results\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5bc_llm_input_functional_correctness.csv\")\n",
    ")\n",
    "# response_df = pd.read_csv(os.path.join(DATASETS_DIR, '5e_llm_output_robustness.csv'))\n",
    "# response_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# input_df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149996e-6b16-4c7e-bb7c-9e3c8034a55e",
   "metadata": {},
   "source": [
    "### Save evidence to the specicified scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54a6f4-5606-4ee3-a2fa-431bbdca41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm\n",
    "time_performances = []\n",
    "chat_responses = []\n",
    "\n",
    "\n",
    "def time_model(input_df):\n",
    "    for index, row in input_df.iterrows():\n",
    "        pii_data = {\n",
    "            \"employee_name\": row.EmployeeName,\n",
    "            \"goals_and_objectives\": row.goalsAndObjectives,\n",
    "            \"self_eval\": row.employeeSelfEval,\n",
    "            \"manager_comments\": row.managerComments,\n",
    "        }\n",
    "\n",
    "    prompt = prompt_template.format(**pii_data)\n",
    "\n",
    "    start_time = time.time()\n",
    "    chat_response = chain.invoke(pii_data)\n",
    "    time_performances.append(time.time() - start_time)\n",
    "\n",
    "    return time_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c9c77-1d0f-4ed7-b51d-2dbd2dfcd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "time_measurement = ExternalMeasurement(\n",
    "    \"results returned promptly\", Array, time_model\n",
    ")\n",
    "time_res = time_measurement.evaluate(input_df)\n",
    "\n",
    "# Inspect value\n",
    "print(time_res)\n",
    "\n",
    "# Save to artifact store\n",
    "time_res.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa6086-419b-4587-affa-c10d872a6953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
