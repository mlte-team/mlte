{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Evidence - Fairness QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Fairness scenario defined in the previous step. Note that some functions will be loaded from external Python files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from demo.scenarios.session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "\n",
    "from demo.scenarios import garden\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(data_folder: str):\n",
    "    \"\"\"Loads all garden data results and taxonomy categories.\"\"\"\n",
    "    df_results = garden.load_base_results(data_folder, \"predictions_test.csv\")\n",
    "    df_results.head()\n",
    "\n",
    "    # Load the taxonomic data and merge with results.\n",
    "    df_info = garden.load_taxonomy(data_folder)\n",
    "    df_results.rename(columns={\"label\": \"Label\"}, inplace=True)\n",
    "    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n",
    "\n",
    "    return df_info, df_all\n",
    "\n",
    "\n",
    "def split_data(df_info, df_all, population_size=100):\n",
    "    \"\"\"Splits the data into 3 different populations to evaluate them.\"\"\"\n",
    "    df_gardenpop = df_info.copy()\n",
    "    df_gardenpop[\"Population1\"] = (\n",
    "        np.around(\n",
    "            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n",
    "            decimals=3,\n",
    "        )\n",
    "        * population_size\n",
    "    ).astype(int)\n",
    "    df_gardenpop[\"Population2\"] = (\n",
    "        np.around(\n",
    "            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n",
    "            decimals=3,\n",
    "        )\n",
    "        * population_size\n",
    "    ).astype(int)\n",
    "    df_gardenpop[\"Population3\"] = (\n",
    "        np.around(\n",
    "            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n",
    "            decimals=3,\n",
    "        )\n",
    "        * population_size\n",
    "    ).astype(int)\n",
    "    # df_gardenpop\n",
    "    print(\"Hello\")  # df_gardenpop[\"Population2\"])\n",
    "\n",
    "    print(df_gardenpop[\"Population2\"])\n",
    "    # build populations from test data set that match the garden compositions\n",
    "    from random import choices\n",
    "\n",
    "    # build 3 gardens with populations of population_size.\n",
    "    pop_names = [\"Population1\", \"Population2\", \"Population3\"]\n",
    "    gardenpops = np.zeros((3, population_size), int)\n",
    "    gardenmems = np.zeros((3, population_size), int)\n",
    "\n",
    "    for j in range(population_size):\n",
    "        for i in range(len(df_gardenpop)):\n",
    "            my_flower = df_gardenpop.iloc[i][\"Common Name\"]\n",
    "\n",
    "            for g in range(3):\n",
    "                n_choices = df_gardenpop.iloc[i][pop_names[g]]\n",
    "                my_choices = df_all[df_all[\"Common Name\"] == my_flower][\n",
    "                    \"model correct\"\n",
    "                ].to_list()\n",
    "                # print(f\"{n_choices} {my_choices}\")\n",
    "                my_selection = choices(my_choices, k=n_choices)\n",
    "\n",
    "                gardenpops[g][j] += sum(my_selection)\n",
    "                gardenmems[g][j] += len(my_selection)\n",
    "\n",
    "    gardenpops\n",
    "\n",
    "    return gardenpops, gardenmems\n",
    "\n",
    "\n",
    "def calculate_model_performance_acc(\n",
    "    gardenpops, gardenmems, population_size=100\n",
    "):\n",
    "    \"\"\"Get accucray of models across the garden populations\"\"\"\n",
    "    gardenacc = np.zeros((3, population_size), float)\n",
    "    for i in range(population_size):\n",
    "        for g in range(3):\n",
    "            gardenacc[g][i] = gardenpops[g][i] / gardenmems[g][i]\n",
    "    gardenacc\n",
    "\n",
    "    model_performance_acc = []\n",
    "    for g in range(3):\n",
    "        avg = round(np.average(gardenacc[g][:]), 3)\n",
    "        std = round(np.std(gardenacc[g][:]), 3)\n",
    "        min = round(np.amin(gardenacc[g][:]), 3)\n",
    "        max = round(np.amax(gardenacc[g][:]), 3)\n",
    "        model_performance_acc.append(round(avg, 3))\n",
    "\n",
    "        print(\"%1d %1.3f %1.3f %1.3f %1.3f\" % (g, avg, std, min, max))\n",
    "\n",
    "    return model_performance_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\n",
    "data = load_data(DATASETS_DIR)\n",
    "print(\"Splitting Data\")\n",
    "# print(data)\n",
    "split_data = split_data(data[0], data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "In this first example, we simply wrap the output from `accuracy_score` with a custom `Result` type to cope with the output of a third-party library that is not supported by a MLTE builtin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "accuracy_measurement = ExternalMeasurement(\n",
    "    \"accuracy across gardens\", Array, calculate_model_performance_acc\n",
    ")\n",
    "accuracy = accuracy_measurement.evaluate(split_data[0], split_data[1])\n",
    "\n",
    "# Inspect value\n",
    "print(accuracy)\n",
    "\n",
    "# Save to artifact store\n",
    "accuracy.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
