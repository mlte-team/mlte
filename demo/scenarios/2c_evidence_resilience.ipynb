{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85ee8a3c-5dac-409c-a06d-19a9e88df67b",
   "metadata": {},
   "source": [
    "## 2c. Evidence - Resilience QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Resilence scenario defined in the previous step. Note that some functions will be loaded from external Python files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa94b9df-9aab-4d9a-9eb4-166b7d07e882",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7667837-9d5e-47fe-a91f-806920593eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from demo.scenarios.session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b41342-15f5-4504-a9f3-b613251256e3",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3a4471-b618-44a3-b845-c56908b97d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "from demo.scenarios import garden\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate the base model accuracy result per data label\n",
    "    df_pos = (\n",
    "        df_results[df_results[\"model correct\"] == True].groupby(\"label\").count()\n",
    "    )\n",
    "    # df_pos.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg = (\n",
    "        df_results[df_results[\"model correct\"] == False]\n",
    "        .groupby(\"label\")\n",
    "        .count()\n",
    "    )\n",
    "    # df_neg.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg.rename(columns={\"model correct\": \"model incorrect\"}, inplace=True)\n",
    "    df_res = df_pos.merge(\n",
    "        df_neg, right_on=\"label\", left_on=\"label\", how=\"outer\"\n",
    "    )\n",
    "    df_res.fillna(0, inplace=True)\n",
    "    df_res[\"model acc\"] = df_res[\"model correct\"] / (\n",
    "        df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    )\n",
    "    df_res[\"count\"] = df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    df_res.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "    df_res.head()\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def calculate_accuracy_per_set(\n",
    "    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Calculate the model accuracy per data label for each blurred data set\n",
    "    base_filename = \"predictions_test\"\n",
    "    ext_filename = \".csv\"\n",
    "    set_filename = [\"_blur2x8\", \"_blur5x8\", \"_blur0x8\", \"_noR\", \"_noG\", \"_noB\"]\n",
    "\n",
    "    col_root = \"model acc\"\n",
    "\n",
    "    for fs in set_filename:\n",
    "        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n",
    "        colname = col_root + fs\n",
    "\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        # print(df_temp.head())\n",
    "        df_temp = df_temp[[\"model correct\", \"label\"]]\n",
    "\n",
    "        df_pos = (\n",
    "            df_temp[df_temp[\"model correct\"] == True].groupby(\"label\").count()\n",
    "        )\n",
    "        # df_pos.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "        df_neg = (\n",
    "            df_results[df_results[\"model correct\"] == False]\n",
    "            .groupby(\"label\")\n",
    "            .count()\n",
    "        )\n",
    "        # df_neg.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "        df_neg.rename(\n",
    "            columns={\"model correct\": \"model incorrect\"}, inplace=True\n",
    "        )\n",
    "        df_res2 = df_pos.merge(\n",
    "            df_neg,\n",
    "            right_on=\"label\",\n",
    "            left_on=\"label\",\n",
    "            how=\"outer\",\n",
    "        ).fillna(0)\n",
    "        df_res2.fillna(0, inplace=True)\n",
    "\n",
    "        df_res2[colname] = df_res2[\"model correct\"] / (\n",
    "            df_res2[\"model correct\"] + df_res2[\"model incorrect\"]\n",
    "        )\n",
    "        df_res2.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "\n",
    "        # print(f\"{fs}_DF_RES={df_res.tail()}\")\n",
    "        # print(f\"{fs}_DF_RES2={df_res2.tail()}\")\n",
    "        df_res = df_res.merge(\n",
    "            df_res2, right_on=\"label\", left_on=\"label\", how=\"outer\"\n",
    "        ).fillna(0)\n",
    "\n",
    "    # df_res.head()\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n",
    "    model_acc = sum(df_res[key] * df_res[\"count\"]) / sum(df_res[\"count\"])\n",
    "    print(name, model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc413466-7c47-459d-9a85-1a5fd7b06a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 102 102\n"
     ]
    }
   ],
   "source": [
    "# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.\n",
    "df_results = garden.load_base_results(DATASETS_DIR, \"predictions_test.csv\")\n",
    "df_results = df_results[[\"model correct\", \"label\"]]\n",
    "df_res = calculate_base_accuracy(df_results)\n",
    "df_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\n",
    "df_info = garden.load_taxonomy(DATASETS_DIR)\n",
    "df_all = garden.merge_taxonomy_with_results(df_res, df_info, \"label\", \"Label\")\n",
    "\n",
    "# fill in missing model accuracy data\n",
    "df_all[\"model acc_noR\"] = df_all[\"model acc_noR\"].fillna(0)\n",
    "df_all[\"model acc_noG\"] = df_all[\"model acc_noG\"].fillna(0)\n",
    "df_all[\"model acc_noB\"] = df_all[\"model acc_noB\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7eae2-25e2-4420-8dd3-c75a84b6aea4",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Now do the actual measurements. First simply see the model accuracy across channel loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862d5617-9ce2-46d0-99a4-52d299d72605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model accuracy 0.947265625\n",
      "model accuracy with no red channel 0.8943445335046898\n",
      "model accuracy with no green channel 0.9276878720238095\n",
      "model accuracy with no blue channel 0.8652988450351732\n"
     ]
    }
   ],
   "source": [
    "# view changes in model accuracy\n",
    "print_model_accuracy(df_res, \"model acc\", \"base model accuracy\")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_noR\", \"model accuracy with no red channel\"\n",
    ")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_noG\", \"model accuracy with no green channel\"\n",
    ")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_noB\", \"model accuracy with no blue channel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3105230c-cb51-4d4d-8b70-0452e3e48b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model acc</th>\n",
       "      <th>count</th>\n",
       "      <th>model acc_blur2x8</th>\n",
       "      <th>model acc_blur5x8</th>\n",
       "      <th>model acc_blur0x8</th>\n",
       "      <th>model acc_noR</th>\n",
       "      <th>model acc_noG</th>\n",
       "      <th>model acc_noB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       model acc  count  model acc_blur2x8  model acc_blur5x8  \\\n",
       "label                                                           \n",
       "0            1.0   10.0                1.0           1.000000   \n",
       "1            1.0   10.0                1.0           1.000000   \n",
       "2            1.0   10.0                1.0           1.000000   \n",
       "3            0.9   10.0                0.9           0.888889   \n",
       "4            1.0   10.0                1.0           1.000000   \n",
       "...          ...    ...                ...                ...   \n",
       "97           1.0   10.0                1.0           1.000000   \n",
       "98           1.0   10.0                1.0           1.000000   \n",
       "99           1.0   10.0                1.0           1.000000   \n",
       "100          1.0   10.0                1.0           1.000000   \n",
       "101          1.0   10.0                1.0           1.000000   \n",
       "\n",
       "       model acc_blur0x8  model acc_noR  model acc_noG  model acc_noB  \n",
       "label                                                                  \n",
       "0               1.000000           1.00            1.0            1.0  \n",
       "1               1.000000           1.00            1.0            1.0  \n",
       "2               1.000000           1.00            1.0            1.0  \n",
       "3               0.666667           0.75            0.8            0.5  \n",
       "4               1.000000           1.00            1.0            1.0  \n",
       "...                  ...            ...            ...            ...  \n",
       "97              1.000000           1.00            1.0            1.0  \n",
       "98              1.000000           1.00            1.0            1.0  \n",
       "99              1.000000           1.00            1.0            1.0  \n",
       "100             1.000000           1.00            1.0            1.0  \n",
       "101             0.000000           1.00            1.0            1.0  \n",
       "\n",
       "[102 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155dba1d-bdd1-4cd3-93d9-df9d5e33f931",
   "metadata": {},
   "source": [
    "Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8782b15-fb65-42e0-9066-f654ed806174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blur R: RanksumsResult(statistic=np.float64(0.8052918417241214), pvalue=np.float64(0.4206512885130542))\n",
      "RanksumsResult(statistic=np.float64(0.8052918417241214), pvalue=np.float64(0.4206512885130542))\n",
      "blur G: RanksumsResult(statistic=np.float64(0.6356942962652858), pvalue=np.float64(0.5249756947411197))\n",
      "RanksumsResult(statistic=np.float64(0.6356942962652858), pvalue=np.float64(0.5249756947411197))\n",
      "blur B: RanksumsResult(statistic=np.float64(0.7673399434396266), pvalue=np.float64(0.44287942555285786))\n",
      "RanksumsResult(statistic=np.float64(0.7673399434396266), pvalue=np.float64(0.44287942555285786))\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "def run_ranksum(samp1, samp2):\n",
    "    res = scipy.stats.ranksums(samp1,samp2)\n",
    "    float_list = [float(x) for x in res]\n",
    "    #print(float(res))\n",
    "    return float_list\n",
    "\n",
    "my_blur = [\"R\", \"G\", \"B\"]\n",
    "for i in range(len(my_blur)):\n",
    "    # Define measurements.\n",
    "    ranksum_measurement = ExternalMeasurement(\n",
    "        f\"ranksums channel loss {my_blur[i]}\", Array, scipy.stats.ranksums\n",
    "    )\n",
    "\n",
    "    # Evaluate.\n",
    "    ranksum: Array = ranksum_measurement.evaluate(\n",
    "        df_res[\"model acc\"], df_res[f\"model acc_no{my_blur[i]}\"]\n",
    "    )\n",
    "    print(f\"blur {my_blur[i]}: {ranksum}\")\n",
    "\n",
    "    # Inspect values\n",
    "    print(ranksum)\n",
    "\n",
    "    # Save to artifact store\n",
    "    ranksum.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a4f02-1ad8-412f-b015-75a4af888e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
