{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation and Report Generation\n",
    "\n",
    "The final phase of SDMT involves aggregating evidence, validating the metrics reflected by the evidence we collected, and displaying this information in a report."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from demo.scenarios.session import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which reports are stored\n",
    "REPORTS_DIR = Path(os.getcwd()) / \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Values and get an updated `ValidatedSpec` with `Result`s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `Spec` ready and we have enough evidence, we create a `SpecValidator` with our spec, and add all the `Value`s we have. With that we can validate our spec and generate an output `ValidatedSpec`, with the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "from mlte.validation.spec_validator import SpecValidator\n",
    "from mlte.value.artifact import Value\n",
    "\n",
    "# Load the specification\n",
    "spec = Spec.load()\n",
    "\n",
    "# Add all values to the validator.\n",
    "spec_validator = SpecValidator(spec)\n",
    "spec_validator.add_values(Value.load_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Id 'overall accuracy' does not have a value that can be validated.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Validate requirements and get validated details.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m validated_spec \u001b[38;5;241m=\u001b[39m \u001b[43mspec_validator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m validated_spec\u001b[38;5;241m.\u001b[39msave(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# We want to see the validation results in the Notebook, regardles sof them being saved.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Repos/MLMismatch/mlte/mlte/validation/spec_validator.py:66\u001b[0m, in \u001b[0;36mSpecValidator.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ValidatedSpec:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Validates the internal properties given its requirements and the stored values, and generates a ValidatedSpec from it.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    :return: The validated specification\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ValidatedSpec(spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec, results\u001b[38;5;241m=\u001b[39mresults)\n",
      "File \u001b[0;32m~/Documents/Repos/MLMismatch/mlte/mlte/validation/spec_validator.py:79\u001b[0m, in \u001b[0;36mSpecValidator._validate_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m measurement_id \u001b[38;5;129;01min\u001b[39;00m conditions\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m measurement_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[0;32m---> 79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeasurement_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not have a value that can be validated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m             )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Validate and aggregate the results.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m results: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Result]] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Id 'overall accuracy' does not have a value that can be validated."
     ]
    }
   ],
   "source": [
    "# Validate requirements and get validated details.\n",
    "validated_spec = spec_validator.validate()\n",
    "validated_spec.save(force=True)\n",
    "\n",
    "# We want to see the validation results in the Notebook, regardles sof them being saved.\n",
    "validated_spec.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see some of the results of the validation.\n",
    "\n",
    "For example, there is a significant difference between original model with no blur and blur 0x8. So we see a drop in model accuracy with increasing blur. But aside from max blur (0x8), the model accuracy fall off isn't bad.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Report\n",
    "\n",
    "The final step of SDMT involves the generation of a report to communicate the results of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.report.artifact import (\n",
    "    Report,\n",
    "    CommentDescriptor,\n",
    "    QuantitiveAnalysisDescriptor,\n",
    ")\n",
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "report = Report(\n",
    "    validated_spec_id=validated_spec.identifier,\n",
    "    comments=[\n",
    "        CommentDescriptor(\n",
    "            content=\"This model should not be used for nefarious purposes.\"\n",
    "        )\n",
    "    ],\n",
    "    quantitative_analysis=QuantitiveAnalysisDescriptor(\n",
    "        content=\"Insert graph here.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "negotiation_card = NegotiationCard.load()\n",
    "report = report.populate_from(negotiation_card)\n",
    "\n",
    "report.save(force=True, parents=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
