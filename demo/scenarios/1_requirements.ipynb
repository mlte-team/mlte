{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAS Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using Quality Attribute Scenarios as guidance for the required Properties and Conditions.\n",
    "\n",
    "NOTE: this demo has an additional set of requirements than MLTE. You can install them with the command: \n",
    "\n",
    "`poetry install --with demo`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "The following are the QASs that we want to validate through the use of MLTE. The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. The system used an ML model that was trained on the flower category dataset [Nilsback 2008] (https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). \n",
    "\n",
    "* **Fairness - Model Impartial to Photo Location**\n",
    "  * The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.\n",
    "* **Robustness- Model Robust to Noise (Image Blur)**\n",
    "  * The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Robustness - Model Robust to Noise (Channel Loss)**\n",
    "  * The model receives a picture taken at a garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). The model should still be able to successfully identify the flower at the same rate as full images. Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets. Images with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Performance on Operational Platform**\n",
    "  * The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 150 MB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 150 MB. This will be measured using by adding the size of each file in the path for the model code.\n",
    "* **Interpretability - Understanding Model Results**\n",
    "  * The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference. \n",
    "\n",
    "* **Functional Correctness - Accuracy**\n",
    "  * The model receives receives a picture taken at the garden and can identify it correctly at least 90% of the time during normal operation.\n",
    "\n",
    "* **Functional Correctness - Input and Output Specification**\n",
    "  * The model reads inputs and provides outputs according to established input and output specifications during normal operation. During test execution all data in the test dataset produces an output that conforms to the output specification.\n",
    "\n",
    "* **Resilience - Input Validation**\n",
    "  * During normal operation, if the ML pipeline receives an input that does conform to the input specification it will generate the output \"N/A\" which the app will interpret as an error. The ML pipeline will create a log entry with the tag \"Model - Input Validation Error - [Input].\"\n",
    "\n",
    "* **Monitorability - Detect OOD inputs**\n",
    "  * During normal operation, the ML pipeline will log errors when out of distribution data is observed. The ML pipeline will create a log entry with the tag \"Model - Input OOD Error - [Input].\"\n",
    "\n",
    "* **Monitorability - Monitor shifts in output (confidence) distribution**\n",
    "  * During normal operation, ML pipeline will log errors when the output distribution changes. The ML pipeline will create a log entry with the tag \"Model - Output Confidence Error - [Output].\"\n",
    "\n",
    "* **Performance - Inference Time on Operational Platform**\n",
    "  * During normal operation, running on the operational platform, the model returns an output within two seconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n",
    "\n",
    "## 1.1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`. This can be done manually through code, but it is easier to use the MLTE UI to do so. Below we are copying a pre-built one that applies to this scenario. In MLTE, we define requirements by constructing a `NegotiationCard` that will include explicit Quality Attribute Scenarios with the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh copy_nc.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define a TestSuite\n",
    "\n",
    "In the first phase of SDMT, we define a `TestSuite` that represents the tests the completed model must will have to pass in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define the tests that will be required for the different requirements in a `TestSuite`. Note that several new `Evidence` types (`MultipleAccuracy`, `RankSums`, `MultipleRanksums`) had to be created in this case to define the Validators for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.tests.test_case import TestCase\n",
    "from mlte.tests.test_suite import TestSuite\n",
    "\n",
    "# The Evidence types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from mlte.evidence.types.image import Image\n",
    "from mlte.evidence.types.real import Real\n",
    "from demo.scenarios.values.multiple_accuracy import MultipleAccuracy\n",
    "from demo.scenarios.values.ranksums import RankSums\n",
    "from demo.scenarios.values.multiple_ranksums import MultipleRanksums\n",
    "from demo.scenarios.values.string import String\n",
    "\n",
    "\n",
    "# The full test suite.\n",
    "test_suite = TestSuite(\n",
    "    test_cases=[\n",
    "        # Fairness QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"accuracy across gardens\",\n",
    "            goal=\"Check if model performs well accross different populations\",\n",
    "            qas_list=[\"qas1\"],\n",
    "            validator=MultipleAccuracy.all_accuracies_more_or_equal_than(0.9),\n",
    "        ),\n",
    "        # Robustness QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur2x8\",\n",
    "            goal=\"Check blur and noise for 2x8 case\",\n",
    "            qas_list=[\"qas2\"],\n",
    "            validator=RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur5x8\",\n",
    "            goal=\"Check blur and noise for 5x8 case\",\n",
    "            qas_list=[\"qas2\"],\n",
    "            validator=RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur0x8\",\n",
    "            goal=\"Check blur and noise for 0x8 case\",\n",
    "            qas_list=[\"qas2\"],\n",
    "            validator=RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"multiple ranksums for clade2\",\n",
    "            goal=\"Check consistency in clade 2\",\n",
    "            qas_list=[\"qas2\"],\n",
    "            validator=MultipleRanksums.all_p_values_greater_or_equal_than(0.05),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"multiple ranksums for between clade2 and 3\",\n",
    "            goal=\"Check consistency between clades\",\n",
    "            qas_list=[\"qas2\"],\n",
    "            validator=MultipleRanksums.all_p_values_greater_or_equal_than(0.05),\n",
    "        ),\n",
    "        # Resource QASs test cases.\n",
    "        TestCase(\n",
    "            identifier=\"model size\",\n",
    "            goal=\"Check storage consumption\",\n",
    "            qas_list=[\"qas3\"],\n",
    "            validator=LocalObjectSize.output_type().less_than(150000000),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting memory\",\n",
    "            goal=\"Check memory used while predicting\",\n",
    "            qas_list=[\"qas4\"],\n",
    "            validator=LocalProcessMemoryConsumption.output_type().average_consumption_less_than(\n",
    "                512000.0\n",
    "            ),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting cpu\",\n",
    "            goal=\"Check cpu % used while predicting\",\n",
    "            qas_list=[\"qas5\"],\n",
    "            validator=LocalProcessCPUUtilization.output_type().max_utilization_less_than(\n",
    "                30.0\n",
    "            ),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting cpu time\",\n",
    "            goal=\"Check cpu time used while predicting\",\n",
    "            qas_list=[\"qas6\"],\n",
    "            validator=Real.less_than(2.0),\n",
    "        ),\n",
    "        # Interpretability QAS test case.\n",
    "        TestCase(\n",
    "            identifier=\"image attributions\",\n",
    "            goal=\"Check what the model is doing\",\n",
    "            qas_list=[\"qas7\"],\n",
    "            validator=Image.register_info(\"Inspect the image.\"),\n",
    "        ),\n",
    "        # Accuracy QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"overall accuracy\",\n",
    "            goal=\"Measure the overall accuracy of your end to end pipeline\",\n",
    "            qas_list=[\"qas8\"],\n",
    "            validator=Real.greater_than(0.9),\n",
    "        ),\n",
    "        # Interoperability: I/O spec QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"input format validation success\",\n",
    "            goal=\"Model input format must conform to specified format\",\n",
    "            qas_list=[\"qas9\"],\n",
    "            validator=String.contains(\"Model - Input Validation Pass\"),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"output format validation success\",\n",
    "            goal=\"Model output format must conform to specified format\",\n",
    "            qas_list=[\"qas9\"],\n",
    "            validator=String.contains(\"Model - Output Validation Pass\"),\n",
    "        ),\n",
    "        # Resilience: Input Validation QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"input format validation error\",\n",
    "            goal=\"Model inputs must conform to specified format\",\n",
    "            qas_list=[\"qas10\"],\n",
    "            validator=String.contains(\"Model - Input Validation Error\"),\n",
    "        ),\n",
    "        # Monitorability QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"detect ood inputs\",\n",
    "            goal=\"Monitor inputs for OOD data and unexpected shifts\",\n",
    "            qas_list=[\"qas11\"],\n",
    "            validator=String.contains(\"Model - Input OOD Error\"),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"monitor output confidence shift\",\n",
    "            goal=\"Monitor inputs for OOD data and unexpected shifts\",\n",
    "            qas_list=[\"qas11\"],\n",
    "            validator=String.contains(\"Model - Output Confidence Error\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_suite.save(parents=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
