{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAS Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using Quality Attribute Scenarios as guidance for the required Properties and Conditions.\n",
    "\n",
    "NOTE: this demo has an additional set of requirements than MLTE. You can install them with the command: \n",
    "\n",
    "`poetry install --with demo`\n",
    "\n",
    "If running on macOS, also install:\n",
    "\n",
    "`poetry install --with demo-mac`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "The following are the QASs that we want to validate through the use of MLTE. The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. The system used an ML model that was trained on the flower category dataset [Nilsback 2008] (https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). \n",
    "\n",
    "* **Fairness - Model Impartial to Photo Location**\n",
    "  * The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.\n",
    "* **Robustness- Model Robust to Noise (Image Blur)**\n",
    "  * The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Robustness - Model Robust to Noise (Channel Loss)**\n",
    "  * The model receives a picture taken at a garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). The model should still be able to successfully identify the flower at the same rate as full images. Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets. Images with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Performance on Operational Platform**\n",
    "  * The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.\n",
    "* **Interpretability - Understanding Model Results**\n",
    "  * The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference. \n",
    "\n",
    "  TODO: add new scenarios\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n",
    "\n",
    "## 1.1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: \"_mexican_aster\",\n",
    "    1: \"alpine_sea_holly\",\n",
    "    2: \"anthurium\",\n",
    "    3: \"artichoke\",\n",
    "    4: \"arum_lily\",\n",
    "    5: \"azalea\",\n",
    "    6: \"ball_moss\",\n",
    "    7: \"ballon_flower\",\n",
    "    8: \"barberton_daisy\",\n",
    "    9: \"bearded_iris\",\n",
    "    10: \"bee_balm\",\n",
    "    11: \"bird_of_paradise\",\n",
    "    12: \"bishop_of_llandaf_dahlia\",\n",
    "    13: \"black-eyed_susan\",\n",
    "    14: \"blackberry_lily\",\n",
    "    15: \"blanket_flower\",\n",
    "    16: \"bolero_deep_blue\",\n",
    "    17: \"bougainvillea\",\n",
    "    18: \"bromelia\",\n",
    "    19: \"buttercup\",\n",
    "    20: \"california_poppy\",\n",
    "    21: \"camellia\",\n",
    "    22: \"canna_lily\",\n",
    "    23: \"canterbury_bells\",\n",
    "    24: \"cape_flower\",\n",
    "    25: \"carnation\",\n",
    "    26: \"cattleya\",\n",
    "    27: \"cautleya_spicata\",\n",
    "    28: \"clematis\",\n",
    "    29: \"coltsfoot\",\n",
    "    30: \"columbine\",\n",
    "    31: \"common_dandelion\",\n",
    "    32: \"corn_poppy\",\n",
    "    33: \"cyclamen\",\n",
    "    34: \"daffodil\",\n",
    "    35: \"dahlia\",\n",
    "    36: \"desert_rose\",\n",
    "    37: \"english_marigold\",\n",
    "    38: \"fire_lily\",\n",
    "    39: \"foxglove\",\n",
    "    40: \"frangipani\",\n",
    "    41: \"fritillary\",\n",
    "    42: \"garden_phlox\",\n",
    "    43: \"gaura\",\n",
    "    44: \"gazania\",\n",
    "    45: \"geranium\",\n",
    "    46: \"globe_flower\",\n",
    "    47: \"globe_thistle\",\n",
    "    48: \"grape_hyacinth\",\n",
    "    49: \"great_masterwort\",\n",
    "    50: \"hard-leaved_pocket_orchid\",\n",
    "    51: \"hibiscus\",\n",
    "    52: \"hippesatrum\",\n",
    "    53: \"japanese_anemone\",\n",
    "    54: \"king_protea\",\n",
    "    55: \"lenten_rose\",\n",
    "    56: \"lotus\",\n",
    "    57: \"love_in_the_mist\",\n",
    "    58: \"magnolia\",\n",
    "    59: \"mallow\",\n",
    "    60: \"marigold\",\n",
    "    61: \"mexican_petunia\",\n",
    "    62: \"monkshood\",\n",
    "    63: \"moon_orchild\",\n",
    "    64: \"morning_glory\",\n",
    "    65: \"osteospermum\",\n",
    "    66: \"oxeye_daisy\",\n",
    "    67: \"passion_flower\",\n",
    "    68: \"pelargonium\",\n",
    "    69: \"peruvian_lily\",\n",
    "    70: \"petunia\",\n",
    "    71: \"pincushion_flower\",\n",
    "    72: \"poinsettia\",\n",
    "    73: \"primrose\",\n",
    "    74: \"primula\",\n",
    "    75: \"prince_of_whales_feather\",\n",
    "    76: \"purple_coneflower\",\n",
    "    77: \"red_ginger\",\n",
    "    78: \"rose\",\n",
    "    79: \"siam_tulip\",\n",
    "    80: \"silverbush\",\n",
    "    81: \"snapdragon\",\n",
    "    82: \"spear_thistle\",\n",
    "    83: \"spring_crocus\",\n",
    "    84: \"stemless_gentain\",\n",
    "    85: \"sunflower\",\n",
    "    86: \"swear_pea\",\n",
    "    87: \"sweet_william\",\n",
    "    88: \"sword_lily\",\n",
    "    89: \"thorn_apple\",\n",
    "    90: \"tiger_lily\",\n",
    "    91: \"tithonia_(incorrectly_labeled_as_orange_dahlia)\",\n",
    "    92: \"toad_lily\",\n",
    "    93: \"tree_mallow\",\n",
    "    94: \"tree_poppy\",\n",
    "    95: \"trumpet_creeper\",\n",
    "    96: \"wallflower\",\n",
    "    97: \"water_lily\",\n",
    "    98: \"watercress\",\n",
    "    99: \"wild_pansy\",\n",
    "    100: \"windflower\",\n",
    "    101: \"yellow_iris\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "from mlte.model.shared import (\n",
    "    MetricDescriptor,\n",
    "    DataDescriptor,\n",
    "    DataClassification,\n",
    "    FieldDescriptor,\n",
    "    LabelDescriptor,\n",
    "    ModelDescriptor,\n",
    "    ModelResourcesDescriptor,\n",
    "    ModelIODescriptor,\n",
    "    QASDescriptor,\n",
    "    SystemDescriptor,\n",
    "    GoalDescriptor,\n",
    "    ProblemType,\n",
    "    RiskDescriptor\n",
    ")\n",
    "\n",
    "card = NegotiationCard(\n",
    "    system=SystemDescriptor(\n",
    "        goals=[\n",
    "            GoalDescriptor(\n",
    "                description=\"The model should perform well.\",\n",
    "                metrics=[\n",
    "                    MetricDescriptor(\n",
    "                        description=\"accuracy\",\n",
    "                        baseline=\"Better than random chance.\",\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        problem_type=ProblemType.CLASSIFICATION,\n",
    "        task=\"Flower Classification\",\n",
    "        usage_context=\"A handheld flower identification device.\",\n",
    "        risks=RiskDescriptor(\n",
    "            fp=\"The wrong type of flower is identified.\",\n",
    "            fn=\"The flower is not identified.\",\n",
    "            other=\"N/A\",\n",
    "        ),\n",
    "    ),\n",
    "    data=[\n",
    "        DataDescriptor(\n",
    "            description=\"Oxford flower dataset.\",\n",
    "            classification=DataClassification.UNCLASSIFIED,\n",
    "            access=\"None\",\n",
    "            labeling_method=\"by hand\",\n",
    "            fields=[\n",
    "                FieldDescriptor(\n",
    "                    name=\"filename\",\n",
    "                    description=\"path to flower image.\",\n",
    "                    type=\"string to png file\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Kingdom\",\n",
    "                    description=\"The second highest taxonomic rank.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Phylum\",\n",
    "                    description=\"The taxonomic rank below kingdom and above Clade 1.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Clade1\",\n",
    "                    description=\"The taxonomic rank below Phylum and above Clade 2.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Clade2\",\n",
    "                    description=\"The taxonomic rank below Clade 1 and above Clade 3.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Clade3\",\n",
    "                    description=\"The taxonomic rank below Clade 2 and above Order.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Order\",\n",
    "                    description=\"The taxonomic rank below Clade 3 and above Family.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Family\",\n",
    "                    description=\"The taxonomic rank below Order and above Subfamily.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Subfamily\",\n",
    "                    description=\"The taxonomic rank below Family and above Genus.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Genus\",\n",
    "                    description=\"The taxonomic rank below Subfamily and above Species.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Common Name\",\n",
    "                    description=\"Image of flower including background.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Other Name\",\n",
    "                    description=\"Image of flower including background.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "                FieldDescriptor(\n",
    "                    name=\"Label Name\",\n",
    "                    description=\"Image Label.\",\n",
    "                    type=\"string\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                ),\n",
    "            ],\n",
    "            labels=[\n",
    "                LabelDescriptor(description=label_dict[k], percent=0.0)\n",
    "                for k in label_dict\n",
    "            ],\n",
    "            policies=\"N/A\",\n",
    "            rights=\"N/A\",\n",
    "            source=\"https://archive.ics.uci.edu/dataset/53/iris\",\n",
    "        )\n",
    "    ],\n",
    "    model=ModelDescriptor(\n",
    "        development_compute_resources=ModelResourcesDescriptor(\n",
    "            cpu=\"1\", gpu=\"0\", memory=\"6MiB\", storage=\"2KiB\"\n",
    "        ),\n",
    "        deployment_platform=\"local server\",\n",
    "        capability_deployment_mechanism=\"API\",\n",
    "        input_specification=[\n",
    "            ModelIODescriptor(\n",
    "                name=\"i1\", description=\"description\", type=\"Vector[150]\"\n",
    "            )\n",
    "        ],\n",
    "        output_specification=[\n",
    "            ModelIODescriptor(\n",
    "                name=\"o1\", description=\"description\", type=\"Vector[3]\"\n",
    "            )\n",
    "        ],\n",
    "        production_compute_resources=ModelResourcesDescriptor(\n",
    "            cpu=\"1\",\n",
    "            gpu=\"0\",\n",
    "            memory=\"6MiB\",\n",
    "            storage=\"2KiB\",\n",
    "        ),\n",
    "    ),\n",
    "    qas=[\n",
    "        QASDescriptor(\n",
    "            quality=\"Fairness - Model Impartial to Photo Location\",\n",
    "            stimulus=\"The model receives a picture taken at the garden.\",\n",
    "            source=\"Provider of the new picture.\",\n",
    "            environment=\"Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from.\",\n",
    "            response=\"Model can correctly identify the correct flowers.\",\n",
    "            measure=\"The total accuracy of the model across each garden population should be higher or equal to 0.9.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Robustness to Noise (Image Blur)\",\n",
    "            stimulus=\"The model receives a picture taken at the garden which is a bit blurry.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"Test data needs to include blurred flower images. Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur.\",\n",
    "            response=\"The model should still be able to successfully identify the flower at the same rate as non-blurry images.\",\n",
    "            measure=\"This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Robustness to Noise (Channel Loss)\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). \",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets.\",\n",
    "            response=\"The model should still be able to successfully identify the flower at the same rate as full images.\",\n",
    "            measure=\"This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Performance on Operational Platform\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device. The model will need to run on this device.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"The model will not exceed the limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively) available.\",\n",
    "            measure=(\n",
    "                \"1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measured using ps.\"\n",
    "                + \"2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap.\"\n",
    "                + \"3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.\"\n",
    "            ),\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Interpretability - Understanding Model Results\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"A validation dataset with feature and label distributions broadly similar to the test dataset.\",\n",
    "            response=\"The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. \",\n",
    "            measure=\"Identify flowers correctly at least 90% of the time during normal operation.\",\n",
    "        ),\n",
    "        # Start of table 2b\n",
    "        QASDescriptor(\n",
    "            quality=\"Functional Correctness: Accuracy\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"Model can correctly identify the correct flowers.\",\n",
    "            measure=\"The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Functional Correctness: Input and Output Specification\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"During test execution all data in the test dataset produces an output that conforms to the output specification.\",\n",
    "            measure=\"The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Resilience: Input Validation\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"Test dataset with invalid input formatting\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"The ML pipeline will create a log entry with the tag 'Model - Input Validation Error - <Input>.'\",\n",
    "            measure=\"Error present in log\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Monitorability: Detect OOD inputs \",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"Input validation dataset designed to induce known failures, generated based on Equivalence Testing and Boundary Testing.\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"Produce log entries for erroneous inputs.\",\n",
    "            measure=\"100% of errors produced by erroneous inputs are present in log files.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Monitorability: Monitor shifts in output (confidence) distribution.\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"Test dataset with known shift in output confidence.\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"Log entries are produced when distribution shift is detected.\",\n",
    "            measure=\"Confidence shift error is present in log files.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Performance: Inference Time on Operational Platform\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"The origional test dataset can be used\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"Log inference time during operation\",\n",
    "            measure=\"inference time should be less than 2 seconds.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "card.save(force=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define a Specification\n",
    "\n",
    "In the first phase of SDMT, we define a `Specification` that represents the requirements the completed model must meet in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define requirements by constructing a specification (`Spec`). For each property, we define the validations to perform as well. Note that several new `Value` types (`MultipleAccuracy`, `RankSums`, `MultipleRanksums`) had to be created to define the validation methods that will validate each Condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "\n",
    "# The Properties we want to validate, associated with our scenarios.\n",
    "from mlte.property.costs.storage_cost import StorageCost\n",
    "from mlte.property.fairness.fairness import Fairness\n",
    "from mlte.property.robustness.robustness import Robustness\n",
    "from mlte.property.interpretability.interpretability import Interpretability\n",
    "from mlte.property.costs.predicting_memory_cost import PredictingMemoryCost\n",
    "from mlte.property.costs.predicting_compute_cost import PredictingComputeCost\n",
    "\n",
    "# The Value types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from mlte.value.types.image import Image\n",
    "from mlte.value.types.real import Real\n",
    "from values.multiple_accuracy import MultipleAccuracy\n",
    "from values.ranksums import RankSums\n",
    "from values.multiple_ranksums import MultipleRanksums\n",
    "from properties.monitorability import Monitorability\n",
    "from properties.interoperability import Interoperability\n",
    "from properties.accuracy import Accuracy\n",
    "from values.string import String\n",
    "\n",
    "\n",
    "# The full spec. Note that the Robustness Property contains conditions for both Robustness scenarios.\n",
    "spec = Spec(\n",
    "    properties={\n",
    "        Fairness(\n",
    "            \"Important check if model performs well accross different populations\"\n",
    "        ): {\n",
    "            \"accuracy across gardens\": MultipleAccuracy.all_accuracies_more_or_equal_than(\n",
    "                0.9\n",
    "            )\n",
    "        },\n",
    "        Robustness(\"Robust against blur and noise\"): {\n",
    "            \"ranksums blur2x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur5x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur0x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"multiple ranksums for clade2\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "            \"multiple ranksums between clade2 and 3\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "        },\n",
    "        StorageCost(\"Critical since model will be in an embedded device\"): {\n",
    "            \"model size\": LocalObjectSize.value().less_than(3000)\n",
    "        },\n",
    "        PredictingMemoryCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting memory\": LocalProcessMemoryConsumption.value().average_consumption_less_than(\n",
    "                512000.0\n",
    "            )\n",
    "        },\n",
    "        PredictingComputeCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting cpu\": LocalProcessCPUUtilization.value().max_utilization_less_than(\n",
    "                30.0\n",
    "            )\n",
    "        },\n",
    "        Interpretability(\"Important to understand what the model is doing\"): {\n",
    "            \"image attributions\": Image.ignore(\"Inspect the image.\")\n",
    "        },\n",
    "        # Functional Correctness: accuracy\n",
    "        Accuracy(\"Measure the overall accuracy of your end to end pipeline\"): {\n",
    "            \"overall accuracy\": Real.greater_than(0.9)\n",
    "        },\n",
    "        # Functional Correctness: I/O spec\n",
    "        Interoperability(\n",
    "            \"Model output format must conform to specified format\"\n",
    "        ): {\n",
    "            \"output format validation\": String.contains(\n",
    "                \"Model - Output Validation Error\"\n",
    "            )\n",
    "        },\n",
    "        # Resilience: Input Validation\n",
    "        Resilience(\"Model inputs must conform to specified format\"): {\n",
    "            \"input format validation\": String.contains(\n",
    "                \"Model - Input Validation Error\"\n",
    "            )\n",
    "        },\n",
    "        # Monitorability: Detect OOD inputs\n",
    "        Monitorability(\"Monitor inputs for OOD data\"): {\n",
    "            \"detect ood inputs\": String.contains(\"Model - Input OOD Error\")\n",
    "        },\n",
    "        # Monitorability: Detect output shifts\n",
    "        Monitorability(\"Monitor outputs for unexpected shifts\"): {\n",
    "            \"monitor output confidence shift\": String.contains(\n",
    "                \"Model - Output Confidence Error\"\n",
    "            )\n",
    "        },\n",
    "        # Performance: Inference time on Operational Platform\n",
    "        PredictingComputeCost(\n",
    "            \"End to end latency for inference on operational platform\"\n",
    "        ): {\"predicting cpu time\": Real.less_than(2.0)},\n",
    "    }\n",
    ")\n",
    "spec.save(parents=True, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
