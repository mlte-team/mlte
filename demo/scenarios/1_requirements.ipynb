{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAS Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using Quality Attribute Scenarios as guidance for the required Properties and Conditions.\n",
    "\n",
    "NOTE: this demo has an additional set of requirements than MLTE. You can install them with the command: \n",
    "\n",
    "`poetry install --with demo`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "The following are the QASs that we want to validate through the use of MLTE. The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. The system used an ML model that was trained on the flower category dataset [Nilsback 2008] (https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). \n",
    "\n",
    "* **Fairness - Model Impartial to Photo Location**\n",
    "  * The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.\n",
    "* **Robustness- Model Robust to Noise (Image Blur)**\n",
    "  * The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Robustness - Model Robust to Noise (Channel Loss)**\n",
    "  * The model receives a picture taken at a garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). The model should still be able to successfully identify the flower at the same rate as full images. Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets. Images with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Performance on Operational Platform**\n",
    "  * The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 150 MB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 150 MB. This will be measured using by adding the size of each file in the path for the model code.\n",
    "* **Interpretability - Understanding Model Results**\n",
    "  * The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference. \n",
    "\n",
    "* **Functional Correctness - Accuracy**\n",
    "  * The model receives receives a picture taken at the garden and can identify it correctly at least 90% of the time during normal operation.\n",
    "\n",
    "* **Functional Correctness - Input and Output Specification**\n",
    "  * The model reads inputs and provides outputs according to established input and output specifications during normal operation. During test execution all data in the test dataset produces an output that conforms to the output specification.\n",
    "\n",
    "* **Resilience - Input Validation**\n",
    "  * During normal operation, if the ML pipeline receives an input that does conform to the input specification it will generate the output \"N/A\" which the app will interpret as an error. The ML pipeline will create a log entry with the tag \"Model - Input Validation Error - [Input].\"\n",
    "\n",
    "* **Monitorability - Detect OOD inputs**\n",
    "  * During normal operation, the ML pipeline will log errors when out of distribution data is observed. The ML pipeline will create a log entry with the tag \"Model - Input OOD Error - [Input].\"\n",
    "\n",
    "* **Monitorability - Monitor shifts in output (confidence) distribution**\n",
    "  * During normal operation, ML pipeline will log errors when the output distribution changes. The ML pipeline will create a log entry with the tag \"Model - Output Confidence Error - [Output].\"\n",
    "\n",
    "* **Performance - Inference Time on Operational Platform**\n",
    "  * During normal operation, running on the operational platform, the model returns an output within two seconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n",
    "\n",
    "## 1.1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`. This can be done manually through code, but it is easier to use the MLTE UI to do so. Below we are copying a pre-built one that applies to this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh copy_nc.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define a Specification\n",
    "\n",
    "In the first phase of SDMT, we define a `Specification` that represents the requirements the completed model must meet in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define requirements by constructing a specification (`Spec`). For each property, we define the validations to perform as well. Note that several new `Value` types (`MultipleAccuracy`, `RankSums`, `MultipleRanksums`) had to be created to define the validation methods that will validate each Condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "\n",
    "# The Properties we want to validate, associated with our scenarios.\n",
    "from mlte.property.costs.storage_cost import StorageCost\n",
    "from mlte.property.fairness.fairness import Fairness\n",
    "from mlte.property.robustness.robustness import Robustness\n",
    "from mlte.property.interpretability.interpretability import Interpretability\n",
    "from mlte.property.costs.predicting_memory_cost import PredictingMemoryCost\n",
    "from mlte.property.costs.predicting_compute_cost import PredictingComputeCost\n",
    "\n",
    "# The Value types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from mlte.value.types.image import Image\n",
    "from mlte.value.types.real import Real\n",
    "from demo.scenarios.values.multiple_accuracy import MultipleAccuracy\n",
    "from demo.scenarios.values.ranksums import RankSums\n",
    "from demo.scenarios.values.multiple_ranksums import MultipleRanksums\n",
    "from demo.scenarios.properties.monitorability import Monitorability\n",
    "from demo.scenarios.properties.interoperability import Interoperability\n",
    "from demo.scenarios.properties.resilience import Resilience\n",
    "from demo.scenarios.properties.accuracy import Accuracy\n",
    "from demo.scenarios.values.string import String\n",
    "\n",
    "\n",
    "# The full spec. Note that the Robustness Property contains conditions for both Robustness scenarios.\n",
    "spec = Spec(\n",
    "    properties={\n",
    "        Fairness(\n",
    "            \"Important check if model performs well accross different populations\"\n",
    "        ): {\n",
    "            \"accuracy across gardens\": MultipleAccuracy.all_accuracies_more_or_equal_than(\n",
    "                0.9\n",
    "            )\n",
    "        },\n",
    "        Robustness(\"Robust against blur and noise\"): {\n",
    "            \"ranksums blur2x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur5x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur0x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"multiple ranksums for clade2\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "            \"multiple ranksums between clade2 and 3\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "        },\n",
    "        StorageCost(\"Critical since model will be in an embedded device\"): {\n",
    "            \"model size\": LocalObjectSize.value().less_than(150000000)\n",
    "        },\n",
    "        PredictingMemoryCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting memory\": LocalProcessMemoryConsumption.value().average_consumption_less_than(\n",
    "                512000.0\n",
    "            )\n",
    "        },\n",
    "        PredictingComputeCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting cpu\": LocalProcessCPUUtilization.value().max_utilization_less_than(\n",
    "                30.0\n",
    "            ),\n",
    "            # Performance: Inference time on Operational Platform\n",
    "            \"predicting cpu time\": Real.less_than(2.0),\n",
    "        },\n",
    "        Interpretability(\"Important to understand what the model is doing\"): {\n",
    "            \"image attributions\": Image.register_info(\"Inspect the image.\")\n",
    "        },\n",
    "        # Functional Correctness: accuracy\n",
    "        Accuracy(\"Measure the overall accuracy of your end to end pipeline\"): {\n",
    "            \"overall accuracy\": Real.greater_than(0.9)\n",
    "        },\n",
    "        # Functional Correctness: I/O spec\n",
    "        Interoperability(\n",
    "            \"Model output format must conform to specified format\"\n",
    "        ): {\n",
    "            \"input format validation success\": String.contains(\n",
    "                \"Model - Input Validation Pass\"\n",
    "            ),\n",
    "            \"output format validation success\": String.contains(\n",
    "                \"Model - Output Validation Pass\"\n",
    "            ),\n",
    "        },\n",
    "        # Resilience: Input Validation\n",
    "        Resilience(\"Model inputs must conform to specified format\"): {\n",
    "            \"input format validation error\": String.contains(\n",
    "                \"Model - Input Validation Error\"\n",
    "            )\n",
    "        },\n",
    "        # Monitorability: Detect OOD inputs and unexpected shifts\n",
    "        Monitorability(\"Monitor inputs for OOD data and unexpected shifts\"): {\n",
    "            \"detect ood inputs\": String.contains(\"Model - Input OOD Error\"),\n",
    "            \"monitor output confidence shift\": String.contains(\n",
    "                \"Model - Output Confidence Error\"\n",
    "            ),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "spec.save(parents=True, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
