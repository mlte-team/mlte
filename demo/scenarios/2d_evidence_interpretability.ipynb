{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evidence - Interpretability QAS Measurements.\n",
    "\n",
    "Now we proceed to gather data about the Interpretability of the model, for the corresponding scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define different folders that will be used as input or output for the data gathering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# The path at which datasets are stored\n",
    "DATASETS_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "# Path where the model files are stored.\n",
    "MODELS_DIR = Path.cwd() / \"model\"\n",
    "\n",
    "# The path at which media is stored\n",
    "MEDIA_DIR = Path.cwd() / \"media\"\n",
    "os.makedirs(MEDIA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = (\n",
    "    MODELS_DIR / \"model_f3_a.json\"\n",
    ")  # The json file of the model to load\n",
    "weights_filename = MODELS_DIR / \"model_f_a.h5\"  # The weights file for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_analysis import *\n",
    "\n",
    "# Load the model/\n",
    "loaded_model = load_model(model_filename, weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show the image.\n",
    "\n",
    "flower_img = \"flower3.jpg\"  # Filename of flower image to use, public domain image adapted from: https://commons.wikimedia.org/wiki/File:Beautiful_white_flower_in_garden.jpg\n",
    "flower_idx = (\n",
    "    42  # Classifier index of associated flower (see OxfordFlower102Labels.csv)\n",
    ")\n",
    "\n",
    "im = read_image(os.path.join(DATASETS_DIR, flower_img))\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = run_model(im, loaded_model)\n",
    "\n",
    "baseline, alphas = generate_baseline_and_alphas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_images = interpolate_images(\n",
    "    baseline=baseline, image=im, alphas=alphas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "i = 0\n",
    "for alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n",
    "    i += 1\n",
    "    plt.subplot(1, len(alphas[0::10]), i)\n",
    "    plt.title(f\"alpha: {alpha:.1f}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gradients = compute_gradients(\n",
    "    loaded_model=loaded_model,\n",
    "    images=interpolated_images,\n",
    "    target_class_idx=flower_idx,\n",
    ")\n",
    "print(path_gradients.shape)\n",
    "\n",
    "ig = integral_approximation(gradients=path_gradients)\n",
    "print(ig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_attributions = integrated_gradients(\n",
    "    baseline=baseline,\n",
    "    image=im,\n",
    "    target_class_idx=flower_idx,\n",
    "    loaded_model=loaded_model,\n",
    "    m_steps=240,\n",
    ")\n",
    "print(ig_attributions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_img_attributions(\n",
    "    image=im,\n",
    "    baseline=baseline,\n",
    "    target_class_idx=flower_idx,\n",
    "    loaded_model=loaded_model,\n",
    "    m_steps=240,\n",
    "    cmap=plt.cm.inferno,\n",
    "    overlay_alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.savefig(MEDIA_DIR / \"attributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "from mlte.value.types.image import Image\n",
    "\n",
    "# Save to MLTE store.\n",
    "img_collector = ExternalMeasurement(\"image attributions\", Image)\n",
    "img = img_collector.ingest(MEDIA_DIR / \"attributions.png\")\n",
    "img.save(force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
