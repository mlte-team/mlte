{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Evidence - Interpretability QAS Measurements.\n",
    "\n",
    "Now we proceed to gather data about the Interpretability of the model, for the corresponding scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from demo.scenarios.session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Functions to help, and data setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo.scenarios.model_analysis import *\n",
    "\n",
    "# Load the model/\n",
    "loaded_model = load_model(MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show the image.\n",
    "\n",
    "flower_img = \"flower3.jpg\"  # Filename of flower image to use, public domain image adapted from: https://commons.wikimedia.org/wiki/File:Beautiful_white_flower_in_garden.jpg\n",
    "flower_idx = (\n",
    "    42  # Classifier index of associated flower (see OxfordFlower102Labels.csv)\n",
    ")\n",
    "\n",
    "im = read_image(os.path.join(SAMPLE_DATASET_DIR, flower_img))\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = run_model(im, loaded_model)\n",
    "\n",
    "baseline, alphas = generate_baseline_and_alphas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_images = interpolate_images(\n",
    "    baseline=baseline, image=im, alphas=alphas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "i = 0\n",
    "for alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n",
    "    i += 1\n",
    "    plt.subplot(1, len(alphas[0::10]), i)\n",
    "    plt.title(f\"alpha: {alpha:.1f}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gradients = compute_gradients(\n",
    "    loaded_model=loaded_model,\n",
    "    images=interpolated_images,\n",
    "    target_class_idx=flower_idx,\n",
    ")\n",
    "print(path_gradients.shape)\n",
    "\n",
    "ig = integral_approximation(gradients=path_gradients)\n",
    "print(ig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_attributions = integrated_gradients(\n",
    "    baseline=baseline,\n",
    "    image=im,\n",
    "    target_class_idx=flower_idx,\n",
    "    loaded_model=loaded_model,\n",
    "    m_steps=240,\n",
    ")\n",
    "print(ig_attributions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Execute and store measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_img_attributions(\n",
    "    image=im,\n",
    "    baseline=baseline,\n",
    "    target_class_idx=flower_idx,\n",
    "    loaded_model=loaded_model,\n",
    "    m_steps=240,\n",
    "    cmap=plt.cm.inferno,\n",
    "    overlay_alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.savefig(MEDIA_DIR / \"attributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "from mlte.evidence.types.image import Image\n",
    "\n",
    "# Save to MLTE store.\n",
    "img_collector = ExternalMeasurement(\"image attributions\", Image)\n",
    "img = img_collector.ingest(MEDIA_DIR / \"attributions.png\")\n",
    "img.save(force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
