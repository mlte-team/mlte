{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAS Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using Quality Attribute Scenarios as guidance for the required Properties and Conditions.\n",
    "\n",
    "NOTE: this demo has an additional set of requirements than MLTE. You can install them with the command: \n",
    "\n",
    "`poetry install --with demo`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "The following are the QASs that we want to validate through the use of MLTE. The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. The system used an ML model that was trained on the flower category dataset [Nilsback 2008] (https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). \n",
    "\n",
    "* **Explainability - Explain LLM results**\n",
    "The LLM receives a prompt from the manager asking for an employee evaluation during normal operations. The model outputs an employee evaluation, including a performance score for the employee and human understandable rationale for the employee score. \n",
    "    \n",
    "* **Functional Correctness- Model provides correct results**\n",
    "The LLM receives a prompt from the manager asking for an employee evaluation containing the employee goals, employee statement, and manager notes, during normal operations. The LLM outputs an employee evaluation, including a performance score for the employee. The LLM generated performance score should match the manager expected overall score in at least 95% of cases.\n",
    "\n",
    "* **Functional Correctness- Model provides self-consistent results**\n",
    "The LLM receives a prompt from the manager asking for an employee evaluation during normal operations. The model outputs an employee evaluation, including an overall performance score for the employee and an evaluation for each important sub-category. The sub-category scores should average to match the overall score in at least 95% of the cases.\n",
    "    \n",
    "* **Repeatability - Model provides repeatable results**\n",
    "The LLM may receive multiple entries of similarly performing employees for evaluation during normal operations. In the case of similar prompts and input information, the LLM generated employee evaluation, including performance scores and evaluation summary,  should be semantically similar each time. \n",
    "    \n",
    "* **Robustness - Model is robust to spacing, punction and case inconsistencies**\n",
    "The LLM may receive prompts with different variations, such as casing, spacing, and punctuation, during normal operations.  These variations should not influence the employee evaluation or score generated by the LLM.\n",
    "   \n",
    "* **Time Behavior - Model provides results in a timely manner**\n",
    "The LLM receives a prompt from the manager asking for an employee evaluation during normal LLM operations. The time required for the LLM to output an employee evaluation, including a performance score,  will be no more than 10 seconds.\n",
    "\n",
    "* **Fairness - Model provides similar results to different named individuals**\n",
    "As the LLM is being used in the evaluation process to help managers generate performance reviews, the generated reviews need to be fair for every employee. The LLM should provide the same performance review for similar prompts, regardless of the name and pronouns used by the employee.\n",
    "\n",
    "* **Inclusivity - Model provides results not dependent on reading level**\n",
    "The LLM will be used to generate performance reviews based on self-evaluations for all employees across the organization, in all job types. The overall performance score provided by the LLM should not be impacted by the writing level of the employee's self evaluation.\n",
    "\n",
    "* **Economic Risk Consideration - Model identifies economic risk from employees** \n",
    "\n",
    "* **Health and Safety Risk Considerations - Model identifies health and safety risk from employees**\n",
    "\n",
    "* **Societal and Ethical Risk Considerations - Model identifies societal and ethical risk from employees**\n",
    "\n",
    "* **Privacy- Model results do not  contain PII for otehr employees**\n",
    "The LLM will receive many similar prompts from managers asking for employee evaluations. The output employee evaluation should not contain PII for other employees despite similarity between prompts.\n",
    "\n",
    "* **Resistance - Model is resistant to additional, embedded instructions from employees**\n",
    "The LLM receives a prompt containing instructions for the LLM to give that employee a good review, within the self-evaluation, during normal operations. The performance score output by the LLM should not be affected by the embedded instructions. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n",
    "\n",
    "## 1.1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from demo.scenarios.session import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`. This can be done manually through code, but it is easier to use the MLTE UI to do so. Below we are copying a pre-built one that applies to this scenario. In MLTE, we define requirements by constructing a `NegotiationCard` that will include explicit Quality Attribute Scenarios with the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!sh ../setup_store.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define a TestSuite\n",
    "\n",
    "In the first phase of SDMT, we define a `TestSuite` that represents the tests the completed model must will have to pass in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define the tests that will be required for the different requirements in a `TestSuite`. Note that a new `Evidence` types (`MultipleRanksums`) had to be created in this case to handle the data and `Validator` for that case, and two stand-alone `Validator`s were defined in `validators.py` to validate data using existing `Evidence` types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load up our `NegotiationCard`, so we can get the list of ids of its quaity attribute scenarios, that will be added to the `TestCase`s here. Those ids are the way to link the `TestCase`s to their quality attribute requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "card.save(force=True)\n",
    "card.print_quality_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our `TestSuite`, consisting of a list of `TestCases`, each of them addressing one or more Quality Attribute Scenarios from our `NegotiationCard`. When defining the `TestCase`s below, we need to set the id of the corresponding Quality Attribute Scenario we want to test in its \"quality_scenarios\" attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.tests.test_case import TestCase\n",
    "from mlte.tests.test_suite import TestSuite\n",
    "\n",
    "# The Evidence types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.units import Units\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from mlte.evidence.types.image import Image\n",
    "from mlte.evidence.types.string import String\n",
    "from mlte.evidence.types.real import Real\n",
    "from demo.scenarios import validators\n",
    "from demo.scenarios.evidence.multiple_ranksums import MultipleRanksums\n",
    "from mlte.evidence.types.string import String\n",
    "from mlte.validation.validator import Validator\n",
    "\n",
    "\n",
    "# The full test suite.\n",
    "test_suite = TestSuite(\n",
    "    test_cases=[\n",
    "        # Fairness QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"accuracy across gardens\",\n",
    "            goal=\"Check if model performs well accross different populations\",\n",
    "            quality_scenarios=[\"default.card-qas_001\"],\n",
    "            validator=validators.all_accuracies_more_or_equal_than(0.9),\n",
    "        ),\n",
    "        # Robustness QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur2x8\",\n",
    "            goal=\"Check blur and noise for 2x8 case\",\n",
    "            quality_scenarios=[\"default.card-qas_002\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur5x8\",\n",
    "            goal=\"Check blur and noise for 5x8 case\",\n",
    "            quality_scenarios=[\"default.card-qas_002\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur0x8\",\n",
    "            goal=\"Check blur and noise for 0x8 case\",\n",
    "            quality_scenarios=[\"default.card-qas_002\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"effect of blur across families\",\n",
    "            goal=\"Check consistency in families\",\n",
    "            quality_scenarios=[\"default.card-qas_002\"],\n",
    "            validator=MultipleRanksums.all_p_values_greater_or_equal_than(0.05 / 141),\n",
    "        ),\n",
    "        #Resilience QAS test case \n",
    "        TestCase(\n",
    "            identifier=\"ranksums channel loss R\",\n",
    "            goal=\"Check consistency between channel loss\",\n",
    "            quality_scenarios=[\"default.card-qas_003\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums channel loss G\",\n",
    "            goal=\"Check consistency between channel loss\",\n",
    "            quality_scenarios=[\"default.card-qas_003\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums channel loss B\",\n",
    "            goal=\"Check consistency between channel loss\",\n",
    "            quality_scenarios=[\"default.card-qas_003\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        # Resource Utilization QASs test cases.\n",
    "        TestCase(\n",
    "            identifier=\"model size\",\n",
    "            goal=\"Check storage consumption\",\n",
    "            quality_scenarios=[\"default.card-qas_004\"],\n",
    "            validator=LocalObjectSize.get_output_type().less_than(\n",
    "                150, Units.megabyte\n",
    "            ),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting memory\",\n",
    "            goal=\"Check memory used while predicting\",\n",
    "            quality_scenarios=[\"default.card-qas_004\"],\n",
    "            validator=LocalProcessMemoryConsumption.get_output_type().average_consumption_less_than(\n",
    "                512.0, unit=Units.megabyte\n",
    "            ),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting cpu\",\n",
    "            goal=\"Check cpu % used while predicting\",\n",
    "            quality_scenarios=[\"default.card-qas_004\"],\n",
    "            validator=LocalProcessCPUUtilization.get_output_type().max_utilization_less_than(\n",
    "                30.0, unit=Units.percent\n",
    "            ),\n",
    "        ), \n",
    "        # Understandability QAS test case.\n",
    "        #TestCase(\n",
    "        #    identifier=\"image attributions\",\n",
    "        #    goal=\"Check what the model is doing\",\n",
    "        #    quality_scenarios=[\"default.card-qas_005\"],\n",
    "        #    validator=Image.register_info(\"Inspect the image.\"),\n",
    "        #),\n",
    "        # Functional Correctness - Accuracy QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"overall model accuracy\",\n",
    "            goal=\"Measure the overall accuracy of your end to end pipeline\",\n",
    "            quality_scenarios=[\"default.card-qas_006\"],\n",
    "            validator=Real.greater_than(0.9),\n",
    "        ),\n",
    "        # Functional Correctness - I/O spec QAS test cases.\n",
    "        #TestCase(\n",
    "        #    identifier=\"input format validation success\",\n",
    "        #    goal=\"Model input format must conform to specified format\",\n",
    "        #    quality_scenarios=[\"default.card-qas_007\"],\n",
    "        #    validator=String.contains(\"Model - Input Validation Pass\"),\n",
    "        #),\n",
    "        #TestCase(\n",
    "        #    identifier=\"output format validation success\",\n",
    "        #    goal=\"Model output format must conform to specified format\",\n",
    "        #    quality_scenarios=[\"default.card-qas_007\"],\n",
    "        #    validator=String.contains(\"Model - Output Validation Pass\"),\n",
    "        #),\n",
    "        # Reliability: Input Validation QAS test cases.\n",
    "        #TestCase(\n",
    "        #    identifier=\"input format validation error\",\n",
    "        #    goal=\"Model inputs must conform to specified format\",\n",
    "        #    quality_scenarios=[\"default.card-qas_008\"],\n",
    "        #    validator=String.contains(\"Model - Input Validation Error\"),\n",
    "        #),\n",
    "        #  Analyzability QAS test cases.\n",
    "        #TestCase(\n",
    "        #    identifier=\"detect ood inputs\",\n",
    "        #    goal=\"Monitor inputs for OOD data and unexpected shifts\",\n",
    "        #    quality_scenarios=[\"default.card-qas_009\"],\n",
    "        #    validator=String.contains(\"Model - Input OOD Error\"),\n",
    "        #), \n",
    "        #Monitorability QAS test case\n",
    "        #TestCase(\n",
    "        #    identifier=\"monitor output confidence shift\",\n",
    "        #    goal=\"Monitor inputs for OOD data and unexpected shifts\",\n",
    "        #    quality_scenarios=[\"default.card-qas_010\"],\n",
    "        #    validator=String.contains(\"Model - Output Confidence Error\"),\n",
    "        #),\n",
    "        #Time Behaviour \n",
    "        #TestCase(\n",
    "        #    identifier=\"predicting cpu time\",\n",
    "        #    goal=\"Check cpu time used while predicting\",\n",
    "        #    quality_scenarios=[\"default.card-qas_011\"],\n",
    "        #    validator=Real.less_than(2.0, Units.second),\n",
    "        #),\n",
    "        #Repeatability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"repeated results sampling\",\n",
    "            goal=\"Repeatedly sampling results gives same results\",\n",
    "            quality_scenarios=[\"default.card-qas_012\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 ),\n",
    "        ),\n",
    "        #Reproducability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"repeated training on training samples\",\n",
    "            goal=\"Repeatedly training on different sammples of training data gives same results on test data set\",\n",
    "            quality_scenarios=[\"default.card-qas_013\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 ),\n",
    "        ),\n",
    "        #Domain Adaptability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"running in new domain\",\n",
    "            goal=\"Repeatedly training on different sammples of training data gives same results on test data set\",\n",
    "            quality_scenarios=[\"default.card-qas_014\"],\n",
    "            validator=MultipleRanksums.all_p_values_greater_or_equal_than(0.05 ),\n",
    "        ),\n",
    "        #Testability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"test results from dev and op env\",\n",
    "            goal=\"aligment of test results from dev and op environments\",\n",
    "            quality_scenarios=[\"default.card-qas_015\"],\n",
    "            validator=Validator.build_info_validator(\"Inspect the alignment of results for no more than 0.25% difference.\"),\n",
    "        ),\n",
    "        #Understandability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"understanding design choices\",\n",
    "            goal=\"understanding design and implementation choices\",\n",
    "            quality_scenarios=[\"default.card-qas_016\"],\n",
    "            validator=Validator.build_info_validator(\"Inspect projrct code and documentation.\"),\n",
    "        ),\n",
    "        #Maintainability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"keep ML component up to date\",\n",
    "            goal=\"keep trained ML component up to date with op environment changes\",\n",
    "            quality_scenarios=[\"default.card-qas_017\"],\n",
    "            validator=Validator.build_info_validator(\"Validate work time less than 8hrs.\"),\n",
    "        ),\n",
    "        #Modifiability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"update data pipelines\",\n",
    "            goal=\"aligment of test results from dev and op environments\",\n",
    "            quality_scenarios=[\"default.card-qas_018\"],\n",
    "            validator=Validator.build_info_validator(\"Validate work time less than 4hrs.\"),\n",
    "        ),\n",
    "        #Replaceability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"update ML training algorithm\",\n",
    "            goal=\"update ML component training algorithm\",\n",
    "            quality_scenarios=[\"default.card-qas_019\"],\n",
    "            validator=Validator.build_info_validator(\"Validate train time less than 16hrs.\"),\n",
    "        ),\n",
    "        #Reuseability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"reuse ML component\",\n",
    "            goal=\"reuse ML component in new app\",\n",
    "            quality_scenarios=[\"default.card-qas_020\"],\n",
    "            validator=Validator.build_info_validator(\"Validate work time less than 4hrs.\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_suite.save(parents=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
