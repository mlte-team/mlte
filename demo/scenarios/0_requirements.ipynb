{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAS Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using Quality Attribute Scenarios as guidance for the required Properties and Conditions.\n",
    "\n",
    "NOTE: this demo requires Python 3.9 or higher to work. This demo also has an additional set of requirements than MLTE. You can install them from the file in this folder, with the command: \n",
    "\n",
    "`pip --default-timeout 1000 install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "The following are the QASs that we want to validate through the use of MLTE. The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. The system used an ML model that was trained on the flower category dataset [Nilsback 2008] (https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). \n",
    "\n",
    "* **Fairness - Model Impartial to Photo Location**\n",
    "  * The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.\n",
    "* **Robustness- Model Robust to Noise (Image Blur)**\n",
    "  * The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Robustness - Model Robust to Noise (Channel Loss)**\n",
    "  * The model receives a picture taken at a garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). The model should still be able to successfully identify the flower at the same rate as full images. Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets. Images with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Performance on Operational Platform**\n",
    "  * The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.\n",
    "* **Interpretability - Understanding Model Results**\n",
    "  * The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n",
    "\n",
    "## 1.1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "from mlte.model.shared import (\n",
    "    MetricDescriptor,\n",
    "    DataDescriptor,\n",
    "    DataClassification,\n",
    "    FieldDescriptor,\n",
    "    LabelDescriptor,\n",
    "    ModelDescriptor,\n",
    "    ModelDevelopmentDescriptor,\n",
    "    ModelResourcesDescriptor,\n",
    "    ModelProductionDescriptor,\n",
    "    ModelInterfaceDescriptor,\n",
    "    ModelIODescriptor,\n",
    "    QASDescriptor,\n",
    ")\n",
    "from mlte.negotiation.model import (\n",
    "    SystemDescriptor,\n",
    "    GoalDescriptor,\n",
    "    ProblemType,\n",
    "    RiskDescriptor,\n",
    ")\n",
    "\n",
    "card = NegotiationCard(\n",
    "    system=SystemDescriptor(\n",
    "        goals=[\n",
    "            GoalDescriptor(\n",
    "                description=\"The model should perform well.\",\n",
    "                metrics=[\n",
    "                    MetricDescriptor(\n",
    "                        description=\"accuracy\",\n",
    "                        baseline=\"Better than random chance.\",\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        problem_type=ProblemType.CLASSIFICATION,\n",
    "        task=\"Flower Classification\",\n",
    "        usage_context=\"A handheld flower identification device.\",\n",
    "        risks=RiskDescriptor(\n",
    "            fp=\"The wrong type of flower is identified.\",\n",
    "            fn=\"The flower is not identified.\",\n",
    "            other=\"N/A\",\n",
    "        ),\n",
    "    ),\n",
    "    data=[\n",
    "        DataDescriptor(\n",
    "            description=\"Oxford flower dataset.\",\n",
    "            classification=DataClassification.UNCLASSIFIED,\n",
    "            access=\"None\",\n",
    "            labeling_method=\"by hand\",\n",
    "            fields=[\n",
    "                FieldDescriptor(\n",
    "                    name=\"Sepal length\",\n",
    "                    description=\"The length of the sepal.\",\n",
    "                    type=\"float\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                )\n",
    "            ],\n",
    "            labels=[\n",
    "                LabelDescriptor(description=\"Setosa\", percentage=30.0),\n",
    "                LabelDescriptor(description=\"Versicolour\", percentage=30.0),\n",
    "                LabelDescriptor(description=\"Virginica\", percentage=40.0),\n",
    "            ],\n",
    "            policies=\"N/A\",\n",
    "            rights=\"N/A\",\n",
    "            source=\"https://archive.ics.uci.edu/dataset/53/iris\",\n",
    "        )\n",
    "    ],\n",
    "    model=ModelDescriptor(\n",
    "        development=ModelDevelopmentDescriptor(\n",
    "            resources=ModelResourcesDescriptor(\n",
    "                cpu=\"1\", gpu=\"0\", memory=\"6MiB\", storage=\"2KiB\"\n",
    "            )\n",
    "        ),\n",
    "        production=ModelProductionDescriptor(\n",
    "            deployment_platform=\"local server\",\n",
    "            capability_deployment_mechanism=\"API\",\n",
    "            interface=ModelInterfaceDescriptor(\n",
    "                input=ModelIODescriptor(\n",
    "                    name=\"i1\", description=\"description\", type=\"Vector[150]\"\n",
    "                ),\n",
    "                output=ModelIODescriptor(\n",
    "                    name=\"o1\", description=\"description\", type=\"Vector[3]\"\n",
    "                ),\n",
    "            ),\n",
    "            resources=ModelResourcesDescriptor(\n",
    "                cpu=\"1\",\n",
    "                gpu=\"0\",\n",
    "                memory=\"6MiB\",\n",
    "                storage=\"2KiB\",\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    qas=[\n",
    "        QASDescriptor(\n",
    "            quality=\"Fairness - Model Impartial to Photo Location\",\n",
    "            stimulus=\"The model receives a picture taken at the garden.\",\n",
    "            source=\"Provider of the new picture.\",\n",
    "            environment=\"Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from.\",\n",
    "            response=\"Model can correctly identify the correct flowers.\",\n",
    "            measure=\"The total accuracy of the model across each garden population should be higher or equal to 0.9.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Robustness to Noise (Image Blur)\",\n",
    "            stimulus=\"The model receives a picture taken at the garden which is a bit blurry.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"Test data needs to include blurred flower images. Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur.\",\n",
    "            response=\"The model should still be able to successfully identify the flower at the same rate as non-blurry images.\",\n",
    "            measure=\"This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Robustness to Noise (Channel Loss)\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). \",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets.\",\n",
    "            response=\"The model should still be able to successfully identify the flower at the same rate as full images.\",\n",
    "            measure=\"This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Performance on Operational Platform\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device. The model will need to run on this device.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"The model will not exceed the limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively) available.\",\n",
    "            measure=(\n",
    "                \"1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measured using ps.\"\n",
    "                + \"2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap.\"\n",
    "                + \"3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.\"\n",
    "            ),\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Interpretability - Understanding Model Results\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. \",\n",
    "            measure=\"The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "card.save(force=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define a Specification\n",
    "\n",
    "In the first phase of SDMT, we define a `Specification` that represents the requirements the completed model must meet in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define requirements by constructing a specification (`Spec`). For each property, we define the validations to perform as well. Note that several new `Value` types (`MultipleAccuracy`, `RankSums`, `MultipleRanksums`) had to be created to define the validation methods that will validate each Condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "\n",
    "# The Properties we want to validate, associated with our scenarios.\n",
    "from mlte.property.costs.storage_cost import StorageCost\n",
    "from mlte.property.fairness.fairness import Fairness\n",
    "from mlte.property.robustness.robustness import Robustness\n",
    "from mlte.property.interpretability.interpretability import Interpretability\n",
    "from mlte.property.costs.predicting_memory_cost import PredictingMemoryCost\n",
    "from mlte.property.costs.predicting_compute_cost import PredictingComputeCost\n",
    "\n",
    "# The Value types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from mlte.value.types.image import Image\n",
    "from values.multiple_accuracy import MultipleAccuracy\n",
    "from values.ranksums import RankSums\n",
    "from values.multiple_ranksums import MultipleRanksums\n",
    "\n",
    "# The full spec. Note that the Robustness Property contains conditions for both Robustness scenarios.\n",
    "spec = Spec(\n",
    "    properties={\n",
    "        Fairness(\n",
    "            \"Important check if model performs well accross different populations\"\n",
    "        ): {\n",
    "            \"accuracy across gardens\": MultipleAccuracy.all_accuracies_more_or_equal_than(\n",
    "                0.9\n",
    "            )\n",
    "        },\n",
    "        Robustness(\"Robust against blur and noise\"): {\n",
    "            \"ranksums blur2x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur5x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur0x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"multiple ranksums for clade2\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "            \"multiple ranksums between clade2 and 3\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "        },\n",
    "        StorageCost(\"Critical since model will be in an embedded device\"): {\n",
    "            \"model size\": LocalObjectSize.value().less_than(3000)\n",
    "        },\n",
    "        PredictingMemoryCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting memory\": LocalProcessMemoryConsumption.value().average_consumption_less_than(\n",
    "                512000.0\n",
    "            )\n",
    "        },\n",
    "        PredictingComputeCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting cpu\": LocalProcessCPUUtilization.value().max_utilization_less_than(\n",
    "                30.0\n",
    "            )\n",
    "        },\n",
    "        Interpretability(\"Important to understand what the model is doing\"): {\n",
    "            \"image attributions\": Image.ignore(\"Inspect the image.\")\n",
    "        },\n",
    "    }\n",
    ")\n",
    "spec.save(parents=True, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
