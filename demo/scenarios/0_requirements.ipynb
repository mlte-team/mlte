{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAS Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using Quality Attribute Scenarios as guidance for the required Properties and Conditions.\n",
    "\n",
    "NOTE: this demo has an additional set of requirements than MLTE. You can install them with the command: \n",
    "\n",
    "`poetry install --with demo`\n",
    "\n",
    "If running on macOS, also install:\n",
    "\n",
    "`poetry install --with demo-mac`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "The following are the QASs that we want to validate through the use of MLTE. The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. The system used an ML model that was trained on the flower category dataset [Nilsback 2008] (https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). \n",
    "\n",
    "* **Fairness - Model Impartial to Photo Location**\n",
    "  * The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.\n",
    "* **Robustness- Model Robust to Noise (Image Blur)**\n",
    "  * The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Robustness - Model Robust to Noise (Channel Loss)**\n",
    "  * The model receives a picture taken at a garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). The model should still be able to successfully identify the flower at the same rate as full images. Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets. Images with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Performance on Operational Platform**\n",
    "  * The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.\n",
    "* **Interpretability - Understanding Model Results**\n",
    "  * The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements\n",
    "\n",
    "## 1.1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: '_mexican_aster', 1: 'alpine_sea_holly', 2: 'anthurium', 3: 'artichoke', 4: 'arum_lily', 5: 'azalea', 6: 'ball_moss', 7: 'ballon_flower', 8: 'barberton_daisy', 9: 'bearded_iris', 10: 'bee_balm', 11: 'bird_of_paradise', 12: 'bishop_of_llandaf_dahlia', 13: 'black-eyed_susan', 14: 'blackberry_lily', 15: 'blanket_flower', 16: 'bolero_deep_blue', 17: 'bougainvillea', 18: 'bromelia', 19: 'buttercup', 20: 'california_poppy', 21: 'camellia', 22: 'canna_lily', 23: 'canterbury_bells', 24: 'cape_flower', 25: 'carnation', 26: 'cattleya', 27: 'cautleya_spicata', 28: 'clematis', 29: 'coltsfoot', 30: 'columbine', 31: 'common_dandelion', 32: 'corn_poppy', 33: 'cyclamen', 34: 'daffodil', 35: 'dahlia', 36: 'desert_rose', 37: 'english_marigold', 38: 'fire_lily', 39: 'foxglove', 40: 'frangipani', 41: 'fritillary', 42: 'garden_phlox', 43: 'gaura', 44: 'gazania', 45: 'geranium', 46: 'globe_flower', 47: 'globe_thistle', 48: 'grape_hyacinth', 49: 'great_masterwort', 50: 'hard-leaved_pocket_orchid', 51: 'hibiscus', 52: 'hippesatrum', 53: 'japanese_anemone', 54: 'king_protea', 55: 'lenten_rose', 56: 'lotus', 57: 'love_in_the_mist', 58: 'magnolia', 59: 'mallow', 60: 'marigold', 61: 'mexican_petunia', 62: 'monkshood', 63: 'moon_orchild', 64: 'morning_glory', 65: 'osteospermum', 66: 'oxeye_daisy', 67: 'passion_flower', 68: 'pelargonium', 69: 'peruvian_lily', 70: 'petunia', 71: 'pincushion_flower', 72: 'poinsettia', 73: 'primrose', 74: 'primula', 75: 'prince_of_whales_feather', 76: 'purple_coneflower', 77: 'red_ginger', 78: 'rose', 79: 'siam_tulip', 80: 'silverbush', 81: 'snapdragon', 82: 'spear_thistle', 83: 'spring_crocus', 84: 'stemless_gentain', 85: 'sunflower', 86: 'swear_pea', 87: 'sweet_william', 88: 'sword_lily', 89: 'thorn_apple', 90: 'tiger_lily', 91: 'tithonia_(incorrectly_labeled_as_orange_dahlia)', 92: 'toad_lily', 93: 'tree_mallow', 94: 'tree_poppy', 95: 'trumpet_creeper', 96: 'wallflower', 97: 'water_lily', 98: 'watercress', 99: 'wild_pansy', 100: 'windflower', 101: 'yellow_iris'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "from mlte.model.shared import (\n",
    "    MetricDescriptor,\n",
    "    DataDescriptor,\n",
    "    DataClassification,\n",
    "    FieldDescriptor,\n",
    "    LabelDescriptor,\n",
    "    ModelDescriptor,\n",
    "    ModelResourcesDescriptor,\n",
    "    ModelIODescriptor,\n",
    "    QASDescriptor\n",
    ")\n",
    "\n",
    "from mlte.negotiation.model import (\n",
    "    SystemDescriptor,\n",
    "    GoalDescriptor,\n",
    "    ProblemType,\n",
    "    RiskDescriptor,\n",
    ")\n",
    "\n",
    "card = NegotiationCard(\n",
    "    system=SystemDescriptor(\n",
    "        goals=[\n",
    "            GoalDescriptor(\n",
    "                description=\"The model should perform well.\",\n",
    "                metrics=[\n",
    "                    MetricDescriptor(\n",
    "                        description=\"accuracy\",\n",
    "                        baseline=\"Better than random chance.\",\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        problem_type=ProblemType.CLASSIFICATION,\n",
    "        task=\"Flower Classification\",\n",
    "        usage_context=\"A handheld flower identification device.\",\n",
    "        risks=RiskDescriptor(\n",
    "            fp=\"The wrong type of flower is identified.\",\n",
    "            fn=\"The flower is not identified.\",\n",
    "            other=\"N/A\",\n",
    "        ),\n",
    "    ),\n",
    "    data=[\n",
    "        DataDescriptor(\n",
    "            description=\"Oxford flower dataset.\",\n",
    "            classification=DataClassification.UNCLASSIFIED,\n",
    "            access=\"None\",\n",
    "            labeling_method=\"by hand\",\n",
    "            fields=[\n",
    "        FieldDescriptor(\n",
    "            name=\"filename\",\n",
    "            description=\"path to flower image.\",\n",
    "            type=\"string to png file\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Kingdom\",\n",
    "            description=\"The second highest taxonomic rank.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Phylum\",\n",
    "            description=\"The taxonomic rank below kingdom and above Clade 1.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Clade1\",\n",
    "            description=\"The taxonomic rank below Phylum and above Clade 2.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Clade2\",\n",
    "            description=\"The taxonomic rank below Clade 1 and above Clade 3.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Clade3\",\n",
    "            description=\"The taxonomic rank below Clade 2 and above Order.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Order\",\n",
    "            description=\"The taxonomic rank below Clade 3 and above Family.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Family\",\n",
    "            description=\"The taxonomic rank below Order and above Subfamily.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Subfamily\",\n",
    "            description=\"The taxonomic rank below Family and above Genus.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Genus\",\n",
    "            description=\"The taxonomic rank below Subfamily and above Species.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Common Name\",\n",
    "            description=\"Image of flower including background.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Other Name\",\n",
    "            description=\"Image of flower including background.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        ),\n",
    "        FieldDescriptor(\n",
    "            name=\"Label Name\",\n",
    "            description=\"Image Label.\",\n",
    "            type=\"string\",\n",
    "            expected_values=\"N/A\",\n",
    "            missing_values=\"N/A\",\n",
    "            special_values=\"N/A\",\n",
    "        )\n",
    "    ],\n",
    "    labels=[LabelDescriptor(description=label_dict[k],percent=0.0) for k in label_dict],\n",
    "            policies=\"N/A\",\n",
    "            rights=\"N/A\",\n",
    "            source=\"https://archive.ics.uci.edu/dataset/53/iris\",\n",
    "        )\n",
    "    ],\n",
    "    model=ModelDescriptor(\n",
    "        development_compute_resources=ModelResourcesDescriptor(\n",
    "            cpu=\"1\", gpu=\"0\", memory=\"6MiB\", storage=\"2KiB\"\n",
    "        ),\n",
    "        deployment_platform=\"local server\",\n",
    "        capability_deployment_mechanism=\"API\",\n",
    "        input_specification=[\n",
    "            ModelIODescriptor(\n",
    "                name=\"i1\", description=\"description\", type=\"Vector[150]\"\n",
    "            )\n",
    "        ],\n",
    "        output_specification=[\n",
    "            ModelIODescriptor(\n",
    "                name=\"o1\", description=\"description\", type=\"Vector[3]\"\n",
    "            )\n",
    "        ],\n",
    "        production_compute_resources=ModelResourcesDescriptor(\n",
    "            cpu=\"1\",\n",
    "            gpu=\"0\",\n",
    "            memory=\"6MiB\",\n",
    "            storage=\"2KiB\",\n",
    "        ),\n",
    "    ),\n",
    "    qas=[\n",
    "        QASDescriptor(\n",
    "            quality=\"Fairness - Model Impartial to Photo Location\",\n",
    "            stimulus=\"The model receives a picture taken at the garden.\",\n",
    "            source=\"Provider of the new picture.\",\n",
    "            environment=\"Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from.\",\n",
    "            response=\"Model can correctly identify the correct flowers.\",\n",
    "            measure=\"The total accuracy of the model across each garden population should be higher or equal to 0.9.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Robustness to Noise (Image Blur)\",\n",
    "            stimulus=\"The model receives a picture taken at the garden which is a bit blurry.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"Test data needs to include blurred flower images. Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur.\",\n",
    "            response=\"The model should still be able to successfully identify the flower at the same rate as non-blurry images.\",\n",
    "            measure=\"This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Robustness to Noise (Channel Loss)\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device. These devices are known to sometimes lose a channel (i.e., RGB channel). \",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"Test data needs to include images with a missing channel. Test images will be generated by removing the R, G and B channels in the original test data using ImageMagic, therefore producing three data sets.\",\n",
    "            response=\"The model should still be able to successfully identify the flower at the same rate as full images.\",\n",
    "            measure=\"This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Performance on Operational Platform\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device. The model will need to run on this device.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"The model will not exceed the limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively) available.\",\n",
    "            measure=(\n",
    "                \"1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measured using ps.\"\n",
    "                + \"2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap.\"\n",
    "                + \"3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.\"\n",
    "            ),\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Interpretability - Understanding Model Results\",\n",
    "            stimulus=\"The model receives a picture taken at the garden using a loaned device.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"A validation dataset with feature and label distributions broadly similar to the test dataset.\",\n",
    "            response=\"The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. \",\n",
    "            measure=\"Identify flowers correctly at least 90% of the time during normal operation.\",\n",
    "        ),\n",
    "        # Start of table 2b\n",
    "        QASDescriptor(\n",
    "            quality=\"Functional Correctness: Accuracy\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"Model can correctly identify the correct flowers.\",\n",
    "            measure=\"The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Functional Correctness: Input and Output Specification\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"By a member of the general public.\",\n",
    "            environment=\"The original test dataset can be used.\",\n",
    "            response=\"During test execution all data in the test dataset produces an output that conforms to the output specification.\",\n",
    "            measure=\"The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Resilience: Input Validation\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"Test dataset with invalid input formatting\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"The ML pipeline will create a log entry with the tag 'Model - Input Validation Error - <Input>.'\",\n",
    "            measure=\"Error present in log\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Monitorability: Detect OOD inputs \",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"Input validation dataset designed to induce known failures, generated based on Equivalence Testing and Boundary Testing.\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"Produce log entries for erroneous inputs.\",\n",
    "            measure=\"100% of errors produced by erroneous inputs are present in log files.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Monitorability: Monitor shifts in output (confidence) distribution.\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"Test dataset with known shift in output confidence.\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"Log entries are produced when distribution shift is detected.\",\n",
    "            measure=\"Confidence shift error is present in log files.\",\n",
    "        ),\n",
    "        QASDescriptor(\n",
    "            quality=\"Performance: Inference Time on Operational Platform\",\n",
    "            stimulus=\"The model receives receives a picture taken at the garden.\",\n",
    "            source=\"The origional test dataset can be used\",\n",
    "            environment=\"During normal operation\",\n",
    "            response=\"Log inference time during operation\",\n",
    "            measure=\"inference time should be less than 2 seconds.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "card.save(force=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"OxfordFlower\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define a Specification\n",
    "\n",
    "In the first phase of SDMT, we define a `Specification` that represents the requirements the completed model must meet in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define requirements by constructing a specification (`Spec`). For each property, we define the validations to perform as well. Note that several new `Value` types (`MultipleAccuracy`, `RankSums`, `MultipleRanksums`) had to be created to define the validation methods that will validate each Condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 64\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteroperability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interoperability\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# The full spec. Note that the Robustness Property contains conditions for both Robustness scenarios.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m spec \u001b[38;5;241m=\u001b[39m Spec(\n\u001b[1;32m     24\u001b[0m     properties\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     25\u001b[0m         Fairness(\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportant check if model performs well accross different populations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         ): {\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy across gardens\u001b[39m\u001b[38;5;124m\"\u001b[39m: MultipleAccuracy\u001b[38;5;241m.\u001b[39mall_accuracies_more_or_equal_than(\n\u001b[1;32m     29\u001b[0m                 \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     30\u001b[0m             )\n\u001b[1;32m     31\u001b[0m         },\n\u001b[1;32m     32\u001b[0m         Robustness(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobust against blur and noise\u001b[39m\u001b[38;5;124m\"\u001b[39m): {\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranksums blur2x8\u001b[39m\u001b[38;5;124m\"\u001b[39m: RankSums\u001b[38;5;241m.\u001b[39mp_value_greater_or_equal_to(\u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranksums blur5x8\u001b[39m\u001b[38;5;124m\"\u001b[39m: RankSums\u001b[38;5;241m.\u001b[39mp_value_greater_or_equal_to(\u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranksums blur0x8\u001b[39m\u001b[38;5;124m\"\u001b[39m: RankSums\u001b[38;5;241m.\u001b[39mp_value_greater_or_equal_to(\u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple ranksums for clade2\u001b[39m\u001b[38;5;124m\"\u001b[39m: MultipleRanksums\u001b[38;5;241m.\u001b[39mall_p_values_greater_or_equal_than(\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     38\u001b[0m             ),\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple ranksums between clade2 and 3\u001b[39m\u001b[38;5;124m\"\u001b[39m: MultipleRanksums\u001b[38;5;241m.\u001b[39mall_p_values_greater_or_equal_than(\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     41\u001b[0m             ),\n\u001b[1;32m     42\u001b[0m         },\n\u001b[1;32m     43\u001b[0m         StorageCost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCritical since model will be in an embedded device\u001b[39m\u001b[38;5;124m\"\u001b[39m): {\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel size\u001b[39m\u001b[38;5;124m\"\u001b[39m: LocalObjectSize\u001b[38;5;241m.\u001b[39mvalue()\u001b[38;5;241m.\u001b[39mless_than(\u001b[38;5;241m3000\u001b[39m)\n\u001b[1;32m     45\u001b[0m         },\n\u001b[1;32m     46\u001b[0m         PredictingMemoryCost(\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUseful to evaluate resources needed when predicting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m         ): {\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicting memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: LocalProcessMemoryConsumption\u001b[38;5;241m.\u001b[39mvalue()\u001b[38;5;241m.\u001b[39maverage_consumption_less_than(\n\u001b[1;32m     50\u001b[0m                 \u001b[38;5;241m512000.0\u001b[39m\n\u001b[1;32m     51\u001b[0m             )\n\u001b[1;32m     52\u001b[0m         },\n\u001b[1;32m     53\u001b[0m         PredictingComputeCost(\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUseful to evaluate resources needed when predicting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         ): {\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicting cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: LocalProcessCPUUtilization\u001b[38;5;241m.\u001b[39mvalue()\u001b[38;5;241m.\u001b[39mmax_utilization_less_than(\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;241m30.0\u001b[39m\n\u001b[1;32m     58\u001b[0m             )\n\u001b[1;32m     59\u001b[0m         },\n\u001b[1;32m     60\u001b[0m         Interpretability(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportant to understand what the model is doing\u001b[39m\u001b[38;5;124m\"\u001b[39m): {\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage attributions\u001b[39m\u001b[38;5;124m\"\u001b[39m: Image\u001b[38;5;241m.\u001b[39mignore(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInspect the image.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m         },\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# Functional Correctness: accuracy\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m         \u001b[43mAccuracy\u001b[49m(\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasure the overall accuracy of your end to end pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         ): {\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: Real\u001b[38;5;241m.\u001b[39mgreater_than(\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     68\u001b[0m         },\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Functional Correctness: I/O spec\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         Integratability(\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel output format must conform to specified format\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         ): {\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput format validation\u001b[39m\u001b[38;5;124m\"\u001b[39m: String\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel - Output Validation Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m         },\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# Resilience: Input Validation\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         Resilience(\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel inputs must conform to specified format\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         ): {\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput format validation\u001b[39m\u001b[38;5;124m\"\u001b[39m: String\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel - Input Validation Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m         },\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# Monitorability: Detect OOD inputs\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         Monitorability(\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonitor inputs for OOD data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         ): {\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect ood inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: String\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel - Input OOD Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m         },\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# Monitorability: Detect output shifts\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         Monitorability(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonitor outputs for unexpected shifts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         ): {\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonitor output confidence shift\u001b[39m\u001b[38;5;124m\"\u001b[39m: String\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel - Output Confidence Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m         },\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;66;03m# Performance: Inference time on Operational Platform\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         PredictingComputeCost(\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd to end latency for inference on operational platform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         ): {\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicting cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[1;32m     98\u001b[0m             Real\u001b[38;5;241m.\u001b[39mless_than(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m     99\u001b[0m         },\n\u001b[1;32m    100\u001b[0m     }\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m spec\u001b[38;5;241m.\u001b[39msave(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "\n",
    "# The Properties we want to validate, associated with our scenarios.\n",
    "from mlte.property.costs.storage_cost import StorageCost\n",
    "from mlte.property.fairness.fairness import Fairness\n",
    "from mlte.property.robustness.robustness import Robustness\n",
    "from mlte.property.interpretability.interpretability import Interpretability\n",
    "from mlte.property.costs.predicting_memory_cost import PredictingMemoryCost\n",
    "from mlte.property.costs.predicting_compute_cost import PredictingComputeCost\n",
    "\n",
    "# The Value types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from mlte.value.types.image import Image\n",
    "from values.multiple_accuracy import MultipleAccuracy\n",
    "from values.ranksums import RankSums\n",
    "from values.multiple_ranksums import MultipleRanksums\n",
    "from properties.monitorability import Monitorability\n",
    "from properties.interoperability import Interoperability\n",
    "from properties.accuracy import Accuracy\n",
    "# The full spec. Note that the Robustness Property contains conditions for both Robustness scenarios.\n",
    "spec = Spec(\n",
    "    properties={\n",
    "        Fairness(\n",
    "            \"Important check if model performs well accross different populations\"\n",
    "        ): {\n",
    "            \"accuracy across gardens\": MultipleAccuracy.all_accuracies_more_or_equal_than(\n",
    "                0.9\n",
    "            )\n",
    "        },\n",
    "        Robustness(\"Robust against blur and noise\"): {\n",
    "            \"ranksums blur2x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur5x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"ranksums blur0x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3),\n",
    "            \"multiple ranksums for clade2\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "            \"multiple ranksums between clade2 and 3\": MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05\n",
    "            ),\n",
    "        },\n",
    "        StorageCost(\"Critical since model will be in an embedded device\"): {\n",
    "            \"model size\": LocalObjectSize.value().less_than(3000)\n",
    "        },\n",
    "        PredictingMemoryCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting memory\": LocalProcessMemoryConsumption.value().average_consumption_less_than(\n",
    "                512000.0\n",
    "            )\n",
    "        },\n",
    "        PredictingComputeCost(\n",
    "            \"Useful to evaluate resources needed when predicting\"\n",
    "        ): {\n",
    "            \"predicting cpu\": LocalProcessCPUUtilization.value().max_utilization_less_than(\n",
    "                30.0\n",
    "            )\n",
    "        },\n",
    "        Interpretability(\"Important to understand what the model is doing\"): {\n",
    "            \"image attributions\": Image.ignore(\"Inspect the image.\")\n",
    "        },\n",
    "        # Functional Correctness: accuracy\n",
    "        Accuracy(\n",
    "            \"Measure the overall accuracy of your end to end pipeline\"\n",
    "        ): {\n",
    "            \"overall accuracy\": Real.greater_than(0.9)\n",
    "        },\n",
    "        # Functional Correctness: I/O spec\n",
    "        Integratability(\n",
    "            \"Model output format must conform to specified format\"\n",
    "        ): {\n",
    "            \"output format validation\": String.contains(\"Model - Output Validation Error\")\n",
    "        },\n",
    "        # Resilience: Input Validation\n",
    "        Resilience(\n",
    "            \"Model inputs must conform to specified format\"\n",
    "        ): {\n",
    "            \"input format validation\": String.contains(\"Model - Input Validation Error\")\n",
    "        },\n",
    "        # Monitorability: Detect OOD inputs\n",
    "        Monitorability(\n",
    "            \"Monitor inputs for OOD data\"\n",
    "        ): {\n",
    "            \"detect ood inputs\": String.contains(\"Model - Input OOD Error\")\n",
    "        },\n",
    "        # Monitorability: Detect output shifts\n",
    "        Monitorability(\n",
    "            \"Monitor outputs for unexpected shifts\"\n",
    "        ): {\n",
    "            \"Monitor output confidence shift\": String.contains(\"Model - Output Confidence Error\")\n",
    "        },\n",
    "        # Performance: Inference time on Operational Platform\n",
    "        PredictingComputeCost(\n",
    "            \"End to end latency for inference on operational platform\"\n",
    "        ): {\n",
    "            \"predicting cpu\": \n",
    "            Real.less_than(2.0)\n",
    "        },\n",
    "    }\n",
    ")\n",
    "spec.save(parents=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
