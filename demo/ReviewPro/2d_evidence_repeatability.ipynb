{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685c7082-2a1d-4219-9d0f-5148603b947a",
   "metadata": {},
   "source": [
    "## 2d. Evidence - Repeatability QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the repeatability QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865c5ce-5b44-4517-8e68-0649836c4906",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dad714-7300-4edb-9543-2c7c7fdf39c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial custom lists at URI: local:///Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/ReviewPro/../store\n",
      "Loaded 7 qa_categories for initial list\n",
      "Loaded 30 quality_attributes for initial list\n",
      "Creating sample catalog at URI: StoreType.LOCAL_FILESYSTEM:local:///Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/ReviewPro/../store\n",
      "Loading sample catalog entries.\n",
      "Loaded 9 entries for sample catalog.\n"
     ]
    }
   ],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *\n",
    "from session_LLMinfo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08b681-cef1-47d3-a53b-13f5047c616e",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adcefa87-00b0-4dc8-a8e9-04096ef3bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card.default-qas_004\n",
      "Repeatability\n",
      "ReviewPro may receive multiple entries of similarly performing employees for evaluation from  the manager  during  normal operation .  n the case of similar prompts and input information, the LLM generated employee evaluation, including performance scores and evaluation summary, should be semantically similar each time. \n"
     ]
    }
   ],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 3\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4ad37-114c-4585-a57b-7da6958bea42",
   "metadata": {},
   "source": [
    "### A Specific test case generated from the scenario:\n",
    "\n",
    "**Data and Data Source:**\tThe original test data set can be used, but will be augmented to contain repeated instances of the prompts.\n",
    "\n",
    "**Measurement and Condition:**\tThe LLM output will be analyzed to determine if the scores generated to the prompt series, which are didentical, are the same 95% of the time.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156d34c-f3ba-4ba6-a3a0-09db1d2687ac",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5dd68a6-e59f-4f70-9321-377007431321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9fe4f2-6aa1-41ab-bd6f-2db43fa8ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3.0\n",
      "1    0.0\n",
      "2    3.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    3.0\n",
      "6    0.0\n",
      "7    0.0\n",
      "Name: extractedOverallRating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Read the files with with the necessary input data and LLM evaluation results\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"2d_llm_input_repeatability.csv\")\n",
    ")\n",
    "response_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"2d_llm_output_repeatability.csv\")\n",
    ")\n",
    "response_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Preview the cleaned dataframe\n",
    "print(response_df.extractedOverallRating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3cb81-cae5-41f8-8127-3d7867da61ec",
   "metadata": {},
   "source": [
    "### Save evidence to the specified scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbb2cae-a73b-456e-b7ba-b22e0b501f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test fails with only 0.625 being the same\n"
     ]
    }
   ],
   "source": [
    "# evaluate if all results are the same\n",
    "def all_scores_equal(response_df):\n",
    "    mx = 0\n",
    "    for s in response_df.extractedOverallRating.unique():\n",
    "        n = len(response_df[response_df.extractedOverallRating == s])\n",
    "        if n > mx:\n",
    "            mx = n\n",
    "    max_val_pcent = mx / len(response_df)\n",
    "\n",
    "    return float(max_val_pcent)\n",
    "\n",
    "\n",
    "max_val_pcent = all_scores_equal(response_df)\n",
    "if max_val_pcent >= 0.95:\n",
    "    print(\n",
    "        f\"test passes with {max_val_pcent} of evaluation scores being the same\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"test fails with only {max_val_pcent} being the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01502f7b-e40c-4ed6-834c-643312b9bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArtifactModel(header=ArtifactHeaderModel(identifier='evidence.repeatable review', type='evidence', timestamp=1761930134, creator=None, level='version'), body=EvidenceModel(artifact_type=<ArtifactType.EVIDENCE: 'evidence'>, metadata=EvidenceMetadata(test_case_id='repeatable review', measurement=MeasurementMetadata(measurement_class='mlte.measurement.external_measurement.ExternalMeasurement', output_class='mlte.evidence.types.real.Real', additional_data={'function': '__main__.all_scores_equal'})), evidence_class='mlte.evidence.types.real.Real', value=RealValueModel(evidence_type=<EvidenceType.REAL: 'real'>, real=0.625, unit=None)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlte.evidence.types.real import Real\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "repeatable_measurement = ExternalMeasurement(\n",
    "    \"repeatable review\", Real, all_scores_equal\n",
    ")\n",
    "repeated_pcent = repeatable_measurement.evaluate(response_df)\n",
    "\n",
    "# Inspect value\n",
    "print(repeated_pcent)\n",
    "\n",
    "# Save to artifact store\n",
    "repeated_pcent.save(force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
