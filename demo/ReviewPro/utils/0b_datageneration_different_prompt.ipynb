{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3f2e8b-0af1-47b6-9b46-859f34c3c7cb",
   "metadata": {},
   "source": [
    "## Data Generation Notebook 2\n",
    "\n",
    "This notebook is showing how the the LLM was queried to ask to explain the evaluation scores generated by the EvalPro application. \n",
    "\n",
    "NOTE: This demo requires a `.env` file to be configured with API keys to execute. Consult [README](README.md) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c1ad9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1764cb7d-2a05-46f2-8b4c-add63a5fc929",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291e069-ba62-4622-a9f3-6983a71258f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from session import *\n",
    "\n",
    "from demo.ReviewPro.utils.evaluation_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bd3d9-bd2d-4bc7-9dd1-54f4393f2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the input files used to evaluate functional correctness:\n",
    "# Read the CSV with the correct encoding\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5abc_llm_input_functional_correctness.csv\")\n",
    ")\n",
    "output_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"5abc_llm_output_functional_correctness.csv\")\n",
    ")\n",
    "output_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Preview the cleaned dataframe\n",
    "print(input_df.columns)\n",
    "output_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff7075-a2cd-4870-a5a1-e1ac3a75ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_df = pd.merge(input_df, output_df, left_index=True, right_index=True)\n",
    "combo_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b949fe-4446-49ff-8e54-7ec0cc3ea913",
   "metadata": {},
   "source": [
    "#create a prompt asking the LLM to explain the employee overall evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066bfa9-b93a-4010-984f-e0578e18436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prompt template to ask the LLM to explain its evaluation\n",
    "prompt_template2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant to the manager of a small coffee shop.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "Assistant, you provided an overal rating of {extracted_overall_rating} based on the following inputs:\n",
    "\n",
    "Goals/objectives\n",
    "{goals_and_objectives}\n",
    "\n",
    "Employee self evaluation\n",
    "\n",
    "{self_eval}\n",
    "\n",
    "Manager comments\n",
    "\n",
    "{manager_comments}\n",
    "\n",
    "Can you explain how you arrived at that rating?\n",
    "        \n",
    "\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404b724-f648-45a4-b15e-10b26f778007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the LLM with the prompt and data\n",
    "\n",
    "chain = prompt_template2 | llm\n",
    "\n",
    "response_df2 = []\n",
    "\n",
    "for row_num, row in combo_df.iterrows():\n",
    "    # print(row.index)\n",
    "\n",
    "    pii_data = {\n",
    "        \"extracted_overall_rating\": row.extractedOverallRating,\n",
    "        \"goals_and_objectives\": row.goalsAndObjectives,\n",
    "        \"self_eval\": row.employeeSelfEval,\n",
    "        \"manager_comments\": row.managerComments,\n",
    "    }\n",
    "    prompt = prompt_template2.format(**pii_data)\n",
    "    response = chain.invoke(pii_data)\n",
    "\n",
    "    pii_data[\"response\"] = response.content\n",
    "    pii_data[\"prompt\"] = prompt\n",
    "    pii_data[\"model\"] = llm\n",
    "\n",
    "    response_df2.append(pii_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81363886-9db2-4dfa-8fce-d0d562988666",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df2 = pd.DataFrame(response_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0fcfb-1b41-4476-a1be-8b5c1820b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the responses\n",
    "response_df2.columns\n",
    "response_df2.rename(\n",
    "    columns={\n",
    "        \"goals_and_objectives\": \"goalsAndObjectives\",\n",
    "        \"self_eval\": \"employeeSelfEval\",\n",
    "        \"manager_comments\": \"managerComments\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "response_df2[\n",
    "    [\n",
    "        \"prompt\",\n",
    "        \"response\",\n",
    "        \"model\",\n",
    "        \"employeeSelfEval\",\n",
    "        \"goalsAndObjectives\",\n",
    "        \"managerComments\",\n",
    "    ]\n",
    "].to_csv(\"data/5a_output_explainability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e80ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viusualize LLM explination response\n",
    "for i in response_df2.response.tolist():\n",
    "    print(i)\n",
    "    print(\"\\n______________________\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
