{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3f2e8b-0af1-47b6-9b46-859f34c3c7cb",
   "metadata": {},
   "source": [
    "## 2b. Evidence - Functional Correctness QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the functional correctness QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8d2fc-bb07-4b56-b6d7-0eb445b7875d",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7487868-fcca-4fb2-bf4e-f9e3185e75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial custom lists at URI: local:///Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/ReviewPro/../store\n",
      "Loaded 7 qa_categories for initial list\n",
      "Loaded 30 quality_attributes for initial list\n",
      "Creating sample catalog at URI: StoreType.LOCAL_FILESYSTEM:local:///Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/ReviewPro/../store\n",
      "Loading sample catalog entries.\n",
      "Loaded 9 entries for sample catalog.\n"
     ]
    }
   ],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *\n",
    "from session_LLMinfo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe4131-15cf-41fd-8680-ac2552723d74",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14e3621-eb55-47ce-a489-f9ea9ddf6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card.default-qas_002\n",
      "Functional Correctness\n",
      "ReviewPro receives a prompt asking for an employee review from  the manager  during  normal operations .  The model outputs an employee evaluation, including an overall performance score for the employee and an evaluation for each important sub-category.  The LLM generated performance score should match the manager expected overall score in at least 95% of cases.\n"
     ]
    }
   ],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 1\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc553393-3155-4416-abaf-c74f1ab532eb",
   "metadata": {},
   "source": [
    "### A Specific test case generated from the scenario:\n",
    "\n",
    "**Data and Data Source:**\tThe LLM receives a prompt, containing the employee goals, employee statement, and manager notes, for an employee evaluation and performance score. The original test data set can be used to simulate this request.\n",
    "\n",
    "**Measurement and Condition:**\tThe LLM generated score matches the manager score for 95% of samples.\n",
    "\n",
    "**Context:**\tNormal Operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020e9e8-a137-4387-9909-60f4a042fab5",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd69f57c-59e2-4805-9ddd-7513a60d99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6cf9ab-810b-41d7-8da1-3752b65c8dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    employeeSelfEval  \\\n",
      "0  I need this job to pay my bills and I make cof...   \n",
      "1  As I reflect on my time working as a barista, ...   \n",
      "2  Over the past six months, I’ve focused on buil...   \n",
      "3  I have received positive feedback from custome...   \n",
      "4  Over the past six months, I’ve focused on buil...   \n",
      "5  During this review period, I’ve focused on wha...   \n",
      "6  Over the past review period, I’ve really enjoy...   \n",
      "\n",
      "                                     managerComments  \\\n",
      "0  Kate has shown up late to work numerous times ...   \n",
      "1  Casey has shown up late to work a few times ov...   \n",
      "2  Seems more focused on customers than on runnin...   \n",
      "3  Collin has the highest rating in customer sati...   \n",
      "4  Seems more focused on customers than on runnin...   \n",
      "5  Goals for Future Development:\\nKeep encouragin...   \n",
      "6  Strengths:\\nAnnabeth always shows up to the co...   \n",
      "\n",
      "                                  goalsAndObjectives EmployeeName  \\\n",
      "0                         Make coffee\\nEarn a living         Kate   \n",
      "1                         Make coffee\\nEarn a living        Casey   \n",
      "2  Improve cleanliness of work environment\\nLearn...      Charlie   \n",
      "3  I want to improve my average customer satisfac...       Collin   \n",
      "4  oals/objectives: - Improve cleanliness of work...         Dave   \n",
      "5                     Make coffee\\n    Earn a living         Beth   \n",
      "6                     Make coffee\\n    Earn a living     Annabeth   \n",
      "\n",
      "   correctEvalScore  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 3  \n",
      "3                 5  \n",
      "4                 3  \n",
      "5                 4  \n",
      "6                 4  \n",
      "                                    evaluationOutput  \\\n",
      "0  Employee: Kate  \\nDate and history: [Insert Da...   \n",
      "1  Employee Evaluation\\n\\nEmployee: Casey  \\nDate...   \n",
      "2  **Employee Evaluation**\\n\\n**Employee:** Charl...   \n",
      "3  Employee Evaluation\\n\\nEmployee: Collin  \\nDat...   \n",
      "4  Employee Evaluation\\n\\nEmployee: Dave  \\nDate ...   \n",
      "5  Employee: Beth  \\nDate and history: Reviewed o...   \n",
      "6  Employee Evaluation\\n\\nEmployee: Annabeth  \\nD...   \n",
      "\n",
      "                                              prompt  extractedOverallRating  \\\n",
      "0  System: You are an assistant to the manager of...                     3.0   \n",
      "1  System: You are an assistant to the manager of...                     3.0   \n",
      "2  System: You are an assistant to the manager of...                     4.0   \n",
      "3  System: You are an assistant to the manager of...                     5.0   \n",
      "4  System: You are an assistant to the manager of...                     4.0   \n",
      "5  System: You are an assistant to the manager of...                     4.0   \n",
      "6  System: You are an assistant to the manager of...                     3.0   \n",
      "\n",
      "   extractedDrinks  extractedTimeliness  extractedCustomerSatisfaction  \\\n",
      "0              3.0                  3.0                            3.0   \n",
      "1              3.0                  0.0                            3.0   \n",
      "2              3.0                  3.0                            5.0   \n",
      "3              5.0                  5.0                            5.0   \n",
      "4              3.0                  3.0                            5.0   \n",
      "5              5.0                  4.0                            4.0   \n",
      "6              3.0                  4.0                            3.0   \n",
      "\n",
      "   extractedStoreOperations  extractedOnTime extractedName modelCalled  \\\n",
      "0                       2.0              2.0          Kate      gpt-4o   \n",
      "1                       0.0              0.0         Casey      gpt-4o   \n",
      "2                       3.0              5.0    ** Charlie      gpt-4o   \n",
      "3                       5.0              5.0        Collin      gpt-4o   \n",
      "4                       3.0              4.0          Dave      gpt-4o   \n",
      "5                       3.0              3.0          Beth      gpt-4o   \n",
      "6                       4.0              5.0      Annabeth      gpt-4o   \n",
      "\n",
      "   averageScore  \n",
      "0           3.0  \n",
      "1           1.0  \n",
      "2           4.0  \n",
      "3           5.0  \n",
      "4           4.0  \n",
      "5           4.0  \n",
      "6           4.0  \n"
     ]
    }
   ],
   "source": [
    "# Read the files with with the necessary input data and LLM evaluation results\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"2abc_llm_input_functional_correctness.csv\")\n",
    ")\n",
    "results_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"2abc_llm_output_functional_correctness.csv\")\n",
    ")\n",
    "results_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Preview the cleaned dataframe\n",
    "print(input_df)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f95304-5673-4bb7-9130-27183f0f7ea5",
   "metadata": {},
   "source": [
    "### Save evidence to the specified scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd548f5a-1174-4092-b6fa-022c92024e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test fails with 0.7142857142857143 failures\n"
     ]
    }
   ],
   "source": [
    "# show percentage of incorrect results\n",
    "def evaluate_mismatch_pcent(input_df, results_df):\n",
    "    mismatches = (\n",
    "        input_df[\"correctEvalScore\"] != results_df[\"extractedOverallRating\"]\n",
    "    )\n",
    "    # print(mismatches)\n",
    "    mismatch_count = mismatches.sum()\n",
    "    data_size = len(results_df)\n",
    "    mismatch_val = mismatch_count / data_size  # * 100\n",
    "    return float(mismatch_val)\n",
    "\n",
    "\n",
    "mismatch_val = evaluate_mismatch_pcent(input_df, results_df)\n",
    "if mismatch_val < 0.05:\n",
    "    print(f\"test passes with {mismatch_val} failures\")\n",
    "else:\n",
    "    print(f\"test fails with {mismatch_val} failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f379a6-687c-430e-a8dd-13a061491271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArtifactModel(header=ArtifactHeaderModel(identifier='evidence.evaluation is correct', type='evidence', timestamp=1761930082, creator=None, level='version'), body=EvidenceModel(artifact_type=<ArtifactType.EVIDENCE: 'evidence'>, metadata=EvidenceMetadata(test_case_id='evaluation is correct', measurement=MeasurementMetadata(measurement_class='mlte.measurement.external_measurement.ExternalMeasurement', output_class='mlte.evidence.types.real.Real', additional_data={'function': '__main__.evaluate_mismatch_pcent'})), evidence_class='mlte.evidence.types.real.Real', value=RealValueModel(evidence_type=<EvidenceType.REAL: 'real'>, real=0.7142857142857143, unit=None)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlte.evidence.types.real import Real\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "mismatch_measurement = ExternalMeasurement(\n",
    "    \"evaluation is correct\", Real, evaluate_mismatch_pcent\n",
    ")\n",
    "mismatch_pcent = mismatch_measurement.evaluate(input_df, results_df)\n",
    "\n",
    "# Inspect value\n",
    "print(mismatch_pcent)\n",
    "\n",
    "# Save to artifact store\n",
    "mismatch_pcent.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe76a7-2c06-4d3f-a059-61abffb202e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
