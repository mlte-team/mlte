{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3f2e8b-0af1-47b6-9b46-859f34c3c7cb",
   "metadata": {},
   "source": [
    "## 2b. Evidence - Functional Correctness QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the functional correctness QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"LLM\", \"content generation\"],\n",
    "    \"quality_attribute\": \"Functional Correctness\",\n",
    "    \"description\": \"Evaluating if the scores generated by the LLM match the manager scores\",\n",
    "    \"inputs\": \"Manager evaluation scores, a request for a review in a crafted prompt, with the supporting material (employee statement, goals and objectives, and manager comments)\",\n",
    "    \"output\": \"A generated review, with the scores extracted for evaluation and compared to the actual manager generated evaluation scores\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8d2fc-bb07-4b56-b6d7-0eb445b7875d",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7487868-fcca-4fb2-bf4e-f9e3185e75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe4131-15cf-41fd-8680-ac2552723d74",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e3621-eb55-47ce-a489-f9ea9ddf6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 1\n",
    "print(card.quality_scenarios[qa])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc553393-3155-4416-abaf-c74f1ab532eb",
   "metadata": {},
   "source": [
    "### A Specific test case generated from the scenario:\n",
    "\n",
    "**Data and Data Source:**\tThe LLM receives a prompt, containing the employee goals, employee statement, and manager notes, for an employee evaluation and performance score. The original test data set can be used to simulate this request.\n",
    "\n",
    "**Measurement and Condition:**\tThe LLM generated score matches the manager score for 95% of samples.\n",
    "\n",
    "**Context:**\tNormal Operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020e9e8-a137-4387-9909-60f4a042fab5",
   "metadata": {},
   "source": [
    "### Gather evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69f57c-59e2-4805-9ddd-7513a60d99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cf9ab-810b-41d7-8da1-3752b65c8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files with with the necessary input data and LLM evaluation results\n",
    "input_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"2abc_llm_input_functional_correctness.csv\")\n",
    ")\n",
    "results_df = pd.read_csv(\n",
    "    os.path.join(DATASETS_DIR, \"2abc_llm_output_functional_correctness.csv\")\n",
    ")\n",
    "results_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Preview the cleaned dataframe\n",
    "print(input_df)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f95304-5673-4bb7-9130-27183f0f7ea5",
   "metadata": {},
   "source": [
    "### Save evidence to the specified scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd548f5a-1174-4092-b6fa-022c92024e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show percentage of incorrect results\n",
    "def evaluate_mismatch_pcent(input_df, results_df):\n",
    "    mismatches = (\n",
    "        input_df[\"correctEvalScore\"] != results_df[\"extractedOverallRating\"]\n",
    "    )\n",
    "    # print(mismatches)\n",
    "    mismatch_count = mismatches.sum()\n",
    "    data_size = len(results_df)\n",
    "    mismatch_val = mismatch_count / data_size  # * 100\n",
    "    return float(mismatch_val)\n",
    "\n",
    "\n",
    "mismatch_val = evaluate_mismatch_pcent(input_df, results_df)\n",
    "if mismatch_val < 0.05:\n",
    "    print(f\"test passes with {mismatch_val} failures\")\n",
    "else:\n",
    "    print(f\"test fails with {mismatch_val} failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f379a6-687c-430e-a8dd-13a061491271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.real import Real\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "mismatch_measurement = ExternalMeasurement(\n",
    "    \"evaluation is correct\", Real, evaluate_mismatch_pcent\n",
    ")\n",
    "mismatch_pcent = mismatch_measurement.evaluate(input_df, results_df)\n",
    "\n",
    "# Inspect value\n",
    "print(mismatch_pcent)\n",
    "\n",
    "# Save to artifact store\n",
    "mismatch_pcent.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe76a7-2c06-4d3f-a059-61abffb202e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
