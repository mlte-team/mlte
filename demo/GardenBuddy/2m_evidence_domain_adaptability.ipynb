{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2m. Evidence - Domain Adaptability QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Domain Adaptability scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [],\n",
    "    \"quality_attribute\": \"\",\n",
    "    \"description\": \"\",\n",
    "    \"inputs\": \"\",\n",
    "    \"output\": \"\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 13\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tThe original test data set and a test data set from the new domain.\n",
    "\n",
    "**Measurement and Condition:**\tThe effect of the new domain on model performance will be assessed using ANOVA on each label set, with significance at p-value < 0.05. \n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "\n",
    "from utils import garden\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "\n",
    "def load_data(data_folder: str, data_file: str):\n",
    "    \"\"\"Loads all garden data results and taxonomy categories.\"\"\"\n",
    "    df_results = garden.load_base_results(data_folder, data_file)\n",
    "    df_results.head()\n",
    "\n",
    "    # Load the taxonomic data and merge with results.\n",
    "    df_info = garden.load_taxonomy(data_folder)\n",
    "    df_results.rename(columns={\"label\": \"Label\"}, inplace=True)\n",
    "    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n",
    "\n",
    "    return df_info, df_all\n",
    "\n",
    "\n",
    "def load_results(data_folder: str):\n",
    "    \"\"\"loads reproducabilty test result runs\"\"\"\n",
    "    # my_folder = data_folder +\n",
    "    df_results = pd.read_csv(\n",
    "        path.join(data_folder, \"ReproducibilityDataSet_CV.csv\")\n",
    "    )\n",
    "\n",
    "    return df_results\n",
    "    # df_results = garden.load_base_results(data_folder,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\n",
    "# df_info, df_all = load_data(DATASETS_DIR)\n",
    "\n",
    "# df = pd.read_csv('ReproducibilityDataSet_CV.csv')\n",
    "\n",
    "df_info, df_test = load_data(DATASETS_DIR, \"predictions_test.csv\")\n",
    "df_info, df_new = load_data(DATASETS_DIR, \"predictions_dall-e-2.csv\")\n",
    "df_test[\"dataset\"] = \"DALL-E-2\"\n",
    "df_new[\"dataset\"] = \"Test\"\n",
    "df_all = pd.concat([df_new, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = (\n",
    "    df_all.groupby([\"Label\", \"dataset\"]).size().unstack().index.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anova_for_label(df, label):\n",
    "    # Perform ANOVA for a specific label\n",
    "    subset = df[df[\"Label\"] == label]\n",
    "    test_vals = subset[subset[\"dataset\"] == \"Test\"][\"label_prob\"]\n",
    "    dalle_vals = subset[subset[\"dataset\"] == \"DALL-E-2\"][\"label_prob\"]\n",
    "\n",
    "    f_stat, p_val = f_oneway(test_vals, dalle_vals)\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"f_stat\": f_stat,\n",
    "        \"p_val\": p_val,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_anova(df_all):\n",
    "    anova_results = [\n",
    "        run_anova_for_label(df_all, label) for label in valid_labels\n",
    "    ]\n",
    "    results_df = pd.DataFrame(anova_results)\n",
    "    results_df.sort_values(by=\"label\", inplace=True)\n",
    "    results_df.set_index(\"label\", inplace=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def run_anova2(df_all):\n",
    "    res_df = run_anova(df_all)\n",
    "\n",
    "    return res_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ANOVA\n",
    "\n",
    "results_df = run_anova(df_all)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_anova(df_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "In this example, we evaluate the output from our custom `calculate_multiple_anova` using an `ExternalMeasurement` class, and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "from evidence.multiple_ranksums import MultipleRanksums\n",
    "\n",
    "\n",
    "def calculate_multiple_anova(df_all):\n",
    "    evid: list = []\n",
    "    # print(df_all.columns)\n",
    "\n",
    "    labels = df_all.Label.unique()\n",
    "\n",
    "    for lab in labels:\n",
    "\n",
    "        subset = df_all[df_all[\"Label\"] == lab]\n",
    "        test_vals = subset[subset[\"dataset\"] == \"Test\"][\"label_prob\"]\n",
    "        dalle_vals = subset[subset[\"dataset\"] == \"DALL-E-2\"][\"label_prob\"]\n",
    "\n",
    "        # f_oneway(test_vals, dalle_vals)\n",
    "\n",
    "        anova_measurement = ExternalMeasurement(\n",
    "            f\"label {lab}\",\n",
    "            Array,\n",
    "            f_oneway,\n",
    "        )\n",
    "        anova: Array = anova_measurement.evaluate(\n",
    "            test_vals,\n",
    "            dalle_vals,\n",
    "        )\n",
    "        # print(f\"blur {blurs[i]}: {ranksum}\")\n",
    "\n",
    "        evid.append({anova.identifier: anova.array})\n",
    "    return evid\n",
    "\n",
    "\n",
    "multiple_anova_meas = ExternalMeasurement(\n",
    "    \"running in new domain\",\n",
    "    MultipleRanksums,\n",
    "    calculate_multiple_anova,\n",
    ")\n",
    "multiple_anova: MultipleRanksums = multiple_anova_meas.evaluate(df_all)\n",
    "\n",
    "multiple_anova.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
