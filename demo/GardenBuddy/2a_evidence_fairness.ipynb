{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Evidence - Fairness QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Fairness scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"Computer Vision\"],\n",
    "    \"quality_attribute\": \"Model Impartial to Photo Location\",\n",
    "    \"description\": \"The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.\",\n",
    "    \"inputs\": \"three garden populations, model results on Oxford garden data\",\n",
    "    \"output\": \"accuracy across gardens\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 0\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tTest data needs to include pictures of the flowers from the different sections, grouped by the section that the image was taken at. The quantity of the flower images should be representative of the garden section population they are taken from.\n",
    "\n",
    "**Measurement and Condition:**\tThe total accuracy of the model across each garden population should be higher or equal to 0.9.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "\n",
    "from utils import garden\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(data_folder: str):\n",
    "    \"\"\"Loads all garden data results and taxonomy categories.\"\"\"\n",
    "    df_results = garden.load_base_results(data_folder, \"predictions_test.csv\")\n",
    "    df_results.head()\n",
    "\n",
    "    # Load the taxonomic data and merge with results.\n",
    "    df_info = garden.load_taxonomy(data_folder)\n",
    "    df_results.rename(columns={\"label\": \"Label\"}, inplace=True)\n",
    "    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n",
    "\n",
    "    return df_info, df_all\n",
    "\n",
    "\n",
    "def split_data(df_info, df_all, population_size=100):\n",
    "    \"\"\"Splits the data into 3 different populations to evaluate them.\"\"\"\n",
    "    df_gardenpop = df_info.copy()\n",
    "    df_gardenpop[\"Population1\"] = (\n",
    "        np.around(\n",
    "            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n",
    "            decimals=3,\n",
    "        )\n",
    "        * population_size\n",
    "    ).astype(int)\n",
    "    df_gardenpop[\"Population2\"] = (\n",
    "        np.around(\n",
    "            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n",
    "            decimals=3,\n",
    "        )\n",
    "        * population_size\n",
    "    ).astype(int)\n",
    "    df_gardenpop[\"Population3\"] = (\n",
    "        np.around(\n",
    "            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n",
    "            decimals=3,\n",
    "        )\n",
    "        * population_size\n",
    "    ).astype(int)\n",
    "    # df_gardenpop\n",
    "    print(\"Hello\")  # df_gardenpop[\"Population2\"])\n",
    "\n",
    "    print(df_gardenpop[\"Population2\"])\n",
    "    # build populations from test data set that match the garden compositions\n",
    "    from random import choices\n",
    "\n",
    "    # build 3 gardens with populations of population_size.\n",
    "    pop_names = [\"Population1\", \"Population2\", \"Population3\"]\n",
    "    gardenpops = np.zeros((3, population_size), int)\n",
    "    gardenmems = np.zeros((3, population_size), int)\n",
    "\n",
    "    for j in range(population_size):\n",
    "        for i in range(len(df_gardenpop)):\n",
    "            my_flower = df_gardenpop.iloc[i][\"Common Name\"]\n",
    "\n",
    "            for g in range(3):\n",
    "                n_choices = df_gardenpop.iloc[i][pop_names[g]]\n",
    "                my_choices = df_all[df_all[\"Common Name\"] == my_flower][\n",
    "                    \"model correct\"\n",
    "                ].to_list()\n",
    "                # print(f\"{n_choices} {my_choices}\")\n",
    "                my_selection = choices(my_choices, k=n_choices)\n",
    "\n",
    "                gardenpops[g][j] += sum(my_selection)\n",
    "                gardenmems[g][j] += len(my_selection)\n",
    "\n",
    "    gardenpops\n",
    "\n",
    "    return gardenpops, gardenmems\n",
    "\n",
    "\n",
    "def calculate_model_performance_acc(\n",
    "    gardenpops, gardenmems, population_size=100\n",
    "):\n",
    "    \"\"\"Get accucray of models across the garden populations\"\"\"\n",
    "    gardenacc = np.zeros((3, population_size), float)\n",
    "    for i in range(population_size):\n",
    "        for g in range(3):\n",
    "            gardenacc[g][i] = gardenpops[g][i] / gardenmems[g][i]\n",
    "    gardenacc\n",
    "\n",
    "    model_performance_acc = []\n",
    "    for g in range(3):\n",
    "        avg = round(np.average(gardenacc[g][:]), 3)\n",
    "        std = round(np.std(gardenacc[g][:]), 3)\n",
    "        min = round(np.amin(gardenacc[g][:]), 3)\n",
    "        max = round(np.amax(gardenacc[g][:]), 3)\n",
    "        model_performance_acc.append(round(avg, 3))\n",
    "\n",
    "        print(\"%1d %1.3f %1.3f %1.3f %1.3f\" % (g, avg, std, min, max))\n",
    "\n",
    "    return model_performance_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\n",
    "data = load_data(DATASETS_DIR)\n",
    "print(\"Splitting Data\")\n",
    "# print(data)\n",
    "split_data = split_data(data[0], data[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "In this first example, we simply wrap the output from `accuracy_score` with a custom `Result` type to cope with the output of a third-party library that is not supported by a MLTE builtin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\n",
    "accuracy_measurement = ExternalMeasurement(\n",
    "    \"accuracy across gardens\", Array, calculate_model_performance_acc\n",
    ")\n",
    "accuracy = accuracy_measurement.evaluate(split_data[0], split_data[1])\n",
    "\n",
    "# Inspect value\n",
    "print(accuracy)\n",
    "\n",
    "# Save to artifact store\n",
    "accuracy.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
