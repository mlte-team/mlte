{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Evidence - Robustness QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Robustness scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"Computer Vision\"],\n",
    "    \"quality_attribute\": \"Robustness to Noise (Image Blur)\",\n",
    "    \"description\": \"The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\",\n",
    "    \"inputs\": \"three garden populations, model results on Oxford garden data\",\n",
    "    \"output\": \"robustness to noise\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 1\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tTest data needs to include blurred flower images.  Test blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur.\n",
    "\n",
    "**Measurement and Condition:**\tBlurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "import utils.garden as garden\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate the base model accuracy result per data label\n",
    "    df_pos = (\n",
    "        df_results[df_results[\"model correct\"] == True].groupby(\"label\").count()\n",
    "    )\n",
    "    # df_pos.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg = (\n",
    "        df_results[df_results[\"model correct\"] == False]\n",
    "        .groupby(\"label\")\n",
    "        .count()\n",
    "    )\n",
    "    # df_neg.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg.rename(columns={\"model correct\": \"model incorrect\"}, inplace=True)\n",
    "    df_res = df_pos.merge(\n",
    "        df_neg, right_on=\"label\", left_on=\"label\", how=\"outer\"\n",
    "    )\n",
    "    df_res.fillna(0, inplace=True)\n",
    "    df_res[\"model acc\"] = df_res[\"model correct\"] / (\n",
    "        df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    )\n",
    "    df_res[\"count\"] = df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    df_res.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "    df_res.head()\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def calculate_accuracy_per_set(\n",
    "    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Calculate the model accuracy per data label for each blurred data set\n",
    "    base_filename = \"predictions_test\"\n",
    "    ext_filename = \".csv\"\n",
    "    set_filename = [\"_blur2x8\", \"_blur5x8\", \"_blur0x8\"]\n",
    "\n",
    "    col_root = \"model acc\"\n",
    "\n",
    "    for fs in set_filename:\n",
    "        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n",
    "        colname = col_root + fs\n",
    "\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        df_temp = df_temp[[\"model correct\", \"label\"]]\n",
    "\n",
    "        df_pos = (\n",
    "            df_temp[df_temp[\"model correct\"] == True].groupby(\"label\").count()\n",
    "        )\n",
    "        df_neg = (\n",
    "            df_results[df_results[\"model correct\"] == False]\n",
    "            .groupby(\"label\")\n",
    "            .count()\n",
    "        )\n",
    "        df_neg.rename(\n",
    "            columns={\"model correct\": \"model incorrect\"}, inplace=True\n",
    "        )\n",
    "        df_res2 = df_pos.merge(\n",
    "            df_neg,\n",
    "            right_on=\"label\",\n",
    "            left_on=\"label\",\n",
    "            how=\"outer\",\n",
    "        ).fillna(0)\n",
    "        df_res2.fillna(0, inplace=True)\n",
    "\n",
    "        df_res2[colname] = df_res2[\"model correct\"] / (\n",
    "            df_res2[\"model correct\"] + df_res2[\"model incorrect\"]\n",
    "        )\n",
    "        df_res2.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "\n",
    "        df_res = df_res.merge(\n",
    "            df_res2, right_on=\"label\", left_on=\"label\", how=\"outer\"\n",
    "        ).fillna(0)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n",
    "    model_acc = sum(df_res[key] * df_res[\"count\"]) / sum(df_res[\"count\"])\n",
    "    print(name, model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.\n",
    "df_results = garden.load_base_results(DATASETS_DIR, \"predictions_test.csv\")\n",
    "df_results = df_results[[\"model correct\", \"label\"]]\n",
    "df_res = calculate_base_accuracy(df_results)\n",
    "df_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\n",
    "df_info = garden.load_taxonomy(DATASETS_DIR)\n",
    "df_all = garden.merge_taxonomy_with_results(df_res, df_info, \"label\", \"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Now do the actual measurements. First simply see the model accuracy across blurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view changes in model accuracy\n",
    "print_model_accuracy(df_res, \"model acc\", \"base model accuracy\")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_blur2x8\", \"model accuracy with 2x8 blur\"\n",
    ")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_blur5x8\", \"model accuracy with 5x8 blur\"\n",
    ")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_blur0x8\", \"model accuracy with 0x8 blur\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "\n",
    "my_blur = [\"2x8\", \"5x8\", \"0x8\"]\n",
    "for i in range(len(my_blur)):\n",
    "    # Define measurements.\n",
    "    ranksum_measurement = ExternalMeasurement(\n",
    "        f\"ranksums blur{my_blur[i]}\", Array, scipy.stats.ranksums\n",
    "    )\n",
    "\n",
    "    # Evaluate.\n",
    "    ranksum: Array = ranksum_measurement.evaluate(\n",
    "        df_res[\"model acc\"], df_res[f\"model acc_blur{my_blur[i]}\"]\n",
    "    )\n",
    "    print(f\"blur {my_blur[i]}: {ranksum}\")\n",
    "\n",
    "    # Inspect values\n",
    "    print(ranksum)\n",
    "\n",
    "    # Save to artifact store\n",
    "    ranksum.save(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to next part of the question- is this equal across the phylogenic groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, we will check for differences of the effect of the blur between families, using the phylohentic grouping of the plant pictures to stratify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from evidence.multiple_ranksums import MultipleRanksums\n",
    "\n",
    "# use the initial result, blur columns to anaylze effect of blur\n",
    "df_all[\"delta_2x8\"] = df_all[\"model acc\"] - df_all[\"model acc_blur2x8\"]\n",
    "df_all[\"delta_5x8\"] = df_all[\"model acc\"] - df_all[\"model acc_blur5x8\"]\n",
    "df_all[\"delta_0x8\"] = df_all[\"model acc\"] - df_all[\"model acc_blur0x8\"]\n",
    "\n",
    "pops = df_all[\"Order\"].unique().tolist()\n",
    "blurs = [\n",
    "    \"delta_2x8\",\n",
    "    \"delta_5x8\",\n",
    "    \"delta_0x8\",\n",
    "]\n",
    "\n",
    "\n",
    "def run_ranksum(samp1, samp2):\n",
    "    res = scipy.stats.ranksums(samp1, samp2)\n",
    "    float_list = [float(x) for x in res]\n",
    "    # print(float(res))\n",
    "    return float_list\n",
    "\n",
    "\n",
    "def calculate_multiple_ranksums(df_all, pops, blurs):\n",
    "    ranksums: List = []\n",
    "    for i in range(len(blurs)):\n",
    "        for p1 in range(len(pops)):  # pop1 in pops:\n",
    "            pop1 = pops[p1]\n",
    "            for p2 in range(p1, len(pops)):  # pop2 in pops:\n",
    "                pop2 = pops[p2]\n",
    "                ranksum_measurement = ExternalMeasurement(\n",
    "                    f\"ranksums Order {pop1}-{pop2} blur{blurs[i]}\",\n",
    "                    Array,\n",
    "                    run_ranksum,  # scipy.stats.ranksums,\n",
    "                )\n",
    "                ranksum: Array = ranksum_measurement.evaluate(\n",
    "                    df_all[df_all[\"Order\"] == pop1][blurs[i]],\n",
    "                    df_all[df_all[\"Order\"] == pop2][blurs[i]],\n",
    "                )\n",
    "                # print(f\"blur {blurs[i]}: {ranksum}\")\n",
    "\n",
    "                ranksums.append({ranksum.identifier: ranksum.array})\n",
    "    return ranksums\n",
    "\n",
    "\n",
    "multiple_ranksums_meas = ExternalMeasurement(\n",
    "    f\"effect of blur across families\",\n",
    "    MultipleRanksums,\n",
    "    calculate_multiple_ranksums,\n",
    ")\n",
    "multiple_ranksums: MultipleRanksums = multiple_ranksums_meas.evaluate(\n",
    "    df_all, pops, blurs\n",
    ")\n",
    "multiple_ranksums.num_pops = len(pops)\n",
    "multiple_ranksums.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
