{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GardenBuddy Demo\n",
    "\n",
    "This is a set of demo notebooks to illustrate the use of the MLTE library and SDMT process, using a flower classification model.\n",
    "\n",
    "NOTE: this demo has an additional set of requirements than MLTE. You can install them with the command: \n",
    "\n",
    "`poetry install --with demo`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. For details about setting the context, see the session.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Build/import a `NegotiationCard`\n",
    "\n",
    "In MLTE, we negotiation requirements with the help of a `NegotiationCard`. This can be done manually through code, but it is easier to use the MLTE UI to do so. Below we are copying a pre-built one that applies to this scenario. In MLTE, we define requirements by constructing a `NegotiationCard` that will include explicit Quality Attribute Scenarios with the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The examples below relate to a hypothetical system used by visitors to a botanical garden to identify flowers in the different gardens and learn more about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print basic system information in the negogation card to give context to the Quality attribute scenarios\n",
    "print(\"The goals of the system are: \")\n",
    "for goal in card.system.goals:\n",
    "    print(\"  -\", goal.description)\n",
    "    # metric_list = \"\"\n",
    "    # print(goal.metrics[0])\n",
    "    for metric in goal.metrics:\n",
    "        # metric_list = metric_list . metric.description\n",
    "        print(\"\\t as measured by: \", metric)\n",
    "print(\"The data used/to be used in training the system is:\")\n",
    "for data in card.data:\n",
    "    print(\"  -\", data.description, \" for \", data.purpose, \" from \", data.source)\n",
    "\n",
    "print(\n",
    "    \"And the usuage context for the model is that \", card.system.usage_context\n",
    ")\n",
    "\n",
    "if len(card.system.risks) > 0:\n",
    "    print(\n",
    "        \"The stakeholder perceived risks to the system are: \",\n",
    "        \"\\n.  -\".join(card.system.risks),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the QASs that we want to validate through the use of MLTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all quality attribute scenarios\n",
    "card.print_quality_scenarios()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Define a TestSuite\n",
    "\n",
    "In the first phase of SDMT, we define a `TestSuite` that represents the tests the completed model must will have to pass in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLTE, we define the tests that will be required for the different requirements in a `TestSuite`. Note that a new `Evidence` type (`MultipleRanksums`) had to be created in this case to handle the data and `Validator` for that case, and two stand-alone `Validator`s were defined in `validators.py` to validate data using existing `Evidence` types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loaded `NegotiationCard`, we can get the list of ids of its quaity attribute scenarios, that will be added to the `TestCase`s here. Those ids are the way to link the `TestCase`s to their quality attribute requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our `TestSuite`, consisting of a list of `TestCases`, each of them addressing one or more Quality Attribute Scenarios from our `NegotiationCard`. When defining the `TestCase`s below, we need to set the id of the corresponding Quality Attribute Scenario we want to test in its \"quality_scenarios\" attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.tests.test_case import TestCase\n",
    "from mlte.tests.test_suite import TestSuite\n",
    "\n",
    "# The Evidence types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.units import Units\n",
    "from mlte.measurement.memory import LocalProcessMemoryUtilization\n",
    "from mlte.evidence.types.image import Image\n",
    "from mlte.evidence.types.string import String\n",
    "from mlte.evidence.types.real import Real\n",
    "\n",
    "import utils.validators as validators\n",
    "from evidence.multiple_ranksums import MultipleRanksums\n",
    "from mlte.evidence.types.string import String\n",
    "from mlte.validation.validator import Validator\n",
    "\n",
    "\n",
    "# The full test suite.\n",
    "test_suite = TestSuite(\n",
    "    test_cases=[\n",
    "        # Fairness QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"accuracy across gardens\",\n",
    "            goal=\"Check if model performs well accross different populations\",\n",
    "            quality_scenarios=[\"card.default-qas_001\"],\n",
    "            validator=validators.all_accuracies_more_or_equal_than(0.9),\n",
    "        ),\n",
    "        # Robustness QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur2x8\",\n",
    "            goal=\"Check blur and noise for 2x8 case\",\n",
    "            quality_scenarios=[\"card.default-qas_002\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur5x8\",\n",
    "            goal=\"Check blur and noise for 5x8 case\",\n",
    "            quality_scenarios=[\"card.default-qas_002\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums blur0x8\",\n",
    "            goal=\"Check blur and noise for 0x8 case\",\n",
    "            quality_scenarios=[\"card.default-qas_002\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05 / 3),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"effect of blur across families\",\n",
    "            goal=\"Check consistency in families\",\n",
    "            quality_scenarios=[\"card.default-qas_002\"],\n",
    "            validator=MultipleRanksums.all_p_values_greater_or_equal_than(\n",
    "                0.05 / 141\n",
    "            ),\n",
    "        ),\n",
    "        # Resilience QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"ranksums channel loss R\",\n",
    "            goal=\"Check consistency between channel loss\",\n",
    "            quality_scenarios=[\"card.default-qas_003\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums channel loss G\",\n",
    "            goal=\"Check consistency between channel loss\",\n",
    "            quality_scenarios=[\"card.default-qas_003\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"ranksums channel loss B\",\n",
    "            goal=\"Check consistency between channel loss\",\n",
    "            quality_scenarios=[\"card.default-qas_003\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        # Resource Utilization QASs test cases.\n",
    "        TestCase(\n",
    "            identifier=\"model size\",\n",
    "            goal=\"Check storage consumption\",\n",
    "            quality_scenarios=[\"card.default-qas_004\"],\n",
    "            validator=LocalObjectSize.get_output_type().less_than(\n",
    "                150, Units.megabyte\n",
    "            ),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting memory\",\n",
    "            goal=\"Check memory used while predicting\",\n",
    "            quality_scenarios=[\"card.default-qas_004\"],\n",
    "            validator=LocalProcessMemoryUtilization.get_output_type().average_utilization_less_than(\n",
    "                512.0, unit=Units.megabyte\n",
    "            ),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"predicting cpu\",\n",
    "            goal=\"Check cpu % used while predicting\",\n",
    "            quality_scenarios=[\"card.default-qas_004\"],\n",
    "            validator=LocalProcessCPUUtilization.get_output_type().max_utilization_less_than(\n",
    "                30.0, unit=Units.percent\n",
    "            ),\n",
    "        ),\n",
    "        # Understandability QAS test case.\n",
    "        TestCase(\n",
    "            identifier=\"image attributions\",\n",
    "            goal=\"Check what the model is doing\",\n",
    "            quality_scenarios=[\"card.default-qas_005\"],\n",
    "            validator=Image.register_info(\"Inspect the image.\"),\n",
    "        ),\n",
    "        # Functional Correctness - Accuracy QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"overall model accuracy\",\n",
    "            goal=\"Measure the overall accuracy of your end to end pipeline\",\n",
    "            quality_scenarios=[\"card.default-qas_006\"],\n",
    "            validator=Real.greater_than(0.9),\n",
    "        ),\n",
    "        # Functional Correctness - I/O spec QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"input format validation success\",\n",
    "            goal=\"Model input format must conform to specified format\",\n",
    "            quality_scenarios=[\"card.default-qas_007\"],\n",
    "            validator=String.contains(\"Model - Input Validation Pass\"),\n",
    "        ),\n",
    "        TestCase(\n",
    "            identifier=\"output format validation success\",\n",
    "            goal=\"Model output format must conform to specified format\",\n",
    "            quality_scenarios=[\"card.default-qas_007\"],\n",
    "            validator=String.contains(\"Model - Output Validation Pass\"),\n",
    "        ),\n",
    "        # Reliability: Input Validation QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"input format validation error\",\n",
    "            goal=\"Model inputs must conform to specified format\",\n",
    "            quality_scenarios=[\"card.default-qas_008\"],\n",
    "            validator=String.contains(\"Model - Input Validation Error\"),\n",
    "        ),\n",
    "        #  Analyzability QAS test cases.\n",
    "        TestCase(\n",
    "            identifier=\"detect ood inputs\",\n",
    "            goal=\"Monitor inputs for OOD data and unexpected shifts\",\n",
    "            quality_scenarios=[\"card.default-qas_009\"],\n",
    "            validator=String.contains(\"Model - Input OOD Error\"),\n",
    "        ),\n",
    "        # Monitorability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"monitor output confidence shift\",\n",
    "            goal=\"Monitor inputs for OOD data and unexpected shifts\",\n",
    "            quality_scenarios=[\"card.default-qas_010\"],\n",
    "            validator=String.contains(\"Model - Output Confidence Error\"),\n",
    "        ),\n",
    "        # Time Behaviour\n",
    "        TestCase(\n",
    "            identifier=\"predicting cpu time\",\n",
    "            goal=\"Check cpu time used while predicting\",\n",
    "            quality_scenarios=[\"card.default-qas_011\"],\n",
    "            validator=Real.less_than(2.0, Units.second),\n",
    "        ),\n",
    "        # Repeatability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"repeated results sampling\",\n",
    "            goal=\"Repeatedly sampling results gives same results\",\n",
    "            quality_scenarios=[\"card.default-qas_012\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        # Reproducibility QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"repeated training on training samples\",\n",
    "            goal=\"Repeatedly training on different sammples of training data gives same results on test data set\",\n",
    "            quality_scenarios=[\"card.default-qas_013\"],\n",
    "            validator=validators.p_value_greater_or_equal_to(0.05),\n",
    "        ),\n",
    "        # Domain Adaptability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"running in new domain\",\n",
    "            goal=\"Repeatedly training on different sammples of training data gives same results on test data set\",\n",
    "            quality_scenarios=[\"card.default-qas_014\"],\n",
    "            validator=MultipleRanksums.all_p_values_greater_or_equal_than(0.05),\n",
    "        ),\n",
    "        # Testability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"test results from dev and op env\",  # test results from dev and op env\n",
    "            goal=\"aligment of test results from dev and op environments\",\n",
    "            quality_scenarios=[\"card.default-qas_015\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Inspect the alignment of results for no more than 0.25% difference.\"\n",
    "            ),\n",
    "        ),\n",
    "        # Understandability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"understanding design choices\",\n",
    "            goal=\"understanding design and implementation choices\",\n",
    "            quality_scenarios=[\"card.default-qas_016\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Inspect project code and documentation.\"\n",
    "            ),\n",
    "        ),\n",
    "        # Maintainability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"keep ML component up to date\",\n",
    "            goal=\"keep trained ML component up to date with op environment changes\",\n",
    "            quality_scenarios=[\"card.default-qas_017\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Validate work time less than 8hrs.\"\n",
    "            ),\n",
    "        ),\n",
    "        # Modifiability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"update data pipelines\",\n",
    "            goal=\"aligment of test results from dev and op environments\",\n",
    "            quality_scenarios=[\"card.default-qas_018\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Validate work time less than 4hrs.\"\n",
    "            ),\n",
    "        ),\n",
    "        # Replaceability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"update ML training algorithm\",\n",
    "            goal=\"update ML component training algorithm\",\n",
    "            quality_scenarios=[\"card.default-qas_019\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Validate train time less than 16hrs.\"\n",
    "            ),\n",
    "        ),\n",
    "        # Retrainability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"retrain ML model\",\n",
    "            goal=\"update ML component through training\",\n",
    "            quality_scenarios=[\"card.default-qas_020\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Model takes less than 1 hr to retrain.\"\n",
    "            ),\n",
    "        ),\n",
    "        # Reuseability QAS test case\n",
    "        TestCase(\n",
    "            identifier=\"reuse ML component\",\n",
    "            goal=\"reuse ML component in new app\",\n",
    "            quality_scenarios=[\"card.default-qas_021\"],\n",
    "            validator=Validator.build_info_validator(\n",
    "                \"Validate work time less than 4hrs.\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_suite.save(parents=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
