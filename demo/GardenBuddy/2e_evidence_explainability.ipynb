{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e. Evidence - Explainability QAS Measurements.\n",
    "\n",
    "Now we proceed to gather data about the Interpretability of the model, for the corresponding scenario.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"Computer Vision\",\"Object detection\"],\n",
    "    \"quality_attribute\": \"explainability\",\n",
    "    \"description\": \"The model should return, when prompted, a image highlighting the most informative features to identifying the flower \",\n",
    "    \"inputs\": \"Garden flower image\",\n",
    "    \"output\": \"Image with informative features\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 4\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tThe original test data set can be used.\n",
    "\n",
    "**Measurement and Condition:**\tThe model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference. \n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Functions to help, and data setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_analysis import *\n",
    "\n",
    "# Load the model/\n",
    "loaded_model = load_model(MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show the image.\n",
    "\n",
    "flower_img = \"flower3.jpg\"  # Filename of flower image to use, public domain image adapted from: https://commons.wikimedia.org/wiki/File:Beautiful_white_flower_in_garden.jpg\n",
    "flower_idx = (\n",
    "    42  # Classifier index of associated flower (see OxfordFlower102Labels.csv)\n",
    ")\n",
    "\n",
    "im = read_image(os.path.join(SAMPLE_DATASET_DIR, flower_img))\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = run_model(im, loaded_model)\n",
    "\n",
    "baseline, alphas = generate_baseline_and_alphas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_images = interpolate_images(\n",
    "    baseline=baseline, image=im, alphas=alphas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "i = 0\n",
    "for alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n",
    "    i += 1\n",
    "    plt.subplot(1, len(alphas[0::10]), i)\n",
    "    plt.title(f\"alpha: {alpha:.1f}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gradients = compute_gradients(\n",
    "    loaded_model=loaded_model,\n",
    "    images=interpolated_images,\n",
    "    target_class_idx=flower_idx,\n",
    ")\n",
    "print(path_gradients.shape)\n",
    "\n",
    "ig = integral_approximation(gradients=path_gradients)\n",
    "print(ig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_attributions = integrated_gradients(\n",
    "    baseline=baseline,\n",
    "    image=im,\n",
    "    target_class_idx=flower_idx,\n",
    "    loaded_model=loaded_model,\n",
    "    m_steps=240,\n",
    ")\n",
    "print(ig_attributions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Execute and store measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_img_attributions(\n",
    "    image=im,\n",
    "    baseline=baseline,\n",
    "    target_class_idx=flower_idx,\n",
    "    loaded_model=loaded_model,\n",
    "    m_steps=240,\n",
    "    cmap=plt.cm.inferno,\n",
    "    overlay_alpha=0.4,\n",
    ")\n",
    "\n",
    "plt.savefig(MEDIA_DIR / \"attributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "from mlte.evidence.types.image import Image\n",
    "\n",
    "# Save to MLTE store.\n",
    "img_collector = ExternalMeasurement(\"image attributions\", Image)\n",
    "img = img_collector.evaluate(MEDIA_DIR / \"attributions.png\")\n",
    "img.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
