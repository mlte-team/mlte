{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85ee8a3c-5dac-409c-a06d-19a9e88df67b",
   "metadata": {},
   "source": [
    "## 2c. Evidence - Resilience QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Robustness scenarios defined in the previous step. Note that some functions will be loaded from external Python files.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3982ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"Computer Vision\", ,\"Object detection\"],\n",
    "    \"quality_attribute\": \"resilience\",\n",
    "    \"description\": \"The model performance across different populations will be the same. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.ÃŸ\",\n",
    "    \"inputs\": \"Garden flower images with different channels missing\",\n",
    "    \"output\": \"p-value fron Wilcoxon Rank Sum test evaluating differences in populations (in this case, images w different channel loss)\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa94b9df-9aab-4d9a-9eb4-166b7d07e882",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7667837-9d5e-47fe-a91f-806920593eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7eb644",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 2\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a7b50",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tTest data needs to include images with a missing channel. Test images will be generated by removing the R, G, and B channels in the original test data using ImageMagick, therefore producing three data sets.\n",
    "\n",
    "**Measurement and Condition:**\tImages with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b41342-15f5-4504-a9f3-b613251256e3",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a4471-b618-44a3-b845-c56908b97d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "import utils.garden as garden\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate the base model accuracy result per data label\n",
    "    df_pos = (\n",
    "        df_results[df_results[\"model correct\"] == True].groupby(\"label\").count()\n",
    "    )\n",
    "\n",
    "    df_neg = (\n",
    "        df_results[df_results[\"model correct\"] == False]\n",
    "        .groupby(\"label\")\n",
    "        .count()\n",
    "    )\n",
    "    # df_neg.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg.rename(columns={\"model correct\": \"model incorrect\"}, inplace=True)\n",
    "    df_res = df_pos.merge(\n",
    "        df_neg, right_on=\"label\", left_on=\"label\", how=\"outer\"\n",
    "    )\n",
    "    df_res.fillna(0, inplace=True)\n",
    "    df_res[\"model acc\"] = df_res[\"model correct\"] / (\n",
    "        df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    )\n",
    "    df_res[\"count\"] = df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    df_res.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "    df_res.head()\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def calculate_accuracy_per_set(\n",
    "    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Calculate the model accuracy per data label for each blurred data set\n",
    "    base_filename = \"0c_cv_output_resilience\"\n",
    "    ext_filename = \".csv\"\n",
    "    set_filename = [\"_noR\", \"_noG\", \"_noB\"]\n",
    "\n",
    "    col_root = \"model acc\"\n",
    "\n",
    "    for fs in set_filename:\n",
    "        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n",
    "        colname = col_root + fs\n",
    "\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        df_temp = df_temp[[\"model correct\", \"label\"]]\n",
    "\n",
    "        df_pos = (\n",
    "            df_temp[df_temp[\"model correct\"] == True].groupby(\"label\").count()\n",
    "        )\n",
    "        df_neg = (\n",
    "            df_results[df_results[\"model correct\"] == False]\n",
    "            .groupby(\"label\")\n",
    "            .count()\n",
    "        )\n",
    "        df_neg.rename(\n",
    "            columns={\"model correct\": \"model incorrect\"}, inplace=True\n",
    "        )\n",
    "        df_res2 = df_pos.merge(\n",
    "            df_neg,\n",
    "            right_on=\"label\",\n",
    "            left_on=\"label\",\n",
    "            how=\"outer\",\n",
    "        ).fillna(0)\n",
    "        df_res2.fillna(0, inplace=True)\n",
    "\n",
    "        df_res2[colname] = df_res2[\"model correct\"] / (\n",
    "            df_res2[\"model correct\"] + df_res2[\"model incorrect\"]\n",
    "        )\n",
    "        df_res2.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "\n",
    "        df_res = df_res.merge(\n",
    "            df_res2, right_on=\"label\", left_on=\"label\", how=\"outer\"\n",
    "        ).fillna(0)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n",
    "    model_acc = sum(df_res[key] * df_res[\"count\"]) / sum(df_res[\"count\"])\n",
    "    print(name, model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc413466-7c47-459d-9a85-1a5fd7b06a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.\n",
    "df_results = garden.load_base_results(DATASETS_DIR, \"0abcflmn_cv_output.csv\")\n",
    "df_results = df_results[[\"model correct\", \"label\"]]\n",
    "df_res = calculate_base_accuracy(df_results)\n",
    "df_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\n",
    "df_info = garden.load_taxonomy(DATASETS_DIR)\n",
    "df_all = garden.merge_taxonomy_with_results(df_res, df_info, \"label\", \"Label\")\n",
    "\n",
    "# fill in missing model accuracy data\n",
    "df_all[\"model acc_noR\"] = df_all[\"model acc_noR\"].fillna(0)\n",
    "df_all[\"model acc_noG\"] = df_all[\"model acc_noG\"].fillna(0)\n",
    "df_all[\"model acc_noB\"] = df_all[\"model acc_noB\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7eae2-25e2-4420-8dd3-c75a84b6aea4",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Now do the actual measurements. First simply see the model accuracy across channel loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d5617-9ce2-46d0-99a4-52d299d72605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view changes in model accuracy\n",
    "print_model_accuracy(df_res, \"model acc\", \"base model accuracy\")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_noR\", \"model accuracy with no red channel\"\n",
    ")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_noG\", \"model accuracy with no green channel\"\n",
    ")\n",
    "print_model_accuracy(\n",
    "    df_res, \"model acc_noB\", \"model accuracy with no blue channel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155dba1d-bdd1-4cd3-93d9-df9d5e33f931",
   "metadata": {},
   "source": [
    "Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8782b15-fb65-42e0-9066-f654ed806174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "\n",
    "def run_ranksum(samp1, samp2):\n",
    "    res = scipy.stats.ranksums(samp1, samp2)\n",
    "    float_list = [float(x) for x in res]\n",
    "    # print(float(res))\n",
    "    return float_list\n",
    "\n",
    "\n",
    "my_blur = [\"R\", \"G\", \"B\"]\n",
    "for i in range(len(my_blur)):\n",
    "    # Define measurements.\n",
    "    ranksum_measurement = ExternalMeasurement(\n",
    "        f\"ranksums channel loss {my_blur[i]}\", Array, scipy.stats.ranksums\n",
    "    )\n",
    "\n",
    "    # Evaluate.\n",
    "    ranksum: Array = ranksum_measurement.evaluate(\n",
    "        df_res[\"model acc\"], df_res[f\"model acc_no{my_blur[i]}\"]\n",
    "    )\n",
    "    print(f\"blur {my_blur[i]}: {ranksum}\")\n",
    "\n",
    "    # Inspect values\n",
    "    print(ranksum)\n",
    "\n",
    "    # Save to artifact store\n",
    "    ranksum.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a4f02-1ad8-412f-b015-75a4af888e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
