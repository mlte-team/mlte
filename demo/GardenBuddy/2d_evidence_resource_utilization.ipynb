{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Evidence - Resource Utilization QAS Measurements\n",
    "\n",
    "Now we collect stored, CPU and memory usage data when predicting with the model, for the Performance scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [],\n",
    "    \"quality_attribute\": \"\",\n",
    "    \"description\": \"\",\n",
    "    \"inputs\": \"\",\n",
    "    \"output\": \"\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial custom lists at URI: local:///Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/../store\n",
      "Loaded 7 qa_categories for initial list\n",
      "Loaded 30 quality_attributes for initial list\n",
      "Creating sample catalog at URI: StoreType.LOCAL_FILESYSTEM:local:///Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/../store\n",
      "Loading sample catalog entries.\n",
      "Loaded 9 entries for sample catalog.\n"
     ]
    }
   ],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card.default-qas_004\n",
      "Resource  Utilization\n",
      "The model running on the loaned device receives pictures taken at the garden from  the Garden Buddy application  during  normal operations .  The loaned devices are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 150 MB, respectively). The model executes on the loaned device without any errors due to unavailable resources\n"
     ]
    }
   ],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 3\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tThe original test dataset can be used (i.e., original test data set aside for testing during data split into training, validation, and test).\n",
    "\n",
    "**Measurement and Condition:**\t    \n",
    "    1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps.\n",
    "    2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap.\n",
    "    3- Disk usage will not exceed allocated disk space of 150 MB bytes. This will be measured by adding the size of each file in the path for the model code.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Prepare and execute measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206705557.0 byte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArtifactModel(header=ArtifactHeaderModel(identifier='evidence.model size', type='evidence', timestamp=1762179519, creator=None, level='version'), body=EvidenceModel(artifact_type=<ArtifactType.EVIDENCE: 'evidence'>, metadata=EvidenceMetadata(test_case_id='model size', measurement=MeasurementMetadata(measurement_class='mlte.measurement.storage.local_object_size.LocalObjectSize', output_class='mlte.evidence.types.real.Real', additional_data={})), evidence_class='mlte.evidence.types.real.Real', value=RealValueModel(evidence_type=<EvidenceType.REAL: 'real'>, real=206705557.0, unit='byte')))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.evidence.types.real import Real\n",
    "from mlte.measurement.units import Units\n",
    "\n",
    "store_measurement = LocalObjectSize(\"model size\")\n",
    "size: Real = store_measurement.evaluate(MODELS_DIR, unit=Units.byte)\n",
    "print(size)\n",
    "size.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting log\n",
      "TensorFlow version: 2.18.0\n",
      "Found 1 files.\n",
      "Size of model file (/Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/model/oxford_flower_model.keras): 206705544 bytes\n",
      "Memory used for the entire model loading process: 555089920 bytes.\n",
      "Running inference on 1 samples...\n",
      "(1, 224, 224, 3)\n",
      "Model - Input Validation Pass - RGB image loaded\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518ms/step\n",
      "Model - Output Validation Pass - (1, 102) - validated\n",
      "Model - file: /Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/data/sample predicted to be lotus (class [55]) with confidence [0.02620832]\n",
      "\n",
      "--- STATISTICS ---\n",
      "Average elapsed time per inference: 0.56560 seconds\n",
      "Average memory used per inference: 41467904.00000 bytes.\n",
      "Average: 0.5796666666666667 percent\n",
      "Minimum: 0.0 percent\n",
      "Maximum: 4.8469999999999995 percent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArtifactModel(header=ArtifactHeaderModel(identifier='evidence.predicting cpu', type='evidence', timestamp=1762179543, creator=None, level='version'), body=EvidenceModel(artifact_type=<ArtifactType.EVIDENCE: 'evidence'>, metadata=EvidenceMetadata(test_case_id='predicting cpu', measurement=MeasurementMetadata(measurement_class='mlte.measurement.cpu.local_process_cpu_utilization.LocalProcessCPUUtilization', output_class='mlte.measurement.cpu.local_process_cpu_utilization.CPUStatistics', additional_data={})), evidence_class='mlte.measurement.cpu.local_process_cpu_utilization.CPUStatistics', value=OpaqueValueModel(evidence_type=<EvidenceType.OPAQUE: 'opaque'>, data={'avg': 0.5796666666666667, 'min': 0.0, 'max': 4.8469999999999995, 'unit': 'percent'})))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlte.measurement.process_measurement import ProcessMeasurement\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics\n",
    "\n",
    "cpu_measurement = LocalProcessCPUUtilization(\"predicting cpu\")\n",
    "cpu_stats: CPUStatistics = cpu_measurement.evaluate(MODEL_COMMAND)\n",
    "print(cpu_stats)\n",
    "cpu_stats.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting log\n",
      "TensorFlow version: 2.18.0\n",
      "Found 1 files.\n",
      "Size of model file (/Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/model/oxford_flower_model.keras): 206705544 bytes\n",
      "Memory used for the entire model loading process: 565035008 bytes.\n",
      "Running inference on 1 samples...\n",
      "(1, 224, 224, 3)\n",
      "Model - Input Validation Pass - RGB image loaded\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 533ms/step\n",
      "Model - Output Validation Pass - (1, 102) - validated\n",
      "Model - file: /Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/data/sample predicted to be lotus (class [55]) with confidence [0.02620832]\n",
      "\n",
      "--- STATISTICS ---\n",
      "Average elapsed time per inference: 0.54956 seconds\n",
      "Average memory used per inference: 36847616.00000 bytes.\n",
      "Average: 455528 kilobyte\n",
      "Minimum: 32 kilobyte\n",
      "Maximum: 1080704 kilobyte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArtifactModel(header=ArtifactHeaderModel(identifier='evidence.predicting memory', type='evidence', timestamp=1762179555, creator=None, level='version'), body=EvidenceModel(artifact_type=<ArtifactType.EVIDENCE: 'evidence'>, metadata=EvidenceMetadata(test_case_id='predicting memory', measurement=MeasurementMetadata(measurement_class='mlte.measurement.memory.local_process_memory_utilization.LocalProcessMemoryUtilization', output_class='mlte.measurement.memory.local_process_memory_utilization.MemoryStatistics', additional_data={})), evidence_class='mlte.measurement.memory.local_process_memory_utilization.MemoryStatistics', value=OpaqueValueModel(evidence_type=<EvidenceType.OPAQUE: 'opaque'>, data={'avg': 455528, 'min': 32, 'max': 1080704, 'unit': 'kilobyte'})))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlte.measurement.memory import (\n",
    "    LocalProcessMemoryUtilization,\n",
    "    MemoryStatistics,\n",
    ")\n",
    "\n",
    "mem_measurement = LocalProcessMemoryUtilization(\"predicting memory\")\n",
    "mem_stats: MemoryStatistics = mem_measurement.evaluate(MODEL_COMMAND)\n",
    "print(mem_stats)\n",
    "mem_stats.save(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also avoid starting the training process twice by using the asynch methods for both measurements. We start the training process once and pass the id to both measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting log\n",
      "TensorFlow version: 2.18.0\n",
      "Found 1 files.\n",
      "Size of model file (/Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/model/oxford_flower_model.keras): 206705544 bytes\n",
      "Memory used for the entire model loading process: 541442048 bytes.\n",
      "Running inference on 1 samples...\n",
      "(1, 224, 224, 3)\n",
      "Model - Input Validation Pass - RGB image loaded\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 527ms/step\n",
      "Model - Output Validation Pass - (1, 102) - validated\n",
      "Model - file: /Users/rbrowersinning/Documents/ResearchFolders/Continuum_LTP/GitRepos/mlte_llm/demo/GardenBuddy/data/sample predicted to be lotus (class [55]) with confidence [0.02620832]\n",
      "\n",
      "--- STATISTICS ---\n",
      "Average elapsed time per inference: 0.54392 seconds\n",
      "Average memory used per inference: 38780928.00000 bytes.\n",
      "Average: 1.6158333333333335 percent\n",
      "Minimum: 0.0 percent\n",
      "Maximum: 5.032 percent\n",
      "Average: 462920 kilobyte\n",
      "Minimum: 736 kilobyte\n",
      "Maximum: 1059504 kilobyte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArtifactModel(header=ArtifactHeaderModel(identifier='evidence.predicting memory', type='evidence', timestamp=1762179565, creator=None, level='version'), body=EvidenceModel(artifact_type=<ArtifactType.EVIDENCE: 'evidence'>, metadata=EvidenceMetadata(test_case_id='predicting memory', measurement=MeasurementMetadata(measurement_class='mlte.measurement.memory.local_process_memory_utilization.LocalProcessMemoryUtilization', output_class='mlte.measurement.memory.local_process_memory_utilization.MemoryStatistics', additional_data={})), evidence_class='mlte.measurement.memory.local_process_memory_utilization.MemoryStatistics', value=OpaqueValueModel(evidence_type=<EvidenceType.OPAQUE: 'opaque'>, data={'avg': 462920, 'min': 736, 'max': 1059504, 'unit': 'kilobyte'})))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryUtilization\n",
    "from mlte.measurement.process_measurement_group import ProcessMeasurementGroup\n",
    "\n",
    "# Create measurement group\n",
    "measurements = ProcessMeasurementGroup()\n",
    "\n",
    "# Add measurements to group.\n",
    "measurements.add(LocalProcessCPUUtilization(\"predicting cpu\"))\n",
    "measurements.add(LocalProcessMemoryUtilization(\"predicting memory\"))\n",
    "\n",
    "# Evaluate the measurements.\n",
    "evidences = measurements.evaluate(command=MODEL_COMMAND)\n",
    "\n",
    "# Get results.\n",
    "cpu_stats = evidences[\"predicting cpu\"]\n",
    "mem_stats = evidences[\"predicting memory\"]\n",
    "\n",
    "# Inspect values\n",
    "print(cpu_stats)\n",
    "print(mem_stats)\n",
    "\n",
    "# Save to artifact store\n",
    "cpu_stats.save(force=True)\n",
    "mem_stats.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
