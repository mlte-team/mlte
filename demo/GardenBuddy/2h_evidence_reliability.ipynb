{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2h. Evidence - Reliability QAS Measurements\n",
    "\n",
    "Measure reliability in the context of input variations.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"Computer Vision\", \"Image\"],\n",
    "    \"quality_attribute\": \"Reliability\",\n",
    "    \"description\": \"Check that if the ML pipeline receives input that doesn't conform to the input specification, it will generate the output N/A \",\n",
    "    \"inputs\": \"Input that doesn't conform to the input specification\",\n",
    "    \"output\": \"Model returns N/A\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 7\n",
    "print(card.quality_scenarios[qa])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\tTest dataset needs to include entries that are intended to fail, generated based on Equivalence Testing and Boundary Testing.\n",
    "\n",
    "**Measurement and Condition:**\tLog entries are produced for all erroneous inputs.\n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Prepare all functions and data for the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model module\n",
    "from utils import model_predict\n",
    "\n",
    "# from utils import garden\n",
    "\n",
    "\n",
    "def run_and_get_log() -> str:\n",
    "    \"\"\"Runs the model and gets the log.\"\"\"\n",
    "    model_predict.run_model(OOD_DATASET_DIR, MODEL_FILE_PATH)\n",
    "\n",
    "    return model_predict.load_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model module\n",
    "from utils import model_predict\n",
    "from utils import garden\n",
    "\n",
    "\n",
    "def run_and_get_log() -> str:\n",
    "    \"\"\"Runs the model and gets the log.\"\"\"\n",
    "    model_predict.run_model(SAMPLE_DATASET_DIR, MODEL_FILE_PATH)\n",
    "\n",
    "    return model_predict.load_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Finally, we execute the measurements and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "from mlte.evidence.types.string import String\n",
    "\n",
    "# Evaluate, identifier has to be the same one defined in the TestSuite.\n",
    "measurement = ExternalMeasurement(\n",
    "    \"input format validation error\", String, run_and_get_log\n",
    ")\n",
    "result = measurement.evaluate()\n",
    "\n",
    "# Inspect value\n",
    "# print(result)\n",
    "\n",
    "# Save to artifact store\n",
    "result.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
