{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2l. Evidence - Reproducibility QAS Measurements\n",
    "\n",
    "Evidence collected in this section checks for the Reproducibility scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n",
    "\n",
    "The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"tags\": [\"Computer Vision\",\"Object detection\"],\n",
    "    \"quality_attribute\": \"reproducibility\",\n",
    "    \"description\": \"testing the ability of the ML model to produce similar model outputs when training multiple models on different random samples of the training data\",\n",
    "    \"inputs\": \"model results from different models, trained on different random sample sets of the training data\",\n",
    "    \"output\": \"p-value from the Friedman test\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up context for the model being used, sets up constants related to folders and model data to be used.\n",
    "from session import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up scenario test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "\n",
    "card = NegotiationCard.load()\n",
    "qa = 12\n",
    "print(card.quality_scenarios[qa].identifier)\n",
    "print(card.quality_scenarios[qa].quality)\n",
    "print(\n",
    "    card.quality_scenarios[qa].stimulus,\n",
    "    \"from \",\n",
    "    card.quality_scenarios[qa].source,\n",
    "    \" during \",\n",
    "    card.quality_scenarios[qa].environment,\n",
    "    \". \",\n",
    "    card.quality_scenarios[qa].response,\n",
    "    card.quality_scenarios[qa].measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Specific test case generated from the scenario:**\n",
    "\n",
    "**Data and Data Source:**\t10 class-balanced sets of the training data set will be generated via sampling of the training data set. The ML algorithm will then be used to generate a trained on each of those data sets, producing 10 ML models. The test data used will be used to evaluate and compare the 10 models' performance. \n",
    "\n",
    "**Measurement and Condition:**\tML components will be compared for each data label class. A Friedman test will be used to evaluate if similarity of class of model results, with p<0.05. \n",
    "\n",
    "**Context:**\tNormal Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "General functions and external imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions.\n",
    "\n",
    "from utils import garden\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from os import path\n",
    "\n",
    "\n",
    "def load_data(data_folder: str):\n",
    "    \"\"\"Loads all garden data results and taxonomy categories.\"\"\"\n",
    "    df_results = garden.load_base_results(data_folder, \"predictions_test.csv\")\n",
    "    df_results.head()\n",
    "\n",
    "    # Load the taxonomic data and merge with results.\n",
    "    df_info = garden.load_taxonomy(data_folder)\n",
    "    df_results.rename(columns={\"label\": \"Label\"}, inplace=True)\n",
    "    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n",
    "\n",
    "    return df_info, df_all\n",
    "\n",
    "\n",
    "def load_results(data_folder: str):\n",
    "    \"\"\"loads reproducabilty test result runs\"\"\"\n",
    "    # my_folder = data_folder +\n",
    "    df_results = pd.read_csv(\n",
    "        path.join(data_folder, \"ReproducibilityDataSet_CV.csv\")\n",
    "    )\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\n",
    "\n",
    "df = load_results(DATASETS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = stats.friedmanchisquare(\n",
    "    df.Accuracy_r0,\n",
    "    df.Accuracy_r1,\n",
    "    df.Accuracy_r2,\n",
    "    df.Accuracy_r3,\n",
    "    df.Accuracy_r4,\n",
    "    df.Accuracy_r5,\n",
    "    df.Accuracy_r6,\n",
    "    df.Accuracy_r7,\n",
    "    df.Accuracy_r8,\n",
    "    df.Accuracy_r9,\n",
    ")\n",
    "results.pvalue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "In this example, we evaluate the output from `stats.friedmanchisquare` using an `ExternalMeasurement` class, and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.evidence.types.array import Array\n",
    "from mlte.measurement.external_measurement import ExternalMeasurement\n",
    "\n",
    "\n",
    "kruskal_measurement = ExternalMeasurement(\n",
    "    \"repeated training on training samples\", Array, stats.friedmanchisquare\n",
    ")\n",
    "\n",
    "# Evaluate.\n",
    "kruskal_res = kruskal_measurement.evaluate(\n",
    "    df.Accuracy_r0,\n",
    "    df.Accuracy_r1,\n",
    "    df.Accuracy_r2,\n",
    "    df.Accuracy_r3,\n",
    "    df.Accuracy_r4,\n",
    "    df.Accuracy_r5,\n",
    "    df.Accuracy_r6,\n",
    "    df.Accuracy_r7,\n",
    "    df.Accuracy_r8,\n",
    "    df.Accuracy_r9,\n",
    ")\n",
    "\n",
    "# Inspect values\n",
    "print(kruskal_res)\n",
    "\n",
    "# Save to artifact store\n",
    "kruskal_res.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
