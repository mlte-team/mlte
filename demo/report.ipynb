{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Report Generation\n",
    "\n",
    "The final phase of SDMT involves aggregating evidence, validating the metrics reflected by the evidence we collected, and displaying this information in a report."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries for loading the package locally\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def package_root() -> str:\n",
    "    \"\"\"Resolve the path to the project root.\"\"\"\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src/\"))\n",
    "\n",
    "sys.path.append(package_root())\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which media is stored\n",
    "REPORTS_DIR = Path(os.getcwd()) / \"reports\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlte\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "\n",
    "mlte.set_model(\"IrisClassifier\", \"0.0.1\")\n",
    "mlte.set_artifact_store_uri(f\"local://{store_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Binding\n",
    "\n",
    "In MLTE, a `Binding` associates individual results with the properties to which they attest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.binding import Binding\n",
    "\n",
    "binding = Binding(\n",
    "    {\n",
    "        \"TaskEfficacy\": [\n",
    "            \"accuracy\",\n",
    "            \"confusion matrix\",\n",
    "            \"class distribution\"\n",
    "        ],\n",
    "        \"StorageCost\": [\n",
    "            \"model size\"\n",
    "        ],\n",
    "        \"TrainingComputeCost\": [\n",
    "            \"training cpu\"\n",
    "        ],\n",
    "        \"TrainingMemoryCost\": [\n",
    "            \"training memory\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Binding also supports persistence\n",
    "binding.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Results\n",
    "\n",
    "With our `Binding` defined, we can load the results we generated previously, and _validate_ them by invoking type-specific `Validator` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.result import Integer\n",
    "model_size: Integer = Integer.load(\"model size\")\n",
    "model_size = model_size.less_than(3000)\n",
    "\n",
    "# ValidationResults support introspection\n",
    "print(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.cpu import CPUStatistics\n",
    "cpu_utilization: CPUStatistics = CPUStatistics.load(\"training cpu\")\n",
    "cpu_utilization = cpu_utilization.max_utilization_less_than(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.memory import MemoryStatistics\n",
    "memory_consumption: MemoryStatistics = MemoryStatistics.load(\"training memory\")\n",
    "memory_consumption = memory_consumption.average_consumption_less_than(1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.result import Real\n",
    "accuracy: Real = Real.load(\"accuracy\")\n",
    "accuracy = accuracy.greater_or_equal_to(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confusion_matrix import ConfusionMatrix\n",
    "confusion_matrix: ConfusionMatrix = ConfusionMatrix.load(\"confusion matrix\")\n",
    "confusion_matrix = confusion_matrix.misclassification_count_less_than(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.result import Image\n",
    "class_distribution: Image = Image.load(\"class distribution\")\n",
    "class_distribution = class_distribution.ignore(\"Inspect the image.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind `Result`s to `Property`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec import Spec, BoundSpec\n",
    "from mlte.measurement.result import Real, Integer\n",
    "from mlte.measurement.memory import MemoryStatistics\n",
    "\n",
    "# Load the specification\n",
    "spec = Spec.load()\n",
    "\n",
    "# Bind results to properties, according to Binding\n",
    "bound_spec: BoundSpec = spec.bind(binding, [\n",
    "    model_size,\n",
    "    cpu_utilization,\n",
    "    memory_consumption,\n",
    "    accuracy,\n",
    "    confusion_matrix,\n",
    "    class_distribution\n",
    "])\n",
    "\n",
    "# BoundSpec also supports persistence\n",
    "bound_spec.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Report\n",
    "\n",
    "The final step of SDMT involves the generation of a report to communicate the results of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mlte.report import Report, Dataset, User, UseCase, Limitation\n",
    "\n",
    "def unix_timestamp() -> str:\n",
    "    return f\"{int(time.time())}\"\n",
    "\n",
    "def build_report() -> Report:\n",
    "    report = Report()\n",
    "    report.metadata.project_name = \"IrisClassificationProject\"\n",
    "    report.metadata.authors = [\"Kyle Dotterrer\", \"Kate Maffey\"]\n",
    "    report.metadata.source_url = \"https://github.com/mlte-team\"\n",
    "    report.metadata.artifact_url = \"https://github.com/mlte-team\"\n",
    "    report.metadata.timestamp = unix_timestamp()\n",
    "\n",
    "    report.model_details.name = \"IrisClassifier\"\n",
    "    report.model_details.overview = \"A model that distinguishes among three (3) types of irises.\"\n",
    "    report.model_details.documentation = \"This is a simple model that can distinguish between the setosa, versicolour, and virginica species of Iris based on physical characteristics.\"\n",
    "\n",
    "    report.model_specification.domain = \"Classification\"\n",
    "    report.model_specification.architecture = \"Decision Tree\"\n",
    "    report.model_specification.input = \"Vector[4]\"\n",
    "    report.model_specification.output = \"Binary\"\n",
    "    report.model_specification.data = [\n",
    "        Dataset(\"Dataset0\", \"https://github.com/mlte-team\", \"This is one training dataset.\"),\n",
    "        Dataset(\"Dataset1\", \"https://github.com/mlte-team\", \"This is the other one we used.\"),\n",
    "    ]\n",
    "\n",
    "    report.considerations.users = [\n",
    "        User(\"Botanist\", \"A professional botanist.\"),\n",
    "        User(\"Explorer\", \"A weekend-warrior outdoor explorer.\"),\n",
    "    ]\n",
    "    report.considerations.use_cases = [\n",
    "        UseCase(\"Personal Edification\", \"Quench your curiosity: what species of iris IS that? Wonder no longer.\")\n",
    "    ]\n",
    "    report.considerations.limitations = [\n",
    "        Limitation(\n",
    "            \"Low Training Data Volume\",\n",
    "            \"\"\"\n",
    "            This model was trained on a low volume of training data.\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.report import render\n",
    "\n",
    "# Build the base report\n",
    "report = build_report()\n",
    "# Attach the bound specification\n",
    "report.spec = bound_spec\n",
    "\n",
    "# Save the report as an HTML document\n",
    "report.to_html(REPORTS_DIR / \"report.html\", local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82adda432962015d5f71beb9387a99f24d390514e497c776c87ff3434daf7312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
