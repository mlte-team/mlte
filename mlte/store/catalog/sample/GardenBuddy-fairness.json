{
    "header": {
        "identifier": "GardenBuddy-fairness",
        "creator": "admin",
        "created": 1763412874,
        "updater": null,
        "updated": 1763412874,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Object Detection"
    ],
    "quality_attribute": "Fairness",
    "code": "# ## 2a. Evidence - Fairness QAS Measurements\n# \n# Evidence collected in this section checks for the Fairness scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\n\n# print the QAS scenario\nqa = 0\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tTest data needs to include pictures of the flowers from the different sections, grouped by the section that the image was taken at. The quantity of the flower images should be representative of the garden section population they are taken from.\n# \n# **Measurement and Condition:**\tThe total accuracy of the model across each garden population should be higher or equal to 0.9.\n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# General functions and external imports.\n\n# General functions.\n\nfrom utils import garden\nimport numpy as np\n\n\ndef load_data(data_folder: str):\n    \\\"\\\"\\\"Loads all garden data results and taxonomy categories.\\\"\\\"\\\"\n    df_results = garden.load_base_results(data_folder, \\\"0abcflmn_cv_output.csv\\\")\n    df_results.head()\n\n    # Load the taxonomic data and merge with results.\n    df_info = garden.load_taxonomy(data_folder)\n    df_results.rename(columns={\\\"label\\\": \\\"Label\\\"}, inplace=True)\n    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n\n    return df_info, df_all\n\n\ndef split_data(df_info, df_all, population_size=100):\n    \\\"\\\"\\\"Splits the data into 3 different populations to evaluate them.\\\"\\\"\\\"\n    df_gardenpop = df_info.copy()\n    df_gardenpop[\\\"Population1\\\"] = (\n        np.around(\n            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n            decimals=3,\n        )\n        * population_size\n    ).astype(int)\n    df_gardenpop[\\\"Population2\\\"] = (\n        np.around(\n            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n            decimals=3,\n        )\n        * population_size\n    ).astype(int)\n    df_gardenpop[\\\"Population3\\\"] = (\n        np.around(\n            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n            decimals=3,\n        )\n        * population_size\n    ).astype(int)\n    # df_gardenpop\n    print(\\\"Hello\\\")  # df_gardenpop[\\\"Population2\\\"])\n\n    print(df_gardenpop[\\\"Population2\\\"])\n    # build populations from test data set that match the garden compositions\n    from random import choices\n\n    # build 3 gardens with populations of population_size.\n    pop_names = [\\\"Population1\\\", \\\"Population2\\\", \\\"Population3\\\"]\n    gardenpops = np.zeros((3, population_size), int)\n    gardenmems = np.zeros((3, population_size), int)\n\n    for j in range(population_size):\n        for i in range(len(df_gardenpop)):\n            my_flower = df_gardenpop.iloc[i][\\\"Common Name\\\"]\n\n            for g in range(3):\n                n_choices = df_gardenpop.iloc[i][pop_names[g]]\n                my_choices = df_all[df_all[\\\"Common Name\\\"] == my_flower][\n                    \\\"model correct\\\"\n                ].to_list()\n                # print(f\\\"{n_choices} {my_choices}\\\")\n                my_selection = choices(my_choices, k=n_choices)\n\n                gardenpops[g][j] += sum(my_selection)\n                gardenmems[g][j] += len(my_selection)\n\n    gardenpops\n\n    return gardenpops, gardenmems\n\n\ndef calculate_model_performance_acc(\n    gardenpops, gardenmems, population_size=100\n):\n    \\\"\\\"\\\"Get accucray of models across the garden populations\\\"\\\"\\\"\n    gardenacc = np.zeros((3, population_size), float)\n    for i in range(population_size):\n        for g in range(3):\n            gardenacc[g][i] = gardenpops[g][i] / gardenmems[g][i]\n    gardenacc\n\n    model_performance_acc = []\n    for g in range(3):\n        avg = round(np.average(gardenacc[g][:]), 3)\n        std = round(np.std(gardenacc[g][:]), 3)\n        min = round(np.amin(gardenacc[g][:]), 3)\n        max = round(np.amax(gardenacc[g][:]), 3)\n        model_performance_acc.append(round(avg, 3))\n\n        print(\\\"%1d %1.3f %1.3f %1.3f %1.3f\\\" % (g, avg, std, min, max))\n\n    return model_performance_acc\n\n\n# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\ndata = load_data(DATASETS_DIR)\nprint(\\\"Splitting Data\\\")\n# print(data)\nsplit_data = split_data(data[0], data[1])\n\n\n# ### Measurements\n# \n# In this first example, we simply wrap the output from `accuracy_score` with a custom `Result` type to cope with the output of a third-party library that is not supported by a MLTE builtin.\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\naccuracy_measurement = ExternalMeasurement(\n    \\\"accuracy across gardens\\\", Array, calculate_model_performance_acc\n)\naccuracy = accuracy_measurement.evaluate(split_data[0], split_data[1])\n\n# Inspect value\nprint(accuracy)\n\n# Save to artifact store\naccuracy.save(force=True)\n\n\n\n",
    "description": "The model is evaluated across N populations; and the accuracy of the model is greater than or equal to 90% for each population.",
    "inputs": "An [Nx1] arrary where each row is a population accuracy",
    "output": "T/F"
}