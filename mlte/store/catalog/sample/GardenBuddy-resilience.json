{
    "header": {
        "identifier": "GardenBuddy-resilience",
        "creator": "admin",
        "created": 1763412895,
        "updater": null,
        "updated": 1763412895,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Object Detection"
    ],
    "quality_attribute": "Resilience",
    "code": "# ## 2c. Evidence - Resilience QAS Measurements\n# \n# Evidence collected in this section checks for the Robustness scenarios defined in the previous step. Note that some functions will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 2\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tTest data needs to include images with a missing channel. Test images will be generated by removing the R, G, and B channels in the original test data using ImageMagick, therefore producing three data sets.\n# \n# **Measurement and Condition:**\tImages with a missing channel are successfully identified at rates equal to that of original images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# \n# General functions and external imports.\n\n# General functions.\nimport utils.garden as garden\nimport pandas as pd\n\n\ndef calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n    # Calculate the base model accuracy result per data label\n    df_pos = (\n        df_results[df_results[\\\"model correct\\\"] == True].groupby(\\\"label\\\").count()\n    )\n\n    df_neg = (\n        df_results[df_results[\\\"model correct\\\"] == False]\n        .groupby(\\\"label\\\")\n        .count()\n    )\n    # df_neg.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n    df_neg.rename(columns={\\\"model correct\\\": \\\"model incorrect\\\"}, inplace=True)\n    df_res = df_pos.merge(\n        df_neg, right_on=\\\"label\\\", left_on=\\\"label\\\", how=\\\"outer\\\"\n    )\n    df_res.fillna(0, inplace=True)\n    df_res[\\\"model acc\\\"] = df_res[\\\"model correct\\\"] / (\n        df_res[\\\"model correct\\\"] + df_res[\\\"model incorrect\\\"]\n    )\n    df_res[\\\"count\\\"] = df_res[\\\"model correct\\\"] + df_res[\\\"model incorrect\\\"]\n    df_res.drop(columns=[\\\"model correct\\\", \\\"model incorrect\\\"], inplace=True)\n    df_res.head()\n\n    return df_res\n\n\ndef calculate_accuracy_per_set(\n    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n) -> pd.DataFrame:\n    # Calculate the model accuracy per data label for each blurred data set\n    base_filename = \\\"0c_cv_output_resilience\\\"\n    ext_filename = \\\".csv\\\"\n    set_filename = [\\\"_noR\\\", \\\"_noG\\\", \\\"_noB\\\"]\n\n    col_root = \\\"model acc\\\"\n\n    for fs in set_filename:\n        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n        colname = col_root + fs\n\n        df_temp = pd.read_csv(filename)\n        df_temp = df_temp[[\\\"model correct\\\", \\\"label\\\"]]\n\n        df_pos = (\n            df_temp[df_temp[\\\"model correct\\\"] == True].groupby(\\\"label\\\").count()\n        )\n        df_neg = (\n            df_results[df_results[\\\"model correct\\\"] == False]\n            .groupby(\\\"label\\\")\n            .count()\n        )\n        df_neg.rename(\n            columns={\\\"model correct\\\": \\\"model incorrect\\\"}, inplace=True\n        )\n        df_res2 = df_pos.merge(\n            df_neg,\n            right_on=\\\"label\\\",\n            left_on=\\\"label\\\",\n            how=\\\"outer\\\",\n        ).fillna(0)\n        df_res2.fillna(0, inplace=True)\n\n        df_res2[colname] = df_res2[\\\"model correct\\\"] / (\n            df_res2[\\\"model correct\\\"] + df_res2[\\\"model incorrect\\\"]\n        )\n        df_res2.drop(columns=[\\\"model correct\\\", \\\"model incorrect\\\"], inplace=True)\n\n        df_res = df_res.merge(\n            df_res2, right_on=\\\"label\\\", left_on=\\\"label\\\", how=\\\"outer\\\"\n        ).fillna(0)\n\n    return df_res\n\n\ndef print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n    model_acc = sum(df_res[key] * df_res[\\\"count\\\"]) / sum(df_res[\\\"count\\\"])\n    print(name, model_acc)\n\n\n# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.\ndf_results = garden.load_base_results(DATASETS_DIR, \\\"0abcflmn_cv_output.csv\\\")\ndf_results = df_results[[\\\"model correct\\\", \\\"label\\\"]]\ndf_res = calculate_base_accuracy(df_results)\ndf_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\ndf_info = garden.load_taxonomy(DATASETS_DIR)\ndf_all = garden.merge_taxonomy_with_results(df_res, df_info, \\\"label\\\", \\\"Label\\\")\n\n# fill in missing model accuracy data\ndf_all[\\\"model acc_noR\\\"] = df_all[\\\"model acc_noR\\\"].fillna(0)\ndf_all[\\\"model acc_noG\\\"] = df_all[\\\"model acc_noG\\\"].fillna(0)\ndf_all[\\\"model acc_noB\\\"] = df_all[\\\"model acc_noB\\\"].fillna(0)\n\n\n# ### Measurements\n# \n# Now do the actual measurements. First simply see the model accuracy across channel loss.\n\n# view changes in model accuracy\nprint_model_accuracy(df_res, \\\"model acc\\\", \\\"base model accuracy\\\")\nprint_model_accuracy(\n    df_res, \\\"model acc_noR\\\", \\\"model accuracy with no red channel\\\"\n)\nprint_model_accuracy(\n    df_res, \\\"model acc_noG\\\", \\\"model accuracy with no green channel\\\"\n)\nprint_model_accuracy(\n    df_res, \\\"model acc_noB\\\", \\\"model accuracy with no blue channel\\\"\n)\n\n\n# Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper.\n\nimport scipy.stats\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n\ndef run_ranksum(samp1, samp2):\n    res = scipy.stats.ranksums(samp1, samp2)\n    float_list = [float(x) for x in res]\n    # print(float(res))\n    return float_list\n\n\nmy_blur = [\\\"R\\\", \\\"G\\\", \\\"B\\\"]\nfor i in range(len(my_blur)):\n    # Define measurements.\n    ranksum_measurement = ExternalMeasurement(\n        f\\\"ranksums channel loss {my_blur[i]}\\\", Array, scipy.stats.ranksums\n    )\n\n    # Evaluate.\n    ranksum: Array = ranksum_measurement.evaluate(\n        df_res[\\\"model acc\\\"], df_res[f\\\"model acc_no{my_blur[i]}\\\"]\n    )\n    print(f\\\"blur {my_blur[i]}: {ranksum}\\\")\n\n    # Inspect values\n    print(ranksum)\n\n    # Save to artifact store\n    ranksum.save(force=True)\n\n\n\n",
    "description": "The model performance across different populations will be the same. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\u00df",
    "inputs": "Garden flower images with different channels missing",
    "output": "p-value fron Wilcoxon Rank Sum test evaluating differences in populations (in this case, images w different channel loss)"
}