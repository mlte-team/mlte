{
    "header": {
        "identifier": "ReviewPro-functional_correctness_correct",
        "creator": "admin",
        "created": 1763413872,
        "updater": null,
        "updated": 1763413872,
        "catalog_id": "sample"
    },
    "tags": [
        "LLM",
        "Content Generation"
    ],
    "quality_attribute": "Functional Correctness",
    "code": "# ## 2b. Evidence - Functional Correctness QAS Measurements\n# \n# Evidence collected in this section checks for the functional correctness QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 1\nprint(card.quality_scenarios[qa])\n\n\n# ### A Specific test case generated from the scenario:\n# \n# **Data and Data Source:**\tThe LLM receives a prompt, containing the employee goals, employee statement, and manager notes, for an employee evaluation and performance score. The original test data set can be used to simulate this request.\n# \n# **Measurement and Condition:**\tThe LLM generated score matches the manager score for 95% of samples.\n# \n# **Context:**\tNormal Operation\n# \n\n# ### Gather evidence\n\n# import necessary packages\nimport pandas as pd\n\n\n# Read the files with with the necessary input data and LLM evaluation results\ninput_df = pd.read_csv(\n    os.path.join(DATASETS_DIR, \\\"2abc_llm_input_functional_correctness.csv\\\")\n)\nresults_df = pd.read_csv(\n    os.path.join(DATASETS_DIR, \\\"2abc_llm_output_functional_correctness.csv\\\")\n)\nresults_df.drop(columns=[\\\"Unnamed: 0\\\"], inplace=True)\n\n# Preview the cleaned dataframe\nprint(input_df)\nprint(results_df)\n\n\n# ### Save evidence to the specified scenario\n\n# show percentage of incorrect results\ndef evaluate_mismatch_pcent(input_df, results_df):\n    mismatches = (\n        input_df[\\\"correctEvalScore\\\"] != results_df[\\\"extractedOverallRating\\\"]\n    )\n    # print(mismatches)\n    mismatch_count = mismatches.sum()\n    data_size = len(results_df)\n    mismatch_val = mismatch_count / data_size  # * 100\n    return float(mismatch_val)\n\n\nmismatch_val = evaluate_mismatch_pcent(input_df, results_df)\nif mismatch_val < 0.05:\n    print(f\\\"test passes with {mismatch_val} failures\\\")\nelse:\n    print(f\\\"test fails with {mismatch_val} failures\\\")\n\n\nfrom mlte.evidence.types.real import Real\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\nmismatch_measurement = ExternalMeasurement(\n    \\\"evaluation is correct\\\", Real, evaluate_mismatch_pcent\n)\nmismatch_pcent = mismatch_measurement.evaluate(input_df, results_df)\n\n# Inspect value\nprint(mismatch_pcent)\n\n# Save to artifact store\nmismatch_pcent.save(force=True)\n\n\n\n",
    "description": "Evaluating if the scores generated by the LLM match the manager scores",
    "inputs": "Manager evaluation scores, a request for a review in a crafted prompt, with the supporting material (employee statement, goals and objectives, and manager comments)",
    "output": "A generated review, with the scores extracted for evaluation and compared to the actual manager generated evaluation scores"
}