{
    "header": {
        "identifier": "demo-fairness",
        "creator": "admin",
        "created": 1727114774,
        "updater": null,        
        "updated": -1,
        "catalog_id": "sample"
    },
    "tags": ["Computer Vision"],
    "property_category": "Fairness",
    "property": "Model Impartial to Photo Location",
    "code_type": "measurement",
    "code": "# %% [markdown]\n# ### Fairnesss QAS Measurements\n# \n# Evidence collected in this section checks for the Fairness scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n\n# %%\n# General functions.\n\nimport garden\nimport numpy as np\n\nfrom mlte.spec.condition import Condition\nfrom mlte.validation.result import Failure, Success\nfrom mlte.value.types.array import Array\n\n\nclass MultipleAccuracy(Array):\n    \"\"\"An array with multiple accuracies.\"\"\"\n\n    @classmethod\n    def all_accuracies_more_or_equal_than(cls, threshold: float) -> Condition:\n        \"\"\"Checks if the accuracy for multiple populations is fair by checking if all of them are over the given threshold.\"\"\"\n        condition: Condition = Condition(\n            \"all_accuracies_more_than\",\n            [threshold],\n            lambda value: Success(\n                f\"All accuracies are equal to or over threshold {threshold}\"\n            )\n            if sum(g >= threshold for g in value.array) == len(value.array)\n            else Failure(\n                f\"One or more accuracies are below threshold {threshold}: {value.array}\"\n            ),\n        )\n        return condition\n\ndef load_data(data_folder: str):\n    \"\"\"Loads all garden data results and taxonomy categories.\"\"\"\n    df_results = garden.load_base_results(data_folder)\n    df_results.head()\n\n    # Load the taxonomic data and merge with results.\n    df_info = garden.load_taxonomy(data_folder)\n    df_results.rename(columns={\"label\": \"Label\"}, inplace=True)\n    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n\n    return df_info, df_all\n\n\ndef split_data(df_info, df_all):\n    \"\"\"Splits the data into 3 different populations to evaluate them.\"\"\"\n    df_gardenpop = df_info.copy()\n    df_gardenpop[\"Population1\"] = (\n        np.around(\n            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n            decimals=3,\n        )\n        * 1000\n    ).astype(int)\n    df_gardenpop[\"Population2\"] = (\n        np.around(\n            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n            decimals=3,\n        )\n        * 1000\n    ).astype(int)\n    df_gardenpop[\"Population3\"] = (\n        np.around(\n            np.random.dirichlet(np.ones(df_gardenpop.shape[0]), size=1)[0],\n            decimals=3,\n        )\n        * 1000\n    ).astype(int)\n    df_gardenpop\n\n    # build populations from test data set that match the garden compositions\n    from random import choices\n\n    # build 3 gardens with populations of 1000.\n    pop_names = [\"Population1\", \"Population2\", \"Population3\"]\n    gardenpops = np.zeros((3, 1000), int)\n    gardenmems = np.zeros((3, 1000), int)\n\n    for j in range(1000):\n        for i in range(len(df_gardenpop)):\n            my_flower = df_gardenpop.iloc[i][\"Common Name\"]\n\n            for g in range(3):\n                n_choices = df_gardenpop.iloc[i][pop_names[g]]\n                my_choices = df_all[df_all[\"Common Name\"] == my_flower][\n                    \"model correct\"\n                ].to_list()\n                my_selection = choices(my_choices, k=n_choices)\n\n                gardenpops[g][j] += sum(my_selection)\n                gardenmems[g][j] += len(my_selection)\n\n    gardenpops\n\n    return gardenpops, gardenmems\n\n\ndef calculate_model_performance_acc(gardenpops, gardenmems):\n    \"\"\"Get accucray of models across the garden populations\"\"\"\n    gardenacc = np.zeros((3, 1000), float)\n    for i in range(1000):\n        for g in range(3):\n            gardenacc[g][i] = gardenpops[g][i] / gardenmems[g][i]\n    gardenacc\n\n    model_performance_acc = []\n    for g in range(3):\n        avg = round(np.average(gardenacc[g][:]), 3)\n        std = round(np.std(gardenacc[g][:]), 3)\n        min = round(np.amin(gardenacc[g][:]), 3)\n        max = round(np.amax(gardenacc[g][:]), 3)\n        model_performance_acc.append(round(avg, 3))\n\n        print(\"%1d %1.3f %1.3f %1.3f %1.3f\" % (g, avg, std, min, max))\n\n    return model_performance_acc\n\n# %%\n# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\ndata = load_data(DATASETS_DIR)\nsplit_data = split_data(data[0], data[1])\n\n# %% [markdown]\n# In this first example, we simply wrap the output from `accuracy_score` with a custom `Result` type to cope with the output of a third-party library that is not supported by a MLTE builtin.\n\n# %%\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n# Evaluate accuracy, identifier has to be the same one defined in the Spec.\naccuracy_measurement = ExternalMeasurement(\n    \"accuracy across gardens\", MultipleAccuracy, calculate_model_performance_acc\n)\naccuracy = accuracy_measurement.evaluate(split_data[0], split_data[1])\n\n# Inspect value\nprint(accuracy)\n\n# Save to artifact store\naccuracy.save(force=True)\n",
    "description": "The model receives a picture taken at the garden and, regardless of the garden location, can correctly identify the correct flowers at least 90% of the time. Test data needs to include pictures of the flowers from the different gardens, grouped by the garden that the image was taken at. The quantity of the flower images should be representative of the garden population they are taken from. The total accuracy of the model across each garden population should be higher or equal to 0.9.",
    "inputs": "three garden populations, model results on Oxford garden data",
    "output": "accuracy across gardens"
}