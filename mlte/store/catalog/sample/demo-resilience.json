{
    "header": {
        "identifier": "demo-resilience",
        "creator": "admin",
        "created": 1727114774,
        "updater": null,        
        "updated": -1,
        "catalog_id": "sample"
    },
    "tags": ["General"],
    "property_category": "Resilience",
    "property": "Input Validation",
    "code_type": "measurement",
    "code": "# %% [markdown]\n# ## Resilience QAS Measurements\n# \n# Measure resilience to input variations.\n\n# %% [markdown]\n# ### Helper Functions\n# \n# Prepare all functions and data for the measurements.\n\n# %%\n# Load model module\nfrom demo.scenarios import model_predict\n\n\ndef run_and_get_log() -> str:\n    \"\"\"Runs the model and gets the log.\"\"\"\n    model_predict.run_model(OOD_DATASET_DIR, MODEL_FILE_PATH, MODEL_WEIGHTS_PATH)\n\n    return model_predict.load_log()\n\n# %% [markdown]\n# ### Measurements\n# \n# Finally, we execute the measurements and store the results.\n\n# %%\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom demo.scenarios.values.string import String\n\n# Evaluate, identifier has to be the same one defined in the Spec.\nmeasurement = ExternalMeasurement(\n    \"input format validation\", String, run_and_get_log\n)\nresult = measurement.evaluate()\n\n# Inspect value\nprint(result)\n\n# Save to artifact store\nresult.save(force=True)\n\n\n",
    "description": "During normal operation, if the ML pipeline receives an input that does conform to the input specification it will generate the output \"N/A\" which the app will interpret as an error. The ML pipeline will create a log entry with a specific tag.",
    "inputs": "existing ML model, sample image data that has input validation issues",
    "output": "log with input issue tagged"
}
