{
    "header": {
        "identifier": "ReviewPro-time_behavior",
        "creator": "admin",
        "created": 1763413896,
        "updater": null,
        "updated": 1763413896,
        "catalog_id": "sample"
    },
    "tags": [
        "LLM",
        "Generative Model"
    ],
    "quality_attribute": "Time Behavior",
    "code": "# ## 2f. Evidence - Time Behaviour QAS Measurements\n# \n# Evidence collected in this section checks for the time behaviour QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files.\n# \n# NOTE: This demo requires a `.env` file to be configured with API keys to execute. Consult [README](README.md) for more information.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 5\nprint(card.quality_scenarios[qa])\n\n\n# ### A Specific test case generated from the scenario:\n# \n# **Data and Data Source:**\tThe original test data set can be used.\n# \n# **Measurement and Condition:**\tThe longest time for a single prompt model response completes is no more than 10 seconds after submission\n# \n# **Context:**\tNormal Operation\n\n# ### Gather evidence\n\n# import necessary packages\nimport pandas as pd\nimport time\n\nfrom utils.evaluation_helpers import *\n\n\n# Read the files with with the necessary input data and LLM evaluation results\ninput_df = pd.read_csv(\n    os.path.join(DATASETS_DIR, \\\"2abc_llm_input_functional_correctness.csv\\\")\n)\n\n\ninput_df\n\n\n# ### Save evidence to the specicified scenario\n\nchain = prompt_template | llm\ntime_performances = []\nchat_responses = []\n\n\ndef time_model(input_df):\n    for index, row in input_df.iterrows():\n        pii_data = {\n            \\\"employee_name\\\": row.EmployeeName,\n            \\\"goals_and_objectives\\\": row.goalsAndObjectives,\n            \\\"self_eval\\\": row.employeeSelfEval,\n            \\\"manager_comments\\\": row.managerComments,\n        }\n\n    prompt = prompt_template.format(**pii_data)\n\n    start_time = time.time()\n    chat_response = chain.invoke(pii_data)\n    time_performances.append(time.time() - start_time)\n\n    return time_performances\n\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\ntime_measurement = ExternalMeasurement(\n    \\\"results returned promptly\\\", Array, time_model\n)\ntime_res = time_measurement.evaluate(input_df)\n\n# Inspect value\nprint(time_res)\n\n# Save to artifact store\ntime_res.save(force=True)\n\n\n\n",
    "description": "A measure of the time required to generate the review",
    "inputs": "A request for a review in a crafted prompt, with the supporting material (employee statement, goals and objectives, and manager comments)",
    "output": "The time it takes for a review to be generated "
}