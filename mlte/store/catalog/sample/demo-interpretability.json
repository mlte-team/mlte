{
    "header": {
        "identifier": "demo-interpretability",
        "creator": "admin",
        "created": 1727114774,
        "updater": null,        
        "updated": -1,
        "catalog_id": "sample"
    },
    "tags": ["Computer Vision"],
    "property_category": "Interpretability",
    "property": "Understanding Model Results",
    "code_type": "measurement",
    "code": "# %% [markdown]\n# ### Interpretability QAS Measurements.\n# \n# Now we proceed to gather data about the Interpretability of the model, for the corresponding scenario. NOTE: the version of tensorflow used in this demo requires running it under Python 3.9 or higher.\n\n# %%\nmodel_filename = (\n    MODELS_DIR / \"model_f3_a.json\"\n)  # The json file of the model to load\nweights_filename = MODELS_DIR / \"model_f_a.h5\"  # The weights file for the model\n\n# %%\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import model_from_json\n\n\ndef load_model(model_filename: str, weights_filename: str):\n    # Load model\n    json_file = open(model_filename, \"r\")\n    loaded_model_json = json_file.read()\n    loaded_model = model_from_json(loaded_model_json)\n    json_file.close()\n\n    # Load weights into new model\n    loaded_model.load_weights(weights_filename)\n\n    return loaded_model\n\n\ndef run_model(im, loaded_model):\n    im_batch = tf.expand_dims(im, 0)\n    predictions = loaded_model(im_batch)\n    return predictions\n\n\ndef read_image(filename):\n    image = tf.io.read_file(filename)\n    image = tf.io.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n    return image\n\n\ndef generate_baseline_and_alphas():\n    baseline = tf.zeros(shape=(224, 224, 3))\n    m_steps = 50\n    alphas = tf.linspace(\n        start=0.0, stop=1.0, num=m_steps + 1\n    )  # Generate m_steps intervals for integral_approximation() below.\n    return baseline, alphas\n\n\ndef interpolate_images(baseline, image, alphas):\n    alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n    baseline_x = tf.expand_dims(baseline, axis=0)\n    input_x = tf.expand_dims(image, axis=0)\n    delta = input_x - baseline_x\n    images = baseline_x + alphas_x * delta\n    return images\n\n\ndef integral_approximation(gradients):\n    # riemann_trapezoidal\n    grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n    int_gradients = tf.math.reduce_mean(grads, axis=0)\n    return int_gradients\n\n\ndef compute_gradients(loaded_model, images, target_class_idx):\n    with tf.GradientTape() as tape:\n        tape.watch(images)\n        probs = loaded_model(images)[:, target_class_idx]\n        \"\"\"\n    If your model does not have a softmax output layer,\n    comment out the above line and un-comment the following 2 lines\n    \"\"\"\n        # logits = loaded_model(images)\n        # probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n\n    return tape.gradient(probs, images)\n\n\n@tf.function\ndef one_batch(baseline, image, alpha_batch, target_class_idx, loaded_model):\n    # Generate interpolated inputs between baseline and input.\n    interpolated_path_input_batch = interpolate_images(\n        baseline=baseline, image=image, alphas=alpha_batch\n    )\n\n    # Compute gradients between model outputs and interpolated inputs.\n    gradient_batch = compute_gradients(\n        loaded_model=loaded_model,\n        images=interpolated_path_input_batch,\n        target_class_idx=target_class_idx,\n    )\n    return gradient_batch\n\n\ndef integrated_gradients(\n    baseline, image, target_class_idx, loaded_model, m_steps=50, batch_size=32\n):\n    # Generate alphas.\n    alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps + 1)\n\n    # Collect gradients.\n    gradient_batches = []\n\n    # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n    for alpha in tf.range(0, len(alphas), batch_size):\n        from_ = alpha\n        to = tf.minimum(from_ + batch_size, len(alphas))\n        alpha_batch = alphas[from_:to]\n\n        gradient_batch = one_batch(\n            baseline, image, alpha_batch, target_class_idx, loaded_model\n        )\n        gradient_batches.append(gradient_batch)\n\n    # Concatenate path gradients together row-wise into single tensor.\n    total_gradients = tf.concat(gradient_batches, axis=0)\n\n    # Integral approximation through averaging gradients.\n    avg_gradients = integral_approximation(gradients=total_gradients)\n\n    # Scale integrated gradients with respect to input.\n    gradients = (image - baseline) * avg_gradients\n    return gradients\n\n\ndef plot_img_attributions(\n    baseline,\n    image,\n    target_class_idx,\n    loaded_model,\n    m_steps=50,\n    cmap=None,\n    overlay_alpha=0.4,\n):\n    attributions = integrated_gradients(\n        baseline=baseline,\n        image=image,\n        target_class_idx=target_class_idx,\n        loaded_model=loaded_model,\n        m_steps=m_steps,\n    )\n\n    # Sum of the attributions across color channels for visualization.\n    # The attribution mask shape is a grayscale image with height and width\n    # equal to the original image.\n    attribution_mask = tf.reduce_sum(tf.math.abs(attributions), axis=-1)\n\n    fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(8, 8))\n\n    axs[0, 0].set_title(\"Baseline image\")\n    axs[0, 0].imshow(baseline)\n    axs[0, 0].axis(\"off\")\n\n    axs[0, 1].set_title(\"Original image\")\n    axs[0, 1].imshow(image)\n    axs[0, 1].axis(\"off\")\n\n    axs[1, 0].set_title(\"Attribution mask\")\n    axs[1, 0].imshow(attribution_mask, cmap=cmap)\n    axs[1, 0].axis(\"off\")\n\n    axs[1, 1].set_title(\"Overlay\")\n    axs[1, 1].imshow(attribution_mask, cmap=cmap)\n    axs[1, 1].imshow(image, alpha=overlay_alpha)\n    axs[1, 1].axis(\"off\")\n\n    plt.tight_layout()\n    return fig\n\n\n# Load the model/\nloaded_model = load_model(model_filename, weights_filename)\n\n# %%\n# Load and show the image.\n\nflower_img = \"flower3.jpg\"  # Filename of flower image to use, public domain image adapted from: https://commons.wikimedia.org/wiki/File:Beautiful_white_flower_in_garden.jpg\nflower_idx = (\n    42  # Classifier index of associated flower (see OxfordFlower102Labels.csv)\n)\n\nim = read_image(os.path.join(SAMPLE_DATASET_DIR, flower_img))\n\nplt.imshow(im)\nplt.axis(\"off\")\nplt.show()\n\n# %%\npredictions = run_model(im, loaded_model)\n\nbaseline, alphas = generate_baseline_and_alphas()\n\n# %%\ninterpolated_images = interpolate_images(\n    baseline=baseline, image=im, alphas=alphas\n)\n\n# %%\nfig = plt.figure(figsize=(20, 20))\n\ni = 0\nfor alpha, image in zip(alphas[0::10], interpolated_images[0::10]):\n    i += 1\n    plt.subplot(1, len(alphas[0::10]), i)\n    plt.title(f\"alpha: {alpha:.1f}\")\n    plt.imshow(image)\n    plt.axis(\"off\")\n\nplt.tight_layout()\n\n# %%\npath_gradients = compute_gradients(\n    loaded_model=loaded_model,\n    images=interpolated_images,\n    target_class_idx=flower_idx,\n)\nprint(path_gradients.shape)\n\nig = integral_approximation(gradients=path_gradients)\nprint(ig.shape)\n\n# %%\nig_attributions = integrated_gradients(\n    baseline=baseline,\n    image=im,\n    target_class_idx=flower_idx,\n    loaded_model=loaded_model,\n    m_steps=240,\n)\nprint(ig_attributions.shape)\n\n# %%\nfig = plot_img_attributions(\n    image=im,\n    baseline=baseline,\n    target_class_idx=flower_idx,\n    loaded_model=loaded_model,\n    m_steps=240,\n    cmap=plt.cm.inferno,\n    overlay_alpha=0.4,\n)\n\nplt.savefig(MEDIA_DIR / \"attributions.png\")\n\n# %%\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom mlte.value.types.image import Image\n\n# Save to MLTE store.\nimg_collector = ExternalMeasurement(\"image attributions\", Image)\nimg = img_collector.ingest(MEDIA_DIR / \"attributions.png\")\nimg.save(force=True)\n",
    "description": "The application that runs on the loaned device should indicate the main features that were used to recognize the flower, as part of the educational experience. The app will display the image highlighting the most informative features in flower identification, in addition to the flower name. The original test data set can be used. The model needs to return evidence, in this case a heat map implementing the Integrated Gradients algorithm, showing the pixels that were most informative in the classification decision. This evidence should be returned with each inference.",
    "inputs": "existing garden ML model, sample image",
    "output": "image with attributions"
}