{
    "header": {
        "identifier": "GardenBuddy-repeatability",
        "creator": "admin",
        "created": 1763412955,
        "updater": null,
        "updated": 1763412955,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Object Detection"
    ],
    "quality_attribute": "Repeatability",
    "code": "# ## 2k. Evidence - Repeatability QAS Measurements\n# \n# Evidence collected in this section checks for the Repeatability scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 11\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\t50 random samples of the test data set will be generated, with replacement each containing 500 test samples.  ML component results will be generated form each test data set.\n# \n# **Measurement and Condition:**\tML components will be compared for each data label class. A Kruskal-Wallis test will be used to evaluate if similarity of class of model results, with p<0.05 and a Bonferroni correction. \n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# General functions and external imports.\n\n# General functions.\n\nfrom utils import garden\nimport pandas as pd\nfrom scipy import stats\n\n\ndef load_data(data_folder: str):\n    \\\"\\\"\\\"Loads all garden data results and taxonomy categories.\\\"\\\"\\\"\n    df_results = garden.load_base_results(data_folder, \\\"0abcflmn_cv_output.csv\\\")\n    df_results.head()\n\n    # Load the taxonomic data and merge with results.\n    df_info = garden.load_taxonomy(data_folder)\n    df_results.rename(columns={\\\"label\\\": \\\"Label\\\"}, inplace=True)\n    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n\n    return df_info, df_all\n\n\n# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\ndf_info, df_all = load_data(DATASETS_DIR)\n\n\nres_df = pd.DataFrame()\n\n# test_res = df[[\\\"model correct\\\"]].to_numpy()\n# test_res\n\nfor i in range(50):\n    # generate 50 samples of the test data\n    col_name = \\\"run\\\" + str(i)\n    sample_df = df_all[[\\\"model correct\\\"]].sample(\n        n=500, replace=True, random_state=i\n    )\n    sample_df.reset_index(drop=True, inplace=True)\n    sample_df.rename(columns={\\\"model correct\\\": col_name}, inplace=True)\n    if i == 0:\n        res_df = sample_df.copy()\n    else:\n        res_df = pd.merge(res_df, sample_df, right_index=True, left_index=True)\n\nres_df\n\n\nresults = stats.kruskal(\n    res_df.run0,\n    res_df.run1,\n    res_df.run2,\n    res_df.run3,\n    res_df.run4,\n    res_df.run5,\n    res_df.run6,\n    res_df.run7,\n    res_df.run8,\n    res_df.run9,\n    res_df.run10,\n    res_df.run11,\n    res_df.run12,\n    res_df.run13,\n    res_df.run14,\n    res_df.run15,\n    res_df.run16,\n    res_df.run17,\n    res_df.run18,\n    res_df.run19,\n    res_df.run20,\n    res_df.run21,\n    res_df.run22,\n    res_df.run23,\n    res_df.run24,\n    res_df.run25,\n    res_df.run26,\n    res_df.run27,\n    res_df.run28,\n    res_df.run29,\n    res_df.run30,\n    res_df.run31,\n    res_df.run32,\n    res_df.run33,\n    res_df.run34,\n    res_df.run35,\n    res_df.run36,\n    res_df.run37,\n    res_df.run38,\n    res_df.run39,\n    res_df.run40,\n    res_df.run41,\n    res_df.run42,\n    res_df.run43,\n    res_df.run44,\n    res_df.run45,\n    res_df.run46,\n    res_df.run47,\n    res_df.run48,\n    res_df.run49,\n)\n\n\n# ### Measurements\n# \n# In this example, we evaluate the output from `stats.kruskal` using an `ExternalMeasurement` class, and store the result.\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n\nkruskal_measurement = ExternalMeasurement(\n    \\\"repeated results sampling\\\", Array, stats.kruskal\n)\n\n# Evaluate.\nkruskal_res = kruskal_measurement.evaluate(\n    res_df.run0,\n    res_df.run1,\n    res_df.run2,\n    res_df.run3,\n    res_df.run4,\n    res_df.run5,\n    res_df.run6,\n    res_df.run7,\n    res_df.run8,\n    res_df.run9,\n    res_df.run10,\n    res_df.run11,\n    res_df.run12,\n    res_df.run13,\n    res_df.run14,\n    res_df.run15,\n    res_df.run16,\n    res_df.run17,\n    res_df.run18,\n    res_df.run19,\n    res_df.run20,\n    res_df.run21,\n    res_df.run22,\n    res_df.run23,\n    res_df.run24,\n    res_df.run25,\n    res_df.run26,\n    res_df.run27,\n    res_df.run28,\n    res_df.run29,\n    res_df.run30,\n    res_df.run31,\n    res_df.run32,\n    res_df.run33,\n    res_df.run34,\n    res_df.run35,\n    res_df.run36,\n    res_df.run37,\n    res_df.run38,\n    res_df.run39,\n    res_df.run40,\n    res_df.run41,\n    res_df.run42,\n    res_df.run43,\n    res_df.run44,\n    res_df.run45,\n    res_df.run46,\n    res_df.run47,\n    res_df.run48,\n    res_df.run49,\n)\n\n# Inspect values\nprint(kruskal_res)\n\n# Save to artifact store\nkruskal_res.save(force=True)\n\n\n\n",
    "description": "Testing the ability of the ML model to produce similar output in repeated samples drawn from the same input distribution",
    "inputs": "A number of sets of random samples of flower images from the garden ",
    "output": "Kruskal-Wallis test p-value, evaluating the similarity of model results"
}