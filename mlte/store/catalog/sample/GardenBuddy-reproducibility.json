{
    "header": {
        "identifier": "GardenBuddy-reproducibility",
        "creator": "admin",
        "created": 1763412962,
        "updater": null,
        "updated": 1763412962,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Object detection"
    ],
    "quality_attribute": "Reproducibility",
    "code": "# ## 2l. Evidence - Reproducibility QAS Measurements\n# \n# Evidence collected in this section checks for the Reproducibility scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 12\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\t10 class-balanced sets of the training data set will be generated via sampling of the training data set. The ML algorithm will then be used to generate a trained on each of those data sets, producing 10 ML models. The test data used will be used to evaluate and compare the 10 models\\\" performance. \n# \n# **Measurement and Condition:**\tML components will be compared for each data label class. A Friedman test will be used to evaluate if similarity of class of model results, with p<0.05. \n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# General functions and external imports.\n\n# General functions.\n\nfrom utils import garden\nimport pandas as pd\nfrom scipy import stats\nfrom os import path\n\n\ndef load_data(data_folder: str):\n    \\\"\\\"\\\"Loads all garden data results and taxonomy categories.\\\"\\\"\\\"\n    df_results = garden.load_base_results(data_folder, \\\"0abcflmn_cv_output.csv\\\")\n\n    # Load the taxonomic data and merge with results.\n    df_info = garden.load_taxonomy(data_folder)\n    df_results.rename(columns={\\\"label\\\": \\\"Label\\\"}, inplace=True)\n    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n\n    return df_info, df_all\n\n\ndef load_results(data_folder: str):\n    \\\"\\\"\\\"loads reproducabilty test result runs\\\"\\\"\\\"\n    # my_folder = data_folder +\n    df_results = pd.read_csv(\n        path.join(data_folder, \\\"0m_cv_output_reproducibility.csv\\\")\n    )\n\n    return df_results\n\n\n# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\n\ndf = load_results(DATASETS_DIR)\n\n\nresults = stats.friedmanchisquare(\n    df.Accuracy_r0,\n    df.Accuracy_r1,\n    df.Accuracy_r2,\n    df.Accuracy_r3,\n    df.Accuracy_r4,\n    df.Accuracy_r5,\n    df.Accuracy_r6,\n    df.Accuracy_r7,\n    df.Accuracy_r8,\n    df.Accuracy_r9,\n)\nresults.pvalue\n\n\n# ### Measurements\n# \n# In this example, we evaluate the output from `stats.friedmanchisquare` using an `ExternalMeasurement` class, and store the result.\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n\nkruskal_measurement = ExternalMeasurement(\n    \\\"repeated training on training samples\\\", Array, stats.friedmanchisquare\n)\n\n# Evaluate.\nkruskal_res = kruskal_measurement.evaluate(\n    df.Accuracy_r0,\n    df.Accuracy_r1,\n    df.Accuracy_r2,\n    df.Accuracy_r3,\n    df.Accuracy_r4,\n    df.Accuracy_r5,\n    df.Accuracy_r6,\n    df.Accuracy_r7,\n    df.Accuracy_r8,\n    df.Accuracy_r9,\n)\n\n# Inspect values\nprint(kruskal_res)\n\n# Save to artifact store\nkruskal_res.save(force=True)\n\n\n\n",
    "description": "Testing the ability of the ML model to produce similar model outputs when training multiple models on different random samples of the training data",
    "inputs": "Model results from different models, trained on different random sample sets of the training data",
    "output": "P-value from the Friedman test"
}