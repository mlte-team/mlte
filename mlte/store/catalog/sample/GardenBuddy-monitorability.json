{
    "header": {
        "identifier": "GardenBuddy-monitorability",
        "creator": "admin",
        "created": 1762532965,
        "updater": null,
        "updated": 1762532965,
        "catalog_id": "sample"
    },
    "tags": [],
    "quality_attribute": "",
    "code": "# ## 2i. Evidence - Analyzability & Monitorability QAS Measurements\n# \n# Measurements to monitor and detect issues with changes in inputs and outputs.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 8\nprint(card.quality_scenarios[qa].identifier)\nprint(card.quality_scenarios[qa].quality)\nprint(\n    card.quality_scenarios[qa].stimulus,\n    \\\"from \\\",\n    card.quality_scenarios[qa].source,\n    \\\" during \\\",\n    card.quality_scenarios[qa].environment,\n    \\\". \\\",\n    card.quality_scenarios[qa].response,\n    card.quality_scenarios[qa].measure,\n)\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tTest dataset must include values that are known to be out of bounds. \n# \n# **Measurement and Condition:**  Log entries are produced for all OOD inputs.\n# \n# **Context:**\tNormal Operation\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 9\nprint(card.quality_scenarios[qa].identifier)\nprint(card.quality_scenarios[qa].quality)\nprint(\n    card.quality_scenarios[qa].stimulus,\n    \\\"from \\\",\n    card.quality_scenarios[qa].source,\n    \\\" during \\\",\n    card.quality_scenarios[qa].environment,\n    \\\". \\\",\n    card.quality_scenarios[qa].response,\n    card.quality_scenarios[qa].measure,\n)\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tTest dataset must include a holdout class to induce a change in the output confidence.\n# \n# **Measurement and Condition:**\tLog entries are produced when distribution shift is detected.\n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# Prepare all functions and data for the measurements.\n\n# Load model module\nfrom utils import model_predict\n\n\ndef run_and_get_log() -> str:\n    \\\"\\\"\\\"Runs the model and gets the log.\\\"\\\"\\\"\n    model_predict.run_model(OOD_DATASET_DIR, MODEL_FILE_PATH)\n    return model_predict.load_log()\n\n\n# ### Measurement for Analyability\n# \n# Finally, we execute the measurements and store the results.\n\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom mlte.evidence.types.string import String\n\n# Evaluate, identifier has to be the same one defined in the Spec.\nmeasurement = ExternalMeasurement(\\\"detect ood inputs\\\", String, run_and_get_log)\nresult = measurement.evaluate()\n\n# Inspect value\n# print(result)\n\n# Save to artifact store\nresult.save(force=True)\n\n\n# ### Measurement for Monitorability\n# \n# Finally, we execute the measurements and store the results.\n\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom mlte.evidence.types.string import String\n\n# Evaluate, identifier has to be the same one defined in the TestSuite.\nmeasurement = ExternalMeasurement(\n    \\\"monitor output confidence shift\\\", String, run_and_get_log\n)\nresult = measurement.evaluate()\n\n# Inspect value\n# print(result)\n\n# Save to artifact store\nresult.save(force=True)\n\n\n\n",
    "description": "",
    "inputs": "",
    "output": ""
}