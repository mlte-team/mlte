{
    "header": {
        "identifier": "GardenBuddy-resource_utilization",
        "creator": "admin",
        "created": 1763414995,
        "updater": null,
        "updated": 1763414995,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Object detection"
    ],
    "quality_attribute": "Resource Utilization",
    "code": "# ## 2d. Evidence - Resource Utilization QAS Measurements\n# \n# Now we collect stored, CPU and memory usage data when predicting with the model, for the Performance scenario.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 3\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tThe original test dataset can be used (i.e., original test data set aside for testing during data split into training, validation, and test).\n# \n# **Measurement and Condition:**\t    \n#     1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps.\n#     2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap.\n#     3- Disk usage will not exceed allocated disk space of 150 MB bytes. This will be measured by adding the size of each file in the path for the model code.\n# \n# **Context:**\tNormal Operation\n\n# ### Measurements\n# \n# Prepare and execute measurements.\n\nfrom mlte.measurement.storage import LocalObjectSize\nfrom mlte.evidence.types.real import Real\nfrom mlte.measurement.units import Units\n\nstore_measurement = LocalObjectSize(\\\"model size\\\")\nsize: Real = store_measurement.evaluate(MODELS_DIR, unit=Units.byte)\nprint(size)\nsize.save(force=True)\n\n\nfrom mlte.measurement.process_measurement import ProcessMeasurement\nfrom mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics\n\ncpu_measurement = LocalProcessCPUUtilization(\\\"predicting cpu\\\")\ncpu_stats: CPUStatistics = cpu_measurement.evaluate(MODEL_COMMAND)\nprint(cpu_stats)\ncpu_stats.save(force=True)\n\n\nfrom mlte.measurement.memory import (\n    LocalProcessMemoryUtilization,\n    MemoryStatistics,\n)\n\nmem_measurement = LocalProcessMemoryUtilization(\\\"predicting memory\\\")\nmem_stats: MemoryStatistics = mem_measurement.evaluate(MODEL_COMMAND)\nprint(mem_stats)\nmem_stats.save(force=True)\n\n\n# We can also avoid starting the training process twice by using the asynch methods for both measurements. We start the training process once and pass the id to both measurements.\n\nfrom mlte.measurement.cpu import LocalProcessCPUUtilization\nfrom mlte.measurement.memory import LocalProcessMemoryUtilization\nfrom mlte.measurement.process_measurement_group import ProcessMeasurementGroup\n\n# Create measurement group\nmeasurements = ProcessMeasurementGroup()\n\n# Add measurements to group.\nmeasurements.add(LocalProcessCPUUtilization(\\\"predicting cpu\\\"))\nmeasurements.add(LocalProcessMemoryUtilization(\\\"predicting memory\\\"))\n\n# Evaluate the measurements.\nevidences = measurements.evaluate(command=MODEL_COMMAND)\n\n# Get results.\ncpu_stats = evidences[\\\"predicting cpu\\\"]\nmem_stats = evidences[\\\"predicting memory\\\"]\n\n# Inspect values\nprint(cpu_stats)\nprint(mem_stats)\n\n# Save to artifact store\ncpu_stats.save(force=True)\nmem_stats.save(force=True)\n\n\n\n",
    "description": "Evaluating model anility to run adequately on deployment platform",
    "inputs": "Model",
    "output": "Memory usuage, CPU usuage and disk space"
}