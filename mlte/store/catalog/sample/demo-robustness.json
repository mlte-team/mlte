{
    "header": {
        "identifier": "demo-robustness",
        "creator": "admin",
        "created": 1727114774,
        "updater": null,
        "updated": 1761165071,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision"
    ],
    "quality_attribute": "Robustness to Noise (Image Blur)",
    "code": "# ## 2b. Evidence - Robustness QAS Measurements\n# \n# Evidence collected in this section checks for the Robustness scenarios defined in the previous step. Note that some functions will be loaded from external Python files.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom demo.scenarios.session import *\n\n\n# ### Helper Functions\n# \n# General functions and external imports.\n\n# General functions.\nfrom demo.scenarios import garden\nimport pandas as pd\n\n\ndef calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n    # Calculate the base model accuracy result per data label\n    df_pos = (\n        df_results[df_results[\\\"model correct\\\"] == True].groupby(\\\"label\\\").count()\n    )\n    # df_pos.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n    df_neg = (\n        df_results[df_results[\\\"model correct\\\"] == False]\n        .groupby(\\\"label\\\")\n        .count()\n    )\n    # df_neg.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n    df_neg.rename(columns={\\\"model correct\\\": \\\"model incorrect\\\"}, inplace=True)\n    df_res = df_pos.merge(\n        df_neg, right_on=\\\"label\\\", left_on=\\\"label\\\", how=\\\"outer\\\"\n    )\n    df_res.fillna(0, inplace=True)\n    df_res[\\\"model acc\\\"] = df_res[\\\"model correct\\\"] / (\n        df_res[\\\"model correct\\\"] + df_res[\\\"model incorrect\\\"]\n    )\n    df_res[\\\"count\\\"] = df_res[\\\"model correct\\\"] + df_res[\\\"model incorrect\\\"]\n    df_res.drop(columns=[\\\"model correct\\\", \\\"model incorrect\\\"], inplace=True)\n    df_res.head()\n\n    return df_res\n\n\ndef calculate_accuracy_per_set(\n    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n) -> pd.DataFrame:\n    # Calculate the model accuracy per data label for each blurred data set\n    base_filename = \\\"predictions_test\\\"\n    ext_filename = \\\".csv\\\"\n    set_filename = [\\\"_blur2x8\\\", \\\"_blur5x8\\\", \\\"_blur0x8\\\", \\\"_noR\\\", \\\"_noG\\\", \\\"_noB\\\"]\n\n    col_root = \\\"model acc\\\"\n\n    for fs in set_filename:\n        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n        colname = col_root + fs\n\n        df_temp = pd.read_csv(filename)\n        # print(df_temp.head())\n        df_temp = df_temp[[\\\"model correct\\\", \\\"label\\\"]]\n\n        df_pos = (\n            df_temp[df_temp[\\\"model correct\\\"] == True].groupby(\\\"label\\\").count()\n        )\n        # df_pos.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n        df_neg = (\n            df_results[df_results[\\\"model correct\\\"] == False]\n            .groupby(\\\"label\\\")\n            .count()\n        )\n        # df_neg.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n        df_neg.rename(\n            columns={\\\"model correct\\\": \\\"model incorrect\\\"}, inplace=True\n        )\n        df_res2 = df_pos.merge(\n            df_neg,\n            right_on=\\\"label\\\",\n            left_on=\\\"label\\\",\n            how=\\\"outer\\\",\n        ).fillna(0)\n        df_res2.fillna(0, inplace=True)\n\n        df_res2[colname] = df_res2[\\\"model correct\\\"] / (\n            df_res2[\\\"model correct\\\"] + df_res2[\\\"model incorrect\\\"]\n        )\n        df_res2.drop(columns=[\\\"model correct\\\", \\\"model incorrect\\\"], inplace=True)\n\n        # print(f\\\"{fs}_DF_RES={df_res.tail()}\\\")\n        # print(f\\\"{fs}_DF_RES2={df_res2.tail()}\\\")\n        df_res = df_res.merge(\n            df_res2, right_on=\\\"label\\\", left_on=\\\"label\\\", how=\\\"outer\\\"\n        ).fillna(0)\n\n    # df_res.head()\n    return df_res\n\n\ndef print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n    model_acc = sum(df_res[key] * df_res[\\\"count\\\"]) / sum(df_res[\\\"count\\\"])\n    print(name, model_acc)\n\n\n# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.\ndf_results = garden.load_base_results(DATASETS_DIR, \\\"predictions_test.csv\\\")\ndf_results = df_results[[\\\"model correct\\\", \\\"label\\\"]]\ndf_res = calculate_base_accuracy(df_results)\ndf_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\ndf_info = garden.load_taxonomy(DATASETS_DIR)\ndf_all = garden.merge_taxonomy_with_results(df_res, df_info, \\\"label\\\", \\\"Label\\\")\n\n# fill in missing model accuracy data\ndf_all[\\\"model acc_noR\\\"] = df_all[\\\"model acc_noR\\\"].fillna(0)\ndf_all[\\\"model acc_noR\\\"] = df_all[\\\"model acc_noG\\\"].fillna(0)\ndf_all[\\\"model acc_noR\\\"] = df_all[\\\"model acc_noB\\\"].fillna(0)\n\n\n# ### Measurements\n# \n# Now do the actual measurements. First simply see the model accuracy across blurs.\n\n# view changes in model accuracy\nprint_model_accuracy(df_res, \\\"model acc\\\", \\\"base model accuracy\\\")\nprint_model_accuracy(\n    df_res, \\\"model acc_blur2x8\\\", \\\"model accuracy with 2x8 blur\\\"\n)\nprint_model_accuracy(\n    df_res, \\\"model acc_blur5x8\\\", \\\"model accuracy with 5x8 blur\\\"\n)\nprint_model_accuracy(\n    df_res, \\\"model acc_blur0x8\\\", \\\"model accuracy with 0x8 blur\\\"\n)\n\n\ndf_res\n\n\n# Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper.\n\nimport scipy.stats\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\nmy_blur = [\\\"2x8\\\", \\\"5x8\\\", \\\"0x8\\\"]\nfor i in range(len(my_blur)):\n    # Define measurements.\n    ranksum_measurement = ExternalMeasurement(\n        f\\\"ranksums blur{my_blur[i]}\\\", Array, scipy.stats.ranksums\n    )\n\n    # Evaluate.\n    ranksum: Array = ranksum_measurement.evaluate(\n        df_res[\\\"model acc\\\"], df_res[f\\\"model acc_blur{my_blur[i]}\\\"]\n    )\n\n    # Inspect values\n    print(ranksum)\n\n    # Save to artifact store\n    ranksum.save(force=True)\n\n\n# Now to next part of the question- is this equal across the phylogenic groups?\n\n# First we will check the effect of blur for Clade 2.\n\nfrom typing import List\n\nfrom demo.scenarios.evidence.multiple_ranksums import MultipleRanksums\n\n# use the initial result, blur columns to anaylze effect of blur\ndf_all[\\\"delta_2x8\\\"] = df_all[\\\"model acc\\\"] - df_all[\\\"model acc_blur2x8\\\"]\ndf_all[\\\"delta_5x8\\\"] = df_all[\\\"model acc\\\"] - df_all[\\\"model acc_blur5x8\\\"]\ndf_all[\\\"delta_0x8\\\"] = df_all[\\\"model acc\\\"] - df_all[\\\"model acc_blur0x8\\\"]\n\npops = df_all[\\\"Clade2\\\"].unique().tolist()\nblurs = [\n    \\\"delta_2x8\\\",\n    \\\"delta_5x8\\\",\n    \\\"delta_0x8\\\",\n]\n\n\ndef calculate_multiple_ranksums(df_all, pops, blurs):\n    ranksums: List = []\n    for i in range(len(blurs)):\n        for pop1 in pops:\n            for pop2 in pops:\n                ranksum_measurement = ExternalMeasurement(\n                    f\\\"ranksums clade2 {pop1}-{pop2} blur{blurs[i]}\\\",\n                    Array,\n                    scipy.stats.ranksums,\n                )\n                ranksum: Array = ranksum_measurement.evaluate(\n                    df_all[df_all[\\\"Clade2\\\"] == pop1][blurs[i]],\n                    df_all[df_all[\\\"Clade2\\\"] == pop2][blurs[i]],\n                )\n                print(f\\\"blur {blurs[i]}: {ranksum}\\\")\n                ranksums.append({ranksum.identifier: ranksum.array})\n    return ranksums\n\n\nmultiple_ranksums_meas = ExternalMeasurement(\n    f\\\"multiple ranksums for clade2\\\",\n    MultipleRanksums,\n    calculate_multiple_ranksums,\n)\nmultiple_ranksums: MultipleRanksums = multiple_ranksums_meas.evaluate(\n    df_all, pops, blurs\n)\nmultiple_ranksums.num_pops = len(pops)\nmultiple_ranksums.save(force=True)\n\n\ndf_all\n\n\n# Now we check between clade 2 and clade 3.\n\ndf_now = (\n    df_all[[\\\"Clade2\\\", \\\"Clade3\\\"]]\n    .copy()\n    .groupby([\\\"Clade2\\\", \\\"Clade3\\\"])\n    .count()\n    .reset_index()\n)\nps1 = df_now[\\\"Clade2\\\"].to_list()\nps2 = df_now[\\\"Clade3\\\"].to_list()\nprint(df_now)\n\n\ndef calculate_multiple_ranksums_two_clades(df_all, ps1, ps2, blurs):\n    ranksums: List = []\n    for k in range(len(blurs)):\n        print(\\\"\\n\\\", blurs[k])\n        for i in range(len(ps1)):\n            p1c1 = ps1[i]\n            p1c2 = ps2[i]\n            for j in range(len(ps1)):\n                p2c1 = ps1[j]\n                p2c2 = ps2[j]\n                if (\n                    len(\n                        df_all[\n                            (df_all[\\\"Clade2\\\"] == p1c1)\n                            & (df_all[\\\"Clade3\\\"] == p2c2)\n                        ][blurs[k]]\n                    )\n                    > 0\n                    | len(\n                        df_all[\n                            (df_all[\\\"Clade2\\\"] == p2c1)\n                            & (df_all[\\\"Clade3\\\"] == p2c2)\n                        ][blurs[k]]\n                    )\n                    > 0\n                ):\n                    ranksum_measurement = ExternalMeasurement(\n                        f\\\"ranksums {p1c1}-{p2c2} - {p2c1}-{p2c2} blur{blurs[k]}\\\",\n                        Array,\n                        scipy.stats.ranksums,\n                    )\n                    ranksum: Array = ranksum_measurement.evaluate(\n                        df_all[\n                            (df_all[\\\"Clade2\\\"] == p1c1)\n                            & (df_all[\\\"Clade3\\\"] == p2c2)\n                        ][blurs[k]],\n                        df_all[\n                            (df_all[\\\"Clade2\\\"] == p2c1)\n                            & (df_all[\\\"Clade3\\\"] == p2c2)\n                        ][blurs[k]],\n                    )\n                    ranksums.append({ranksum.identifier: ranksum.array})\n    return ranksums\n\n\nmultiple_ranksums_meas = ExternalMeasurement(\n    f\\\"multiple ranksums between clade2 and 3\\\",\n    MultipleRanksums,\n    calculate_multiple_ranksums_two_clades,\n)\nmultiple_ranksums: MultipleRanksums = multiple_ranksums_meas.evaluate(\n    df_all, ps1, ps2, blurs\n)\nmultiple_ranksums.num_pops = len(ps1)\nmultiple_ranksums.save(force=True)\n",
    "description": "The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.",
    "inputs": "three garden populations, model results on Oxford garden data",
    "output": "robustness to noise"
}