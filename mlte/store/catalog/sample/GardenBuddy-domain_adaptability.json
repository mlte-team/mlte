{
    "header": {
        "identifier": "GardenBuddy-domain_adaptability",
        "creator": "admin",
        "created": 1763412968,
        "updater": null,
        "updated": 1763412968,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Image"
    ],
    "quality_attribute": "Domain Adaptability",
    "code": "# ## 2m. Evidence - Domain Adaptability QAS Measurements\n# \n# Evidence collected in this section checks for the Domain Adaptability scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 13\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tThe original test data set and a test data set from the new domain.\n# \n# **Measurement and Condition:**\tThe effect of the new domain on model performance will be assessed using ANOVA on each label set, with significance at p-value < 0.05. \n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# General functions and external imports.\n\n# General functions.\n\nfrom utils import garden\nimport pandas as pd\nfrom os import path\nfrom scipy.stats import f_oneway\n\n\ndef load_data(data_folder: str, data_file: str):\n    \\\"\\\"\\\"Loads all garden data results and taxonomy categories.\\\"\\\"\\\"\n    df_results = garden.load_base_results(data_folder, data_file)\n    df_results.head()\n\n    # Load the taxonomic data and merge with results.\n    df_info = garden.load_taxonomy(data_folder)\n    df_results.rename(columns={\\\"label\\\": \\\"Label\\\"}, inplace=True)\n    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n\n    return df_info, df_all\n\n\n# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\n\ndf_info, df_test = load_data(DATASETS_DIR, \\\"0abcflmn_cv_output.csv\\\")\ndf_info, df_new = load_data(\n    DATASETS_DIR, \\\"0n_cv_output_domain_adaptability.csv\\\"\n)\ndf_test[\\\"dataset\\\"] = \\\"DALL-E-2\\\"\ndf_new[\\\"dataset\\\"] = \\\"Test\\\"\ndf_all = pd.concat([df_new, df_test], ignore_index=True)\n\n\nvalid_labels = (\n    df_all.groupby([\\\"Label\\\", \\\"dataset\\\"]).size().unstack().index.tolist()\n)\n\n\ndef run_anova_for_label(df, label):\n    # Perform ANOVA for a specific label\n    subset = df[df[\\\"Label\\\"] == label]\n    test_vals = subset[subset[\\\"dataset\\\"] == \\\"Test\\\"][\\\"label_prob\\\"]\n    dalle_vals = subset[subset[\\\"dataset\\\"] == \\\"DALL-E-2\\\"][\\\"label_prob\\\"]\n\n    f_stat, p_val = f_oneway(test_vals, dalle_vals)\n\n    return {\n        \\\"label\\\": label,\n        \\\"f_stat\\\": f_stat,\n        \\\"p_val\\\": p_val,\n    }\n\n\ndef run_anova(df_all):\n    anova_results = [\n        run_anova_for_label(df_all, label) for label in valid_labels\n    ]\n    results_df = pd.DataFrame(anova_results)\n    results_df.sort_values(by=\\\"label\\\", inplace=True)\n    results_df.set_index(\\\"label\\\", inplace=True)\n    return results_df\n\n\ndef run_anova2(df_all):\n    res_df = run_anova(df_all)\n\n    return res_df.to_numpy()\n\n\n# Run ANOVA\n\nresults_df = run_anova(df_all)\n\nresults_df\n\n\nrun_anova(df_all)\n\n\n# ### Measurements\n# \n# In this example, we evaluate the output from our custom `calculate_multiple_anova` using an `ExternalMeasurement` class, and store the result.\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom evidence.multiple_ranksums import MultipleRanksums\n\n\ndef calculate_multiple_anova(df_all):\n    evid: list = []\n    # print(df_all.columns)\n\n    labels = df_all.Label.unique()\n\n    for lab in labels:\n\n        subset = df_all[df_all[\\\"Label\\\"] == lab]\n        test_vals = subset[subset[\\\"dataset\\\"] == \\\"Test\\\"][\\\"label_prob\\\"]\n        dalle_vals = subset[subset[\\\"dataset\\\"] == \\\"DALL-E-2\\\"][\\\"label_prob\\\"]\n\n        # f_oneway(test_vals, dalle_vals)\n\n        anova_measurement = ExternalMeasurement(\n            f\\\"label {lab}\\\",\n            Array,\n            f_oneway,\n        )\n        anova: Array = anova_measurement.evaluate(\n            test_vals,\n            dalle_vals,\n        )\n\n        evid.append({anova.identifier: anova.array})\n    return evid\n\n\nmultiple_anova_meas = ExternalMeasurement(\n    \\\"running in new domain\\\",\n    MultipleRanksums,\n    calculate_multiple_anova,\n)\nmultiple_anova: MultipleRanksums = multiple_anova_meas.evaluate(df_all)\n\nmultiple_anova.save(force=True)\n\n\n\n",
    "description": "Assessing the effect of input data from a new domain on model performance",
    "inputs": "Distribution of model inferences of images taken in a few operating domain; distribution of model inferences from images taken from the origional operating domain",
    "output": "ANOVA test results"
}