{
    "header": {
        "identifier": "demo-interoperability",
        "creator": "admin",
        "created": 1727114774,
        "updater": null,        
        "updated": -1,
        "catalog_id": "sample"
    },
    "tags": ["General"],
    "property_category": "Interoperability",
    "property": "Input and Output Specification",
    "code_type": "measurement",
    "code": "# %% [markdown]\n# ## Interoperability QAS Measurements\n# \n# Measure proper input specification.\n\n# %% [markdown]\n# ### Helper Functions\n# \n# Prepare all functions and data for the measurements.\n\n# %%\n# Load model module\nfrom demo.scenarios import model_predict\n\n\ndef run_and_get_log() -> str:\n    \"\"\"Runs the model and gets the log.\"\"\"\n    model_predict.run_model(SAMPLE_DATASET_DIR, MODEL_FILE_PATH, MODEL_WEIGHTS_PATH)\n\n    return model_predict.load_log()\n\n# %% [markdown]\n# ### Measurements\n# \n# Finally, we execute the measurements and store the results.\n\n# %%\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom demo.scenarios.values.string import String\n\n# Evaluate, identifier has to be the same one defined in the Spec.\nmeasurement = ExternalMeasurement(\n    \"output format validation\", String, run_and_get_log\n)\nresult = measurement.evaluate()\n\n# Inspect value\nprint(result)\n\n# Save to artifact store\nresult.save(force=True)\n",
    "description": "The model reads inputs and provides outputs according to established input and output specifications during normal operation. During test execution all data in the test dataset produces an output that conforms to the output specification.",
    "inputs": "existing ML model, sample image data",
    "output": "log without validation issues"
}
