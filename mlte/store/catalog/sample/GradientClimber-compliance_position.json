{
    "header": {
        "identifier": "GradientClimber-compliance_position",
        "creator": "admin",
        "created": 1762976437,
        "updater": null,
        "updated": 1762976437,
        "catalog_id": "sample"
    },
    "tags": [
        "Reinforcement Learning"
    ],
    "quality_attribute": "Car controlled by model does not exceed position limits.",
    "code": "# ## 2a. Evidence - Compliance - Position\n# \n# Evidence collected in this section checks for position complience in the Gradient Climber Example\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scnario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 2\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tVehicle state (position and velocity) from sensors (or approximated by simulation engine in development)\n# \n# **Measurement and Condition:**\t\u22121.2\u2264\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\u22640.6 at all times\n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# \n\nMEASURE_NAME = \\\"gradient climber position accuracy\\\"\nNUM_TRIALS = 100\nPOS_MIN = -1.2\nPOS_MAX = 0.6\n\n\nimport numpy as np\nimport gymnasium as gym\n\n\nenv = gym.make(\\\"MountainCar-v0\\\", render_mode=\\\"rgb_array\\\")\nstate, info = env.reset()\n\n\n# Discretize the state space (position, velocity)\nposition_bins = np.linspace(-1.2, 0.6, 20)\nvelocity_bins = np.linspace(-0.07, 0.07, 20)\n\n# Q-table initialization\nq_table = np.load(os.path.join(DATA_DIR, \\\"mountain_car.npy\\\"))\n\n\n# Discretize the continuous state (position and velocity)\ndef discretize_state(state):\n    position, velocity = state\n    position_idx = (\n        np.digitize(position, position_bins) - 1\n    )  # Position bin index\n    velocity_idx = (\n        np.digitize(velocity, velocity_bins) - 1\n    )  # Velocity bin index\n    return position_idx, velocity_idx\n\n\n# Epsilon-greedy action selection\ndef choose_action(state):\n    position_idx, velocity_idx = discretize_state(state)\n    return np.argmax(q_table[position_idx, velocity_idx])\n\n\n# ## Compliance\n\ndef evaluate_action(state, action):\n    \\\"Return 1 if this is the expected action, return 0 if it is the wrong move, and -1 as an error condition\\\"\n    position, velocity = state\n    if (position >= POS_MIN) and (position <= POS_MAX):\n        return np.bool(True)\n    else:\n        return np.bool(False)\n\n\ndef test_position_compliance():\n    done = False\n    states = []\n    test_results = []\n\n    for i in range(NUM_TRIALS):\n        state, info = env.reset()\n        done = False\n        actions = []\n        num_steps = 0\n        num_ok_steps = 0\n        # print(f\\\"run {i}: s={state}\\\")\n\n        while not done:\n            # Random action selection\n            action = choose_action(state)\n            states.append(state)\n            # Take the action and get the next state, reward, done flag, and info\n            next_state, reward, done, truncated, info = env.step(action)\n\n            result = evaluate_action(state, action)\n            if result == True:\n                test_results.append(1)\n                num_ok_steps += 1\n            elif result == False:\n                test_results.append(0)\n            # Update the state for the next iteration\n            state = next_state\n            num_steps += 1\n\n        print(f\\\"Completed trial {i}: {num_ok_steps/num_steps}\\\")\n\n    return test_results\n\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\nposition_compliance_measurement = ExternalMeasurement(\n    MEASURE_NAME, Array, test_position_compliance\n)\nevidence = position_compliance_measurement.evaluate()\n\n# Inspect value\nprint(evidence)\n\n# Save to artifact store\nevidence.save(force=True, parents=True)\n",
    "description": "Regardless of input, the model will not produce output values that cause the vehicle to exit the area of operation set in the operational configuration.",
    "inputs": "Initial random start position",
    "output": "Log with 1 for in-bounds, and 0 for out-of-bounds"
}