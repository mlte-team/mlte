{
    "header": {
        "identifier": "GradientClimber-reliability",
        "creator": "admin",
        "created": 1762876669,
        "updater": null,
        "updated": 1762876669,
        "catalog_id": "sample"
    },
    "tags": [
        "Reinforcement Learning"
    ],
    "quality_attribute": "Model outputs improve progress towards goal 99.9% of the time.",
    "code": "# \n# ## 2c. Evidence - Reliability\n# \n# Evidence collected in this section checks for functional correctness in the Reliability Example\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 1\nprint(card.quality_scenarios[qa].identifier)\nprint(card.quality_scenarios[qa].quality)\nprint(\n    card.quality_scenarios[qa].stimulus,\n    \\\"from \\\",\n    card.quality_scenarios[qa].source,\n    \\\" during \\\",\n    card.quality_scenarios[qa].environment,\n    \\\". \\\",\n    card.quality_scenarios[qa].response,\n    card.quality_scenarios[qa].measure,\n)\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tVehicle state (position and velocity) from sensors (or approximated by simulation engine in development)\n# \n# **Measurement and Condition:**\tCorrectness is measured for all outputs of multiple runs using a heuristic is provided by the evaluate_action implemented in python. \n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n\nMEASURE_NAME = \\\"reliability\\\"\nNUM_TRIALS = 100\n\n\nimport numpy as np\nimport gymnasium as gym\n\n\nenv = gym.make(\\\"MountainCar-v0\\\", render_mode=\\\"rgb_array\\\")\nstate, info = env.reset()\n\n\n# Discretize the state space (position, velocity)\nposition_bins = np.linspace(-1.2, 0.6, 20)\nvelocity_bins = np.linspace(-0.07, 0.07, 20)\n\n# Q-table initialization\nq_table = np.load(os.path.join(DATA_DIR, \\\"mountain_car.npy\\\"))\n\n\n# Discretize the continuous state (position and velocity)\ndef discretize_state(state):\n    position, velocity = state\n    position_idx = (\n        np.digitize(position, position_bins) - 1\n    )  # Position bin index\n    velocity_idx = (\n        np.digitize(velocity, velocity_bins) - 1\n    )  # Velocity bin index\n    return position_idx, velocity_idx\n\n\n# Epsilon-greedy action selection\ndef choose_action(state):\n    position_idx, velocity_idx = discretize_state(state)\n    return np.argmax(q_table[position_idx, velocity_idx])\n\n\n# ## Reliability\n# \n# Agent receives valid values from sensors during Normal Operation. Agent produces actions which improve the expected reward 99.9% of the time.\n\ndef evaluate_action(state, action):\n    \\\"Return 1 if this is the expected action, return 0 if it is the wrong move, and -1 as an error condition\\\"\n    position, velocity = state\n    if (position < 0.1) & (velocity < 0):\n        return np.bool(action == 0)\n    if (position < 0.1) & (velocity > 0):\n        return np.bool(action == 2)\n    if (position > 0.1) & (velocity > 0):\n        return np.bool(action == 0)\n    if (position < 0.1) & (velocity > 0):\n        return np.bool(action == 2)\n    return -1\n\n\ndef test_reliability():\n    done = False\n    total_reward = 0\n    actions = []\n    test_results = []\n\n    for i in range(NUM_TRIALS):\n        state, info = env.reset()\n        done = False\n\n        while not done:\n            # Random action selection\n            action = choose_action(state)\n            actions.append(action)\n\n            # Take the action and get the next state, reward, done flag, and info\n            next_state, reward, done, truncated, info = env.step(action)\n\n            # Evaluate the results\n            result = evaluate_action(state, action)\n            if result == True:\n                test_results.append(1)\n            elif result == False:\n                test_results.append(0)\n\n            # Update the state for the next iteration\n            state = next_state\n        print(f\\\"Completed trial {i}\\\")\n\n    return test_results\n\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n# Evaluate accuracy, identifier has to be the same one defined in the TestSuite.\nposition_compliance_measurement = ExternalMeasurement(\n    MEASURE_NAME, Array, test_reliability\n)\nevidence = position_compliance_measurement.evaluate()\n\n# Inspect value\nprint(evidence)\n\n# Save to artifact store\nevidence.save(force=True, parents=True)\n",
    "description": "Model receives valid values from sensors during Normal Operation and produces outputs (actions) which improve the expected reward 99.9% of the time.  ",
    "inputs": "Initial random start position",
    "output": "Log with 1 for action determined to make progress, and 0 for those that do not."
}