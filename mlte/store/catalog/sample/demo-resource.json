{
    "header": {
        "identifier": "demo-resource",
        "creator": "admin",
        "created": 1727114774,
        "updater": null,        
        "updated": -1,
        "catalog_id": "sample"
    },
    "tags": ["General"],
    "property_category": "Resource Consumption",
    "property": "Performance on Operational Platform",
    "code_type": "measurement",
    "code": "# %% [markdown]\n# ### Performance QAS Measurements\n# \n# Now we collect stored, CPU and memory usage data when predicting with the model, for the Performance scenario. NOTE: the version of tensorflow used in this demo requires running it under Python 3.9 or higher.\n\n# %%\n# This is the external script that will load and run the model for inference/prediction.\nscript = Path.cwd() / \"model_predict.py\"\nargs = [\n    \"--images\",\n    SAMPLE_DATASET_DIR,\n    \"--model\",\n    MODELS_DIR / \"model_f3_a.json\",\n    \"--weights\",\n    MODELS_DIR / \"model_f_a.h5\",\n]\n\n# %%\n# This is the actual external script that will load and run the model for inference/prediction. Should be stored on model_predict.py\nimport argparse\nimport os\nimport sys\nimport time\nfrom resource import *\n\n# Command line args\nparser = argparse.ArgumentParser(\n    description=\"Command line arguments for the model performance tester.\"\n)\nparser.add_argument(\n    \"--images\",\n    help=\"The directory that contains the images to use for testing model performance.\",\n    required=True,\n)\n# --model file example: model_f3_a.json\nparser.add_argument(\n    \"--model\", help=\"The json formatted model file.\", required=True\n)\n# --weights file example: model_f_a.h5\nparser.add_argument(\n    \"--weights\", help=\"The file that contains the model weights.\", required=True\n)\nargs = parser.parse_args()\n\n# getrusage returns Kibibytes on linux and bytes on MacOS\nr_mem_units_str = \"KiB\" if sys.platform.startswith(\"linux\") else \"bytes\"\n\nimport tensorflow as tf\n\nprint(\"TensorFlow version:\", tf.__version__)\n\nfrom tensorflow.keras.models import model_from_json\n\n# Load dataset\ndataset = tf.keras.utils.image_dataset_from_directory(\n    args.images,\n    image_size=(224, 224),\n    labels=None,\n    label_mode=\"categorical\",\n    batch_size=1,\n    shuffle=True,\n)\n\nru1 = getrusage(RUSAGE_SELF).ru_maxrss\n\n# Load model\njson_file = open(args.model, \"r\")\nloaded_model_json = json_file.read()\nloaded_model = model_from_json(loaded_model_json)\njson_file.close()\n\n# Load weights into new model\nloaded_model.load_weights(args.weights)\nprint(\"Loaded model from disk!\")\n\nru2 = getrusage(RUSAGE_SELF).ru_maxrss\n\nprint(loaded_model.summary())\n\nmfile_size = os.path.getsize(args.model)\nwfile_size = os.path.getsize(args.weights)\n\nprint(f\"Size of model json file ({args.model}): {mfile_size} bytes\")\nprint(f\"Size of weights file ({args.weights}): {wfile_size} bytes\")\nprint(\n    f\"Memory used for the entire model loading process: {ru2 - ru1} {r_mem_units_str}.\"\n)\n\ntotal_elapsed_time = 0.0\ntotal_inference_memory = 0.0\nnum_samples = len(dataset)\nprint(f\"Running inference on {num_samples} samples...\")\n\nfor image in dataset:\n    start = time.time()\n    ru3 = getrusage(RUSAGE_SELF).ru_maxrss\n    _ = loaded_model.predict(image)\n    ru4 = getrusage(RUSAGE_SELF).ru_maxrss\n    end = time.time()\n\n    elapsed_time = end - start\n    inference_memory = ru4 - ru3\n    total_elapsed_time += elapsed_time\n    total_inference_memory += inference_memory\n\navg_elapsed_time = total_elapsed_time / num_samples\navg_inference_memory = total_inference_memory / num_samples\nprint(\"\\n--- STATISTICS ---\")\nprint(\n    \"Average elapsed time per inference: {0:.5f} seconds\".format(\n        avg_elapsed_time\n    )\n)\nprint(\n    \"Average memory used per inference: {0:.5f} {1}.\".format(\n        avg_inference_memory, r_mem_units_str\n    )\n)\n\n# %%\nfrom mlte.measurement.storage import LocalObjectSize\nfrom mlte.value.types.integer import Integer\n\nstore_measurement = LocalObjectSize(\"model size\")\nsize: Integer = store_measurement.evaluate(MODELS_DIR)\nprint(size)\nsize.save(force=True)\n\n# %%\nfrom mlte.measurement.process_measurement import ProcessMeasurement\nfrom mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics\n\ncpu_measurement = LocalProcessCPUUtilization(\"predicting cpu\")\ncpu_stats: CPUStatistics = cpu_measurement.evaluate(\n    ProcessMeasurement.start_script(script, args)\n)\nprint(cpu_stats)\ncpu_stats.save(force=True)\n\n# %%\nfrom mlte.measurement.memory import (\n    LocalProcessMemoryConsumption,\n    MemoryStatistics,\n)\n\nmem_measurement = LocalProcessMemoryConsumption(\"predicting memory\")\nmem_stats: MemoryStatistics = mem_measurement.evaluate(\n    ProcessMeasurement.start_script(script, args)\n)\nprint(mem_stats)\nmem_stats.save(force=True)\n",
    "description": "The model will need to run on the devices loaned out by the garden centers to visitors. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively). The original test dataset can be used. 1- Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using ps. 2- Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using pmap. 3 - Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the model code.",
    "inputs": "existing garden ML model, sample image",
    "output": "cpu usage, memory usage, storage size"
}