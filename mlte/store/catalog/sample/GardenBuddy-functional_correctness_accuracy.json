{
    "header": {
        "identifier": "GardenBuddy-functional_correctness_accuracy",
        "creator": "admin",
        "created": 1763412917,
        "updater": null,
        "updated": 1763412917,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision",
        "Object detection"
    ],
    "quality_attribute": "Functional Correctness",
    "code": "# ## 2f. Evidence - Functional Correctness - Accuracy QAS Measurements\n# \n# Measure accuracy of the model.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 5\nprint(card.quality_scenarios[qa])\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tA validation dataset with feature and label distributions broadly similar to the test dataset.\n# \n# **Measurement and Condition:**\tThe accuracy of the model on the entire validation set higher or equal to 0.9.\n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# \n# Prepare all functions and data for the measurements.\n\nfrom utils import garden\nimport numpy as np\n\n\ndef load_data(data_folder: str):\n    \\\"\\\"\\\"Loads all garden data results and taxonomy categories.\\\"\\\"\\\"\n    df_results = garden.load_base_results(data_folder, \\\"0abcflmn_cv_output.csv\\\")\n    df_results.head()\n\n    # Load the taxonomic data and merge with results.\n    df_info = garden.load_taxonomy(data_folder)\n    df_results.rename(columns={\\\"label\\\": \\\"Label\\\"}, inplace=True)\n    df_all = garden.merge_taxonomy_with_results(df_results, df_info)\n\n    return df_results\n\n\ndef calculate_model_performance_basic_acc(df_results):\n    \\\"\\\"\\\"Get basic accucray of model across the entire garden\\\"\\\"\\\"\n    n, d = df_results.shape\n    model_performance_acc = np.sum(df_results[\\\"model correct\\\"]) / n\n\n    return model_performance_acc\n\n\n# Prepare the data. For this section, instead of executing the model, we will use CSV files containing the results of an already executed run of the model.\ndf_results = load_data(DATASETS_DIR)\n\n\nn, d = df_results.shape\n\n\ncalculate_model_performance_basic_acc(df_results)\n\n\n# ### Measurements\n# \n# Finally, we execute the measurements and store the results.\n\nfrom mlte.measurement.external_measurement import ExternalMeasurement\nfrom mlte.evidence.types.real import Real\n\n# Evaluate, identifier has to be the same one defined in the TestSuite.\nmeasurement = ExternalMeasurement(\n    \\\"overall model accuracy\\\", Real, calculate_model_performance_basic_acc\n)\nresult = measurement.evaluate(df_results)\n\n# Inspect value\nprint(result)\n\n# Save to artifact store\nresult.save(force=True)\n\n\n\n",
    "description": "Calculates accuracy on multi-lable data set",
    "inputs": "Model predicted data labels,actual data labels",
    "output": "Accuracy"
}