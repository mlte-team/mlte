{
    "header": {
        "identifier": "ReviewPro-inclusivity",
        "creator": "admin",
        "created": 1763413908,
        "updater": null,
        "updated": 1763413908,
        "catalog_id": "sample"
    },
    "tags": [
        "LLM",
        "content generation"
    ],
    "quality_attribute": "Inclusivity",
    "code": "# ## 2g. Evidence - Inclusivity QAS Measurements\n# \n# Evidence collected in this section checks for the inclusivity QAS scenario defined in the previous step. Note that some functions and data will be loaded from external Python files.\n# \n# The cell below must contain JSON data about this evidence that will be used to automatically populate the sample test catalog.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 7\nprint(card.quality_scenarios[qa])\n\n\n# ### A Specific test case generated from the scenario:\n# \n# **Data and Data Source:**\tThe self evaluations in the original test data set will be used to generate a number of contextually similar but reading level different self-evaluations, which all convey the same information but have dramatically different Flesch-Kincade grade level or Flesch reading ease score. \n# \n# **Measurement and Condition:**\tThe evaluation text should be contextually similar across each evaluation set, but the readability of the text should change. The influence of the readability will be measured using 2-way ANOVA (with prompt group and a readability score being the 2 factors), with significance of p<0.05. The text readability will be measured by the Flesch-Kincade Grade level or the Flesch Reading Ease score.\n# \n# **Context:**\tNormal Operation\n\n# ### Gather evidence\n\nimport numpy as np\nimport pandas as pd\nfrom os import path\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\n# Read the files with with the necessary input data and LLM evaluation results\ninput_df = pd.read_csv(\n    os.path.join(DATASETS_DIR, \\\"2h_llm_input_inclusivity.csv\\\")\n)\n\noutput_df = pd.read_csv(\n    path.join(DATASETS_DIR, \\\"2h_llm_output_inclusivity.csv\\\")\n)  # data file with LLm results\n\ncombo_df = pd.merge(\n    input_df, output_df, left_on=\\\"Unnamed: 0\\\", right_on=\\\"Unnamed: 0\\\"\n)\n\ncombo_df = combo_df[\n    [\n        \\\"evaluationOutput\\\",\n        \\\"extractedOverallRating\\\",\n        \\\"PromptGroupNum\\\",\n        \\\"Flesch-Kincade Grade Level\\\",\n        \\\"Flesch Reading Ease Score\\\",\n    ]\n]\ncombo_df.rename(\n    columns={\n        \\\"extractedOverallRating\\\": \\\"overallRating\\\",\n        \\\"Flesch-Kincade Grade Level\\\": \\\"FKGrade\\\",\n        \\\"Flesch Reading Ease Score\\\": \\\"FReadingScore\\\",\n    },\n    inplace=True,\n)\n\ncombo_df.head()\n\n\n# take a subset of teh data; and make sure are the right type\ncombo_df2 = combo_df[\n    [\\\"overallRating\\\", \\\"PromptGroupNum\\\", \\\"FKGrade\\\", \\\"FReadingScore\\\"]\n]\ncombo_df2 = combo_df2.astype(\n    {\n        \\\"overallRating\\\": int,\n        \\\"PromptGroupNum\\\": str,\n        \\\"FKGrade\\\": float,\n        \\\"FReadingScore\\\": float,\n    }\n)\ncombo_df2\n\n\n# ### Save evidence to the specicified scenario\n\n# run test, collect p-values\n\n\ndef run_statsmodel_lm(combo_df2):\n\n    model = ols(\n        \\\"overallRating ~ C(PromptGroupNum) + FReadingScore+ C(PromptGroupNum):FReadingScore\\\",\n        data=combo_df2,\n    ).fit()\n    res = sm.stats.anova_lm(model, typ=2)\n\n    print(res)\n    if res[\\\"PR(>F)\\\"].loc[\\\"FReadingScore\\\"] < 0.05:\n        print(\\\"fail test\\\")\n    else:\n        print(\\\"pass test\\\")\n\n    f_rs = res[\\\"F\\\"].loc[\\\"FReadingScore\\\"]\n    p_rs = res[\\\"PR(>F)\\\"].loc[\\\"FReadingScore\\\"]\n\n    return [float(p_rs)]\n\n\nres = run_statsmodel_lm(combo_df2)\nprint(res)\n\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\nam_measurement = ExternalMeasurement(\n    \\\"eval not dependent on writing level\\\", Array, run_statsmodel_lm\n)\n\n# evaluate\nresult = am_measurement.evaluate(combo_df2)\n\nprint(result)\nresult.save(force=True)\n\n\n\n",
    "description": "An assessment on if the generated reviews are influenced by employee writing levels",
    "inputs": "A request for a review in a crafted prompt, with the supporting material (employee statement, goals and objectives, and manager comments), where the writing levels of the emplpoyee statements have been edited to be of a wide range",
    "output": "ANOVA results on extracted overall evaluation scores from the generated reviews"
}