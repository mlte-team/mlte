{
    "header": {
        "identifier": "GardenBuddy-robustness",
        "creator": "admin",
        "created": 1762354665,
        "updater": null,
        "updated": 1762354665,
        "catalog_id": "sample"
    },
    "tags": [
        "Computer Vision"
    ],
    "quality_attribute": "Robustness to Noise (Image Blur)",
    "code": "# ## 2b. Evidence - Robustness QAS Measurements\n# \n# Evidence collected in this section checks for the Robustness scenario defined in the previous step. Note that some functions will be loaded from external Python files.\n\n# ### Initialize MLTE Context\n# \n# MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. This import will also set up global constants related to folders and model to use.\n\n# Sets up context for the model being used, sets up constants related to folders and model data to be used.\nfrom session import *\n\n\n# ### Set up scenario test case\n\nfrom mlte.negotiation.artifact import NegotiationCard\n\ncard = NegotiationCard.load()\nqa = 1\nprint(card.quality_scenarios[qa].identifier)\nprint(card.quality_scenarios[qa].quality)\nprint(\n    card.quality_scenarios[qa].stimulus,\n    \\\"from \\\",\n    card.quality_scenarios[qa].source,\n    \\\" during \\\",\n    card.quality_scenarios[qa].environment,\n    \\\". \\\",\n    card.quality_scenarios[qa].response,\n    card.quality_scenarios[qa].measure,\n)\n\n\n# **A Specific test case generated from the scenario:**\n# \n# **Data and Data Source:**\tTest data needs to include blurred flower images.  Test blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur.\n# \n# **Measurement and Condition:**\tBlurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n# \n# **Context:**\tNormal Operation\n\n# ### Helper Functions\n# \n# General functions and external imports.\n\n# General functions.\nimport utils.garden as garden\nimport pandas as pd\n\n\ndef calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n    # Calculate the base model accuracy result per data label\n    df_pos = (\n        df_results[df_results[\\\"model correct\\\"] == True].groupby(\\\"label\\\").count()\n    )\n    # df_pos.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n    df_neg = (\n        df_results[df_results[\\\"model correct\\\"] == False]\n        .groupby(\\\"label\\\")\n        .count()\n    )\n    # df_neg.drop(columns=[\\\"predicted_label\\\"], inplace=True)\n    df_neg.rename(columns={\\\"model correct\\\": \\\"model incorrect\\\"}, inplace=True)\n    df_res = df_pos.merge(\n        df_neg, right_on=\\\"label\\\", left_on=\\\"label\\\", how=\\\"outer\\\"\n    )\n    df_res.fillna(0, inplace=True)\n    df_res[\\\"model acc\\\"] = df_res[\\\"model correct\\\"] / (\n        df_res[\\\"model correct\\\"] + df_res[\\\"model incorrect\\\"]\n    )\n    df_res[\\\"count\\\"] = df_res[\\\"model correct\\\"] + df_res[\\\"model incorrect\\\"]\n    df_res.drop(columns=[\\\"model correct\\\", \\\"model incorrect\\\"], inplace=True)\n    df_res.head()\n\n    return df_res\n\n\ndef calculate_accuracy_per_set(\n    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n) -> pd.DataFrame:\n    # Calculate the model accuracy per data label for each blurred data set\n    base_filename = \\\"predictions_test\\\"\n    ext_filename = \\\".csv\\\"\n    set_filename = [\\\"_blur2x8\\\", \\\"_blur5x8\\\", \\\"_blur0x8\\\"]\n\n    col_root = \\\"model acc\\\"\n\n    for fs in set_filename:\n        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n        colname = col_root + fs\n\n        df_temp = pd.read_csv(filename)\n        df_temp = df_temp[[\\\"model correct\\\", \\\"label\\\"]]\n\n        df_pos = (\n            df_temp[df_temp[\\\"model correct\\\"] == True].groupby(\\\"label\\\").count()\n        )\n        df_neg = (\n            df_results[df_results[\\\"model correct\\\"] == False]\n            .groupby(\\\"label\\\")\n            .count()\n        )\n        df_neg.rename(\n            columns={\\\"model correct\\\": \\\"model incorrect\\\"}, inplace=True\n        )\n        df_res2 = df_pos.merge(\n            df_neg,\n            right_on=\\\"label\\\",\n            left_on=\\\"label\\\",\n            how=\\\"outer\\\",\n        ).fillna(0)\n        df_res2.fillna(0, inplace=True)\n\n        df_res2[colname] = df_res2[\\\"model correct\\\"] / (\n            df_res2[\\\"model correct\\\"] + df_res2[\\\"model incorrect\\\"]\n        )\n        df_res2.drop(columns=[\\\"model correct\\\", \\\"model incorrect\\\"], inplace=True)\n\n        df_res = df_res.merge(\n            df_res2, right_on=\\\"label\\\", left_on=\\\"label\\\", how=\\\"outer\\\"\n        ).fillna(0)\n\n    return df_res\n\n\ndef print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n    model_acc = sum(df_res[key] * df_res[\\\"count\\\"]) / sum(df_res[\\\"count\\\"])\n    print(name, model_acc)\n\n\n# Prepare all data. Same as the case above, we will use CSV files that contain results of a previous execution of the model.\ndf_results = garden.load_base_results(DATASETS_DIR, \\\"predictions_test.csv\\\")\ndf_results = df_results[[\\\"model correct\\\", \\\"label\\\"]]\ndf_res = calculate_base_accuracy(df_results)\ndf_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)\ndf_info = garden.load_taxonomy(DATASETS_DIR)\ndf_all = garden.merge_taxonomy_with_results(df_res, df_info, \\\"label\\\", \\\"Label\\\")\n\n\n# ### Measurements\n# \n# Now do the actual measurements. First simply see the model accuracy across blurs.\n\n# view changes in model accuracy\nprint_model_accuracy(df_res, \\\"model acc\\\", \\\"base model accuracy\\\")\nprint_model_accuracy(\n    df_res, \\\"model acc_blur2x8\\\", \\\"model accuracy with 2x8 blur\\\"\n)\nprint_model_accuracy(\n    df_res, \\\"model acc_blur5x8\\\", \\\"model accuracy with 5x8 blur\\\"\n)\nprint_model_accuracy(\n    df_res, \\\"model acc_blur0x8\\\", \\\"model accuracy with 0x8 blur\\\"\n)\n\n\n# Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper.\n\nimport scipy.stats\n\nfrom mlte.evidence.types.array import Array\nfrom mlte.measurement.external_measurement import ExternalMeasurement\n\n\nmy_blur = [\\\"2x8\\\", \\\"5x8\\\", \\\"0x8\\\"]\nfor i in range(len(my_blur)):\n    # Define measurements.\n    ranksum_measurement = ExternalMeasurement(\n        f\\\"ranksums blur{my_blur[i]}\\\", Array, scipy.stats.ranksums\n    )\n\n    # Evaluate.\n    ranksum: Array = ranksum_measurement.evaluate(\n        df_res[\\\"model acc\\\"], df_res[f\\\"model acc_blur{my_blur[i]}\\\"]\n    )\n    print(f\\\"blur {my_blur[i]}: {ranksum}\\\")\n\n    # Inspect values\n    print(ranksum)\n\n    # Save to artifact store\n    ranksum.save(force=True)\n\n\n# Now to next part of the question- is this equal across the phylogenic groups?\n\n# To do that, we will check for differences of the effect of the blur between families, using the phylohentic grouping of the plant pictures to stratify the data\n\nfrom typing import List\n\nfrom evidence.multiple_ranksums import MultipleRanksums\n\n# use the initial result, blur columns to anaylze effect of blur\ndf_all[\\\"delta_2x8\\\"] = df_all[\\\"model acc\\\"] - df_all[\\\"model acc_blur2x8\\\"]\ndf_all[\\\"delta_5x8\\\"] = df_all[\\\"model acc\\\"] - df_all[\\\"model acc_blur5x8\\\"]\ndf_all[\\\"delta_0x8\\\"] = df_all[\\\"model acc\\\"] - df_all[\\\"model acc_blur0x8\\\"]\n\npops = df_all[\\\"Order\\\"].unique().tolist()\nblurs = [\n    \\\"delta_2x8\\\",\n    \\\"delta_5x8\\\",\n    \\\"delta_0x8\\\",\n]\n\n\ndef run_ranksum(samp1, samp2):\n    res = scipy.stats.ranksums(samp1, samp2)\n    float_list = [float(x) for x in res]\n    # print(float(res))\n    return float_list\n\n\ndef calculate_multiple_ranksums(df_all, pops, blurs):\n    ranksums: List = []\n    for i in range(len(blurs)):\n        for p1 in range(len(pops)):  # pop1 in pops:\n            pop1 = pops[p1]\n            for p2 in range(p1, len(pops)):  # pop2 in pops:\n                pop2 = pops[p2]\n                ranksum_measurement = ExternalMeasurement(\n                    f\\\"ranksums Order {pop1}-{pop2} blur{blurs[i]}\\\",\n                    Array,\n                    run_ranksum,  # scipy.stats.ranksums,\n                )\n                ranksum: Array = ranksum_measurement.evaluate(\n                    df_all[df_all[\\\"Order\\\"] == pop1][blurs[i]],\n                    df_all[df_all[\\\"Order\\\"] == pop2][blurs[i]],\n                )\n                # print(f\\\"blur {blurs[i]}: {ranksum}\\\")\n\n                ranksums.append({ranksum.identifier: ranksum.array})\n    return ranksums\n\n\nmultiple_ranksums_meas = ExternalMeasurement(\n    f\\\"effect of blur across families\\\",\n    MultipleRanksums,\n    calculate_multiple_ranksums,\n)\nmultiple_ranksums: MultipleRanksums = multiple_ranksums_meas.evaluate(\n    df_all, pops, blurs\n)\nmultiple_ranksums.num_pops = len(pops)\nmultiple_ranksums.save(force=True)\n\n\n\n",
    "description": "The model receives a picture taken at a garden by a member of the general public, and it is a bit blurry.  The model should still be able to successfully identify the flower at the same rate as non-blurry images. Test data needs to include blurred flower images.  Blurred images will be created using ImageMagick. Three datasets will be generated, each with different amounts of blur: minimal blur, maximum blur, and in between minimal and maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.",
    "inputs": "three garden populations, model results on Oxford garden data",
    "output": "robustness to noise"
}